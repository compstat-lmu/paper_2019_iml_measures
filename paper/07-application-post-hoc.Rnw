\Sexpr{set_parent('paper.Rnw')}
\section{Improving Post-hoc Interpretation}
\label{sec:post-hoc}

Minimizing the number of features (NF), the interaction strength (IAS) and the main effect complexity (AMEC) improves readability, reliability and summarizability of post-hoc interpretation methods such as partial dependence plots, ALE plots, feature importance, interaction effects, local surrogate models, etc..

\subsubsection{The less features, the less verbose the interpretations.}
Our NF measure improves the readability of post-hoc analysis results.
The computational time and output size of most interpretation methods scales with $O(\text{NF})$, like feature effect plots \citep{apley2016visualizing,friedman2001greedy} or feature importance \citep{fisher2018all,casalicchio2018visualizing}.
As shown in Table~\ref{tab:spark-table-multiobj}, a model with fewer features has a more compact representation and if additionally $IAS=0$, the ALE main effects fully characterize the prediction function.
Interpretation methods that analyze 2-way feature interactions scale with $O(\text{NF}^2)$.
The computation time of a complete functional decomposition \citep{hooker2007generalized,apley2016visualizing} scales with $O(\text{NF}!)$.

\subsubsection{The less interaction, the more reliable feature effects.}
Feature effect plots, such as partial dependence plots and ALE plots visualize the marginal relationship between a feature and the prediction.
The estimated effects are averages across instances.
The effects can vary greatly for individual instances, even have opposite directions when the model includes feature interactions.
<<prepare-pdp-unreliable, out.height="3cm", cache=FALSE>>=
set.seed(123)
n = 500

cnames = c("x1", "x2", "x3", "x4", "y")

dat = mlbench::mlbench.friedman2(n, sd = 0.3)
datx = data.frame(cbind(dat$x, "y" = dat$y))
names(datx) = cnames
task = makeRegrTask(data  = datx, target = "y")

n.test = 100
dat2 = mlbench::mlbench.friedman2(n.test, sd = 0.3)
datx2 = data.frame(cbind(dat2$x, "y" = dat2$y))
names(datx2) = cnames
task2 = makeRegrTask(data  = datx2, target = "y")


y_limit = c(-200, 1200)
grid.size = 100
feature = "x2"
@

In the following simulation, we trained three models with different capabilities of modeling interactions between features: a linear regression model, a support vector machine (C=0.05) and gradient boosted trees.
We simulated \Sexpr{n} data points with 4 features and a continuous target \citep{friedman1991multivariate}.
The features are uniformly distributed in the following intervals: $0\leq x_1 \leq 100$, $ 40\pi \leq x_2 \leq 560 \pi$, $ 0 \leq x_3 \leq 1$, $ 1 \leq x_4 \leq 11$.
The target was simulated as: $y = (x_1^2 + (x_2 \cdot x_3 - (1/(x_2 \cdot x_4)))^2)^{0.5} + e$, where $ e \sim N(0,125)$.
Figure \ref{fig:pdp-unreliable} shows an increasing interaction strength depending on the model used.
This means that we prefer models with less interaction when using feature effect plots.


<<pdp-unreliable, fig.cap="The higher the interaction strength in a model (IAS increases from left to right), the less representative the Partial Dependence Plot (light thick line) becomes for all individual instances represented by their Individual Conditional Expectation curves (dark think lines).", fig.height=4.5, fig.width=12, fig.align="center", out.height="4.5cm", out.width="12cm">>=
lrn = makeLearner("regr.lm")
mod.gamboost = train(lrn, task)
pred = Predictor$new(mod.gamboost, datx2, class = 1)

ylim = c(-100, 1300)
fc = FunComplexity$new(pred, grid.size = grid.size)
r2_gamboost = 1 - fc$r2
fe = FeatureEffects$new(pred, method = "pdp+ice", center.at = 0)
p_gamboost = plot(fe$effects[[feature]], ylim = y_limit) +
	geom_label(x = 700, y = 1000, label = sprintf("IAS = %.2f", r2_gamboost), size = 10) + 
  ggtitle("Linear Model") + 
  scale_y_continuous("Prediction (centered at x=0)", limits = ylim)


lrn = makeLearner("regr.ksvm", C = 0.05)
mod.ranger = train(lrn, task)
pred = Predictor$new(mod.ranger, datx2)
fc = FunComplexity$new(pred, grid.size = grid.size)
r2_ksvm = 1- fc$r2
fe = FeatureEffects$new(pred, method = "pdp+ice", center.at = 0)
p_ksvm = plot(fe$effects[[feature]], ylim = y_limit) +
	geom_label(x = 700, y = 1000, label = sprintf("IAS = %.2f", r2_ksvm), size = 10) + 
  ggtitle("SVM") + 
  scale_y_continuous("", limits = ylim)

lrn = makeLearner("regr.xgboost", nrounds = 1000)
mod.kknn = train(lrn, task)
pred = Predictor$new(mod.kknn, datx2, class = 1)
fc = FunComplexity$new(pred, grid.size = grid.size)
r2_kknn = 1- fc$r2
fe = FeatureEffects$new(pred, method = "pdp+ice", center.at = 0)
p_kknn = plot(fe$effects[[feature]], ylim = y_limit) +
	geom_label(x = 700, y = 1000, label = sprintf("IAS = %.2f", r2_kknn), size = 10) + 
  ggtitle("gbt") + 
  scale_y_continuous("", limits = ylim)

mae.gamboost = measureMAE(datx2$y, getPredictionResponse(predict(mod.gamboost, task2)))
mae.ranger = measureMAE(datx2$y, getPredictionResponse(predict(mod.ranger, task2)))
mae.kknn = measureMAE(datx2$y, getPredictionResponse(predict(mod.kknn, task2)))

grid.arrange(p_gamboost, p_ksvm, p_kknn, nrow = 1)
@



\subsubsection{The less complex the main effects, the better summarizable.}
In linear models, a feature effect can be expressed by a single number, the regression coefficient.
If effects are non-linear the method of choice is visualization  \citep{apley2016visualizing,friedman2001greedy}.
Summarizing the effects to a single number (e.g. average marginal effects \citep{leeper2017interpreting}) can be misleading, e.g. if the effect has a U-shape, the average effect might be zero.
As a by-product of MEC, there is a third option: Instead of reporting a single number, the coefficients of the segmented linear model can be given.
Minimizing MEC means preferring models with main effects that can be described with fewer numbers, offering a more compact model description.

