\Sexpr{set_parent('paper.Rnw')}
\section{Application: Multi-objective optimization}
\label{sec:app2}

TODO: Cite MBO paper

"Note also that it is very hard to define a priori which model
size would be considered “too large” to be analyzed by a user.
Hence, instead of specifying the maximum size of a classification
model as a parameter of a classification algorithm, we prefer a
more principled approach to cope with the accuracy-
comprehensibility trade-off, such as a multi-objective approach
based on Pareto dominance or lexicographic optimization""
\citep{freitas2014comprehensible}


In the following example we use dataset XXX with XXX rows, XXX categorial feautures and XXX numerical features to predict XXX.
We train XXX different models.
The setup is the following:


TODO: Improve description of setup with mbo, optimization objectives, ...
- Nested resampling
- Multi-objective: Performance measured as XXX, number of features used, C and IA.
- we use MBO with following setting: XXX


<<mo-data>>=
devtools::load_all()
set.seed(42)

MAX_SEG_CAT = 9
MAX_SEG_NUM = 5
EPSILON = 0.05
GRID.SIZE = 50
wine = read.csv(here("./data/winequalityN.csv"))
wine = na.omit(wine)

if(DEBUG_MODE) {
  WINE_SAMPLE = sample(1:nrow(wine), size = 500)
} else {
  WINE_SAMPLE = sample(1:nrow(wine))
}


if(length(WINE_SAMPLE) < nrow(wine)) {
  warning("Not using all data points")
}

wine = wine[WINE_SAMPLE, ]

task = makeRegrTask(data = wine, target = "quality")
task.dat = getTaskData(task)
@


<<mo-run, dependson="mo-data">>=
base.learners.regr = list(
  makeLearner("regr.ksvm"),
  makeLearner("regr.kknn"),
  makeLearner("regr.glmnet"),
  makeLearner("regr.xgboost.mod"),
  makeLearner("regr.rpart"),
  makeLearner("regr.lm")
)

lrn.regr = makeModelMultiplexer(base.learners.regr)

ps.regr = makeModelMultiplexerParamSet(lrn.regr,
  regr.ksvm = makeParamSet(
    makeNumericParam("C", lower = 0, upper = 100),
    makeDiscreteParam("kernel", values = c("rbfdot", "vanilladot", "polydot"))
  ),
  regr.kknn = makeParamSet(
    makeIntegerParam("k", lower = 1, upper = 10),
    makeIntegerParam("distance", lower = 1, upper = 2)

  ),
  regr.xgboost.mod = makeParamSet(
    makeIntegerParam ("max_depth" , lower = 1, upper = 10),
    makeIntegerParam("nrounds", lower = 1, upper = 1000)
  ),
  regr.glmnet = makeParamSet(
    makeNumericParam("alpha", lower = 0, upper = 1)
  ),
  regr.rpart = makeParamSet(
    makeIntegerParam ("maxdepth" , lower = 1, upper = 6, requires = quote(booster == "gbtree"))
  )
)


rin = makeResampleInstance(cv2 , task)

lrn = lrn.regr
ps = ps.regr


sample.size = min(nrow(task.dat), 300)
sample.size = nrow(task.dat)
subset_index = sample(1:nrow(task.dat), size = sample.size)

fn = function(x){
  # removes unused params
  x = x[!is.na(x)]
  lrn = setHyperPars(lrn, par.vals = x)
  perf = resample(learner = lrn, show.info = FALSE,
    task = task , resampling = rin ,
    measures = list(mlr::mae))$aggr
  mod = train(lrn, task)
  cat(".")
  pred = Predictor$new(mod, task.dat[subset_index,], y = task$task.desc$target)
  imeasure = FunComplexity$new(pred, max_seg_cat = MAX_SEG_CAT, max_seg_num = MAX_SEG_NUM, epsilon = EPSILON, grid.size = GRID.SIZE)
  c(round(perf, 2),
    round(imeasure$c_wmean, 2),
    round(1 - imeasure$r2, 2),
    imeasure$n_features)
}

obj.fun = makeMultiObjectiveFunction(fn = fn, par.set = ps, n.objectives = 4, has.simple.signature = FALSE)

ctrl = makeMBOControl(n.objectives = 4L)
ctrl = setMBOControlInfill(ctrl, crit = crit.cb)
ctrl = setMBOControlMultiObj(ctrl, method = "parego")

mbo.lrn = makeLearner("regr.randomForest", predict.type = "se", ntree = 200)
mbo.lrn = makeImputeWrapper(mbo.lrn, classes = list(numeric = imputeMax(2), factor = imputeConstant("__miss__")))

design = generateDesign(n = 20L, par.set = ps, fun = lhs::randomLHS)

mbo.iml = mbo(fun = obj.fun, design = design, learner = mbo.lrn, control = ctrl)
pareto.set = rbindlist(lapply(mbo.iml$pareto.set, data.frame))
best.models = data.frame(cbind(round(mbo.iml$pareto.front, 2), pareto.set))
measure_names = c("y_1", "y_2", "y_3", "y_4")

# Some duplicates, because sometimes paramter changes, but all criteria remain the same.
# Only keep one of those solutions
dups = duplicated(best.models[,c("selected.learner", measure_names)])
best.models = best.models[!dups,]

# TODO: select based on params which duplicated to keep
#       e.g. rpart with lowest max.depth

# sometimes xgboost uses no features if alpha too small
best.models = best.models[best.models$y_4 > 0,]
@

<<mo-results-rename, dependson="mo-run">>=
best.models.print = best.models
measure_names = c("y_1", "y_2", "y_3", "y_4")
colnames(best.models.print) = gsub("regr.", "", colnames(best.models.print), fixed = TRUE)
colnames(best.models.print) = gsub("mod.", "", colnames(best.models.print), fixed = TRUE)
param_cols = setdiff(colnames(best.models.print), c(measure_names, "selected.learner"))
numeric_params = sapply(best.models.print[,param_cols], function(x) is.numeric(x))
numeric_params = names(numeric_params[which(numeric_params)])
best.models.print = data.frame(best.models.print)
best.models.print[,numeric_params] = round(best.models.print[,numeric_params], 2)


best.models.print$xgboost.alpha[best.models.print$xgboost.booster == "gbtree"] = NA
best.models.print$xgboost.max_depth[best.models.print$xgboost.booster == "gblinear"] = NA
best.models.print$selected.learner = gsub("regr.", "", best.models.print$selected.learner, fixed = TRUE)
best.models.print$selected.learner = gsub("xgboost.mod", "xgboost", best.models.print$selected.learner, fixed  = TRUE)

# Shorter description of learner
best.models.print$descr = apply(best.models.print, 1, function(row) {
  lrn_name = row['selected.learner']
  params = row[param_cols]
  params = params[!is.na(params)]
  names(params) = gsub(sprintf("%s.", lrn_name), "", names(params), fixed = TRUE)
  param_string = sprintf("%s:%s", names(params), params)
  param_string = paste(param_string, collapse = ",")
  sprintf("%s (%s)", lrn_name, param_string)
})
xtable::xtable(best.models.print[c("descr", measure_names)])

@




We visualize the pareto front, but instead of the loss minimization we show the following metrics:

All metrics are scaled to the interval of $[0,1]$, the higher the better (better performing or better interpretability)

\begin{itemize}
\item Performance as PRL, where we set constant baseline as supremum and the best model as supremum
\item Sparsity as $1 - \frac{n.features}{p}$
\item Addditivity as $R^2$ of first order model
\item Simplicty as $C_{max} - C + 1$
\end{itemize}

<<plot-best-mbo, dependson="mo-results-rename">>=
baseline_performance = measureMAE(task.dat[,task$task.desc$target], response = mean(task.dat[,task$task.desc$target]))
best_performance = min(best.models.print$y_1)
best.models.print$performance = (best.models.print$y_1 - baseline_performance) / (best_performance - baseline_performance)


best.models.print$sparsity  = 1 - best.models.print$y_4/ sum(task$task.desc$n.feat)
best.models.print$additivity  = 1 - best.models.print$y_3 / max(best.models.print$y_3)
best.models.print$simplicity  = 1 - ((best.models.print$y_2 - 1 ) / (max(best.models.print$y_2) - 1))

measure_names = c("performance", "sparsity", "additivity", "simplicity")
plot.dat = melt(best.models.print, measure.vars = measure_names)

#param_cols = setdiff(colnames(best.models), c(measure_names, "selected.learner"))

plot.dat = unique(plot.dat)
p = ggplot(plot.dat) +
  geom_col(aes(x = variable, y = value)) +
  facet_wrap("descr") +
  coord_flip()
print(p)

nudge_y = -0.25
# TODO: change to colorblind friendly colors
p  = ggplot(mapping = aes(y = variable, x = descr, color = value)) +
  #geom_point(data = max.plot.dat, shape = 1) +
  geom_point(data = plot.dat, size = 15) +
  scale_size_continuous(range = c(0,10), guide = "none") +
  scale_x_discrete("") +
  scale_color_gradient2(midpoint = 0.5, mid = "orange", high = "green") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  geom_label(data = plot.dat[plot.dat$variable == "simplicity",],
    aes(label = sprintf("C:%.2f", y_2)), nudge_y = nudge_y, color = "black") +
  geom_label(data = plot.dat[plot.dat$variable == "additivity",],
    aes(label = sprintf("IA:%.2f", y_3)), nudge_y = nudge_y, color = "black") +
  geom_label(data = plot.dat[plot.dat$variable == "sparsity",],
    aes(label = sprintf("P:%.0f", y_4)), nudge_y = nudge_y, color = "black") +
  geom_label(data = plot.dat[plot.dat$variable == "performance",],
    aes(label = sprintf("MAE:%.3f", y_1)), nudge_y = nudge_y, color = "black")
print(p)
@


TODO: Recreate Accuracy x Interpretability graphic, but three times with differen x-axis (sparsity, additivity, simplicity)

Recreation of accuracy x interpretability graphic

Basically inverted paretor front in two dimsenions
<<perf-interpret-tradeoff, dependson="mo-results-rename">>=
ggplot(best.models.print) +
  geom_point(aes(y = performance, x = additivity))

ggplot(best.models.print) +
  geom_point(aes(y = performance, x = sparsity))

ggplot(best.models.print) +
  geom_point(aes(y = performance, x = simplicity))


ggplot(best.models.print) +
  geom_point(aes(y = performance, x = simplicity * additivity * sparsity))

ggplot(best.models.print,aes(y = performance, x = simplicity + additivity + sparsity)) +
  geom_label(aes(label = descr))
@


TODO: Show FunComplexity of model with best performance

TODO: Pick one other interesting model and show FunComplexity


<<mo-best-performance, dependson="mo-run">>=
## Extract parameters and refit best solutions
best_index = which(mbo.iml$pareto.front[,"y_1"] == min(mbo.iml$pareto.front[,"y_1"]))
pp = mbo.iml$pareto.set[[best_index]]
pp = pp[!is.na(pp)]
lrn = setHyperPars(lrn, par.vals = pp)
mod = train(lrn, task)
pred = Predictor$new(mod, task.dat)
fc = FunComplexity$new(pred)
plot(fc, nrow = 2)
@



<<mo-interesting-solution, dependson="mo-run">>=
## Extract parameters and refit best solutions
pareto_index = 8
pp = mbo.iml$pareto.set[[pareto_index]]
pp = pp[!is.na(pp)]
lrn = setHyperPars(lrn, par.vals = pp)
mod = train(lrn, task)
pred = Predictor$new(mod, task.dat)
fc = FunComplexity$new(pred)
plot(fc, nrow = 1)
@





