\Sexpr{set_parent('paper.Rnw')}
\section{Application: Multi-objective optimization}
\label{sec:app2}

TODO: Cite MBO paper

"Note also that it is very hard to define a priori which model
size would be considered “too large” to be analyzed by a user.
Hence, instead of specifying the maximum size of a classification
model as a parameter of a classification algorithm, we prefer a
more principled approach to cope with the accuracy-
comprehensibility trade-off, such as a multi-objective approach
based on Pareto dominance or lexicographic optimization""
\citep{freitas2014comprehensible}


In the following example we use dataset XXX with XXX rows, XXX categorial feautures and XXX numerical features to predict XXX.
We train XXX different models.
The setup is the following:

- Nested resampling
- Multi-objective: Performance measured as XXX, number of features used, C and IA.
- we use MBO with following setting: XXX


<<mo-data>>=
configureMlr(show.info = FALSE)

devtools::load_all()
set.seed(42)

TASK_TYPE = "regr"
MAX_C  = 10
EPSILON = 0.05
GRID.SIZE = 50

tasks = listOMLTasks()
if(TASK_TYPE == "regr") {
  tasks = tasks[grepl("Supervised Regression", tasks$task.type), ]
} else {
  tasks = tasks[grepl("Supervised Classification", tasks$task.type), ]
  tasks = tasks[tasks$number.of.classes  == 2, ]
}
tasks = tasks[tasks$number.of.instances.with.missing.values == 0, ]
tasks = tasks[tasks$number.of.symbolic.features > 0, ]
tasks = tasks[tasks$number.of.features < 150, ]
tasks = tasks[tasks$number.of.features > 4, ]
tasks = tasks[tasks$number.of.instances < 10000, ]
tasks = tasks[tasks$number.of.instances > 200, ]
tasks = tasks[!is.na(tasks$task.id), ]
i = 8


task = getOMLTask(task.id = tasks$task.id[i])
task = convertOMLTaskToMlr(task)
task = task$mlr.task
task.dat = getTaskData(task)
summary(task.dat)
print(dim(task.dat))
devtools::load_all()
set.seed(42)
wine = read.csv("./data/winequalityN.csv")
wine = na.omit(wine)

WINE_SAMPLE = sample(1:nrow(wine), size = 500)
WINE_SAMPLE = sample(1:nrow(wine))

wine = wine[WINE_SAMPLE, ]

task = makeRegrTask(data = wine, target = "quality")
task.dat = getTaskData(task)
summary(task.dat)
print(dim(task.dat))
@


<<mo-run, dependson="mo-data">>=
base.learners.classif = list(
  makeLearner("classif.ksvm", predict.type = "prob"),
  makeLearner("classif.rpart", predict.type = "prob"),
  makeLearner("classif.gamboost", predict.type = "prob"),
  makeLearner("classif.kknn", predict.type = "prob"),
  makeLearner("classif.naiveBayes", predict.type = "prob")
)

base.learners.regr = list(
  makeLearner("regr.ksvm"),
  makeLearner("regr.xgboost.mod"),
  makeLearner("regr.rpart"),
  makeLearner("regr.gamboost")
)

lrn.regr = makeModelMultiplexer(base.learners.regr)
lrn.classif = makeModelMultiplexer(base.learners.classif)

ps.regr = makeModelMultiplexerParamSet(lrn.regr,
  makeNumericParam("C", lower = 0, upper = 100),
  makeIntegerParam ("max_depth" , lower = 1, upper = 6, requires = quote(booster == "gbtree")),
  makeIntegerParam ("maxdepth" , lower = 1, upper = 10),
  makeDiscreteParam("booster", values = c("gbtree", "gblinear")),
  makeNumericParam("alpha", lower = 0, upper = 100, requires = quote(booster == "gblinear")),
  makeIntegerParam("nrounds", lower = 1, upper = 1000),
  makeIntegerParam("mstop", lower = 1, upper = 1000)
)

ps.classif = makeModelMultiplexerParamSet(lrn.classif,
  makeNumericParam("C", lower = 0, upper = 100),
  makeIntegerParam ("maxdepth" , lower = 1, upper = 10),
  makeIntegerParam("mstop", lower = 1, upper = 1000),
  makeIntegerParam("k", lower = 1, upper = 10),
  makeNumericParam("laplace", lower = 0, upper = 1)
)

rin = makeResampleInstance(cv2 , task)

if(TASK_TYPE == "classif") {
  lrn = lrn.classif
  loss = auc
  ps = ps.classif
} else {
  lrn = lrn.regr
  loss = mae
  ps = ps.regr
}


sample.size = min(nrow(task.dat), 300)
sample.size = nrow(task.dat)
subset_index = sample(1:nrow(task.dat), size = sample.size)

fn = function(x){
  # removes unused params
  x = x[!is.na(x)]
  lrn = setHyperPars(lrn, par.vals = x)
  perf = resample(learner = lrn, show.info = FALSE,
    task = task , resampling = rin ,
    measures = list(loss))$aggr
  mod = train(lrn, task)
  pred = Predictor$new(mod, task.dat[subset_index,], y = task$task.desc$target)
  imeasure = FunComplexity$new(pred, max_c = MAX_C, epsilon = EPSILON, grid.size = GRID.SIZE)
  c(round(perf, 2),
    round(imeasure$c_wmean, 2),
    round(1 - imeasure$r2, 2),
    imeasure$n_features)
}

obj.fun = makeMultiObjectiveFunction(fn = fn, par.set = ps, n.objectives = 4, has.simple.signature = FALSE)

ctrl = makeMBOControl(n.objectives = 4L)
ctrl = setMBOControlInfill(ctrl, crit = crit.cb)
ctrl = setMBOControlMultiObj(ctrl, method = "parego")

mbo.lrn = makeLearner("regr.randomForest", predict.type = "se", ntree = 200)
mbo.lrn = makeImputeWrapper(mbo.lrn, classes = list(numeric = imputeMax(2), factor = imputeConstant("__miss__")))

design = generateDesign(n = 20L, par.set = ps, fun = lhs::randomLHS)

mbo.iml = mbo(fun = obj.fun, design = design, learner = mbo.lrn, control = ctrl)
pareto.set = rbindlist(lapply(mbo.iml$pareto.set, data.frame))
best.models = data.frame(cbind(round(mbo.iml$pareto.front, 2), pareto.set))
measure_names = c("y_1", "y_2", "y_3", "y_4")

# Some duplicates, because sometimes paramter changes, but all criteria remain the same.
# Only keep one of those solutions
dups = duplicated(best.models[,c("selected.learner", measure_names)])
best.models = best.models[!dups,]

# TODO: select based on params which duplicated to keep
#       e.g. rpart with lowest max.depth

# sometimes xgboost uses no features if alpha too small
best.models = best.models[best.models$y_4 > 0,]
@

<<mo-results-rename, dependson="mo-setup", eval=FALSE>>=
best.models.print = best.models
measure_names = c("y_1", "y_2", "y_3", "y_4")
colnames(best.models.print) = gsub("regr.", "", colnames(best.models.print), fixed = TRUE)
colnames(best.models.print) = gsub("mod.", "", colnames(best.models.print), fixed = TRUE)
param_cols = setdiff(colnames(best.models.print), c(measure_names, "selected.learner"))
numeric_params = sapply(best.models.print[,param_cols], function(x) is.numeric(x))
numeric_params = names(numeric_params[which(numeric_params)])
best.models.print = data.frame(best.models.print)
best.models.print[,numeric_params] = round(best.models.print[,numeric_params], 2)


best.models.print$xgboost.alpha[best.models.print$xgboost.booster == "gbtree"] = NA
best.models.print$xgboost.max_depth[best.models.print$xgboost.booster == "gblinear"] = NA
best.models.print$selected.learner = gsub("regr.", "", best.models.print$selected.learner, fixed = TRUE)
best.models.print$selected.learner = gsub("xgboost.mod", "xgboost", best.models.print$selected.learner, fixed  = TRUE)

# Shorter description of learner
best.models.print$descr = apply(best.models.print, 1, function(row) {
  lrn_name = row['selected.learner']
  params = row[param_cols]
  params = params[!is.na(params)]
  names(params) = gsub(sprintf("%s.", lrn_name), "", names(params), fixed = TRUE)
  param_string = sprintf("%s:%s", names(params), params)
  param_string = paste(param_string, collapse = ",")
  sprintf("%s (%s)", lrn_name, param_string)
})
xtable::xtable(best.models.print[c("descr", measure_names)])

@




We visualize the pareto front, but instead of the loss minimization we show the following metrics:

All metrics are scaled to the interval of $[0,1]$, the higher the better (better performing or better interpretability)

\begin{itemize}
\item Performance as PRL, where we set constant baseline as supremum and the best model as supremum$
\item Sparsity as $1 - \frac{n.features}{p}$
\item Addditivity as $R^2$ of first order model
\item Simplicty as $C_{max} - C + 1$
\end{itemie}

<<plot-best-mbo>>=
baseline_performance = measureMAE(task.dat[,task$task.desc$target], response = mean(task.dat[,task$task.desc$target]))
best_performance = min(best.models.print$y_1)
best.models.print$performance = (best.models.print$y_1 - baseline_performance) / (best_performance - baseline_performance)


best.models.print$sparsity  = 1 - best.models.print$y_4/ sum(task$task.desc$n.feat)
best.models.print$additivity  = 1 - best.models.print$y_3 / max(best.models.print$y_3)
best.models.print$simplicity  = 1 - ((best.models.print$y_2 - 1 ) / (max(best.models.print$y_2) - 1))

measure_names = c("performance", "sparsity", "additivity", "simplicity")
plot.dat = melt(best.models.print, measure.vars = measure_names)

#param_cols = setdiff(colnames(best.models), c(measure_names, "selected.learner"))

plot.dat = unique(plot.dat)
ggplot(plot.dat) +
  geom_col(aes(x = variable, y = value)) +
  facet_wrap("descr") +
  coord_flip()

nudge_y = -0.25
# TODO: change to colorblind friendly colors
ggplot(mapping = aes(y = variable, x = descr, color = value)) +
  #geom_point(data = max.plot.dat, shape = 1) +
  geom_point(data = plot.dat, size = 15) +
  scale_size_continuous(range = c(0,10), guide = "none") +
  scale_x_discrete("") +
  scale_color_gradient2(midpoint = 0.5, mid = "orange", high = "green") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  geom_label(data = plot.dat[plot.dat$variable == "simplicity",],
    aes(label = sprintf("C:%.2f", y_2)), nudge_y = nudge_y, color = "black") +
  geom_label(data = plot.dat[plot.dat$variable == "additivity",],
    aes(label = sprintf("IA:%.2f", y_3)), nudge_y = nudge_y, color = "black") +
    geom_label(data = plot.dat[plot.dat$variable == "sparsity",],
    aes(label = sprintf("P:%.0f", y_4)), nudge_y = nudge_y, color = "black") +
  geom_label(data = plot.dat[plot.dat$variable == "performance",],
    aes(label = sprintf("MAE:%.3f", y_1)), nudge_y = nudge_y, color = "black")
@


TODO: Recreate Accuracy x Interpretability graphic, but three times with differen x-axis (sparsity, additivity, simplicity)

Recreation of accuracy x interpretability graphic

Basically inverted paretor front in two dimsenions
<<perf-interpret-tradeoff>>=
ggplot(best.models.print) +
  geom_point(aes(y = performance, x = additivity))

ggplot(best.models.print) +
  geom_point(aes(y = performance, x = sparsity))

ggplot(best.models.print) +
  geom_point(aes(y = performance, x = simplicity))


ggplot(best.models.print) +
  geom_point(aes(y = performance, x = simplicity * additivity * sparsity))

ggplot(best.models.print) +
  geom_point(aes(y = performance, x = simplicity + additivity + sparsity))
@


TODO: Show FunComplexity of model with best performance

TODO: Pick one other interesting model and show FunComplexity


<<mo-best-performance, dependson="mo-setup", eval=FALSE>>=
## Extract parameters and refit best solutions
best_index = which(mbo.iml$pareto.front[,"y_1"] == min(mbo.iml$pareto.front[,"y_1"]))
pp = mbo.iml$pareto.set[[best_index]]
pp = pp[!is.na(pp)]
lrn = setHyperPars(lrn, par.vals = pp)
mod = train(lrn, task)
pred = Predictor$new(mod, task.dat)
fc = FunComplexity$new(pred)
plot(fc, nrow = 2)
@



<<mo-interesting-solution, dependson="mo-setup", eval=FALSE>>=
## Extract parameters and refit best solutions
pareto_index = 3
pp = mbo.iml$pareto.set[[pareto_index]]
pp = pp[!is.na(pp)]
lrn = setHyperPars(lrn, par.vals = pp)
mod = train(lrn, task)
pred = Predictor$new(mod, task.dat)
fc = FunComplexity$new(pred)
plot(fc, nrow = 1)
@


















TODO:

- Re-Create the interpretability vs accuracy figure
To get interpretability to one dimension:
We have 100 percent variance of y.
x percent is explained by interations
1 - x percent is explained by 1st order model.
first order complexity is C.
we claim that the average complexity of higher-order is the same as 1st order and simply project the functional complexity to the rest:
$overall.complexity = c  + c \cdot x  = c times (1 + x)$
and to flip direction and scale to 0 and 1 (only when multiple models are tested):
$interpretability = 1 - overall_c / max(overall_c)$

theoretically maximal complexity:
$max.components \cdot n_features \cdot (1 + c)$
where max.components is the maximal complexity a feature can get based on user input and n.features the number of features.

Mathematically writing down objectives

Short mbo summary and setup.
Tune across many learners (xgboost, svm, lm, rpart, ...)
Many datasets
Show Pareto Front for some datasets.

Aggregated results per learner (maybe do mbo per learner) and recreate accuracy / interpretability figures





