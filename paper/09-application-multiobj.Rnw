\Sexpr{set_parent('paper.Rnw')}
\section{Optimization Performance and Interpretability}
\label{sec:multiobj}

% TODO: Rename R package names to actual algos (rpart -> CART, xgboost -> gbt, ranger -> rf)

<<params>>=
### Set parameters for application
DEBUG_MODE = FALSE 
set.seed(42)
# Parameters for functional complexity
MAX_SEG_CAT = 9
MAX_SEG_NUM = 5
EPSILON = 0.05
GRID.SIZE = 50

# Parameters for mbo
n.folds = 3
max.iters = 500
@

<<mo-data>>=
wine = read.csv(here("./data/winequalityN.csv"))
wine = na.omit(wine)

# only use white wine
wine = wine[wine$type == "white", ]
wine$type = NULL

if(DEBUG_MODE) {
  WINE_SAMPLE = sample(1:nrow(wine), size = 1500)
} else {
  WINE_SAMPLE = sample(1:nrow(wine))
}

if(length(WINE_SAMPLE) < nrow(wine)) {
  warning("Not using all data points")
}
wine = wine[WINE_SAMPLE, ]

task = makeRegrTask(data = wine, target = "quality")
task.dat = getTaskData(task)
@

<<mo-learners>>=
base.learners.regr = list(
  makeLearner("regr.ksvm"),
  makeLearner("regr.glmnet"),
  makeLearner("regr.xgboost.mod"),
  makeLearner("regr.rpart"),
  makeLearner("regr.ranger", num.trees = 1000),
  makeLearner("regr.gamboost")
)

lrn.regr = makeModelMultiplexer(base.learners.regr)

ps.regr = makeModelMultiplexerParamSet(lrn.regr,
  regr.ksvm = makeParamSet(
    makeNumericParam("C", lower = -15, upper = 15, trafo = function(x) 2^x),
    makeNumericParam("sigma", lower = -15, upper = 15, trafo = function(x) 2^x)
  ),
  regr.xgboost.mod = makeParamSet(
    makeIntegerParam ("max_depth" , lower = 1, upper = 10),
    makeIntegerParam("nrounds", lower = 1, upper = 1000),
    makeNumericParam("eta", lower = -10, upper = -2, trafo = function(x) 2^x)
  ),
  regr.glmnet = makeParamSet(
    makeNumericParam("alpha", lower = 0, upper = 1),
    makeNumericParam("lambda", lower = -15, upper = 15, trafo = function(x) 2^x)
  ),
  regr.rpart = makeParamSet(
    makeIntegerParam("maxdepth", lower = 1, upper = 20),
    makeNumericParam("cp", lower = 0, upper = 1)
  ),
  regr.gamboost = makeParamSet(
    makeIntegerParam("mstop", lower = 1, upper = 500)
  )
)

@

% =============================================================================
% Motivation for application
% =============================================================================
As one of the main application of the functional complexity  measures, we demonstrate with the wine quality dataset how to optimize performance and interpretability in a multi-object optimization application.


% =============================================================================
% Wine Data
% =============================================================================
\subsubsection{Predicting Wine Quality.}
We used the wine quality dataset \citep{cortez2009modeling}, which contains physicochemical properties like alcohol and residual sugar of \Sexpr{nrow(wine)} white wines.
The goal is to predict the quality of the wineon a scale from 0 to 10, assessed by the median of three blind ratings.
The data is freely available online.

% =============================================================================
% Why like this?
% =============================================================================
\subsubsection{Motivation.}
As \citep{freitas2014comprehensible} points out, it is difficult to know before model what the tradeoffs between interpretability and performance are and suggests multi-objective optimization based on Pareto dominance.
This stands in contrast to apriori selecting an interpretable model class (e.g. decision rules)  and optimizing within this class or exclusively optimizing performance and applying post-hoc interpretations.
We suggest to search over a wide spectrum of different models and hyper parameters, present the set of Pareto optimal models and let the practitioner choose a suitable tradeoff between interpretability and performance.
Since we offer three measures of interpretability, we provide a more in-depth characterization of the model's behavior, allowing practitioners to make more informed decisions (e.g. how much does performance suffer when we use model with interactions?).

% =============================================================================
% Objectives and Models
% =============================================================================
\subsubsection{Objectives and Models.}
We apply the model-based optimization framework \citep{mlrMBO} to find the best model based on the following 4 objectives: number of features used by the model (NF), main effect complexity (MEC), interaction strength (IAS) and the mean absolute error (MAE) of the prediction.
We compare the following models: linear regression model, CART (tuning maximum tree-depth), a support vector machine (tuning C and sigma), sparse linear models (tuning alpha and lambda), gradient boosted trees (gbt) (tuning maximum depth and number of rounds) and gradient boosted gam (tuning mstop).


% =============================================================================
% MBO Setup
% =============================================================================
\subsubsection{Model-based Optimization Setup.}
We use the ParEGO algorithm \citep{knowles2006parego} for the multi-objective optimization.
The fitness function is the 4-dimensional vector  $(MAE, NF, MEC, IAS)$.
Within the fitness function,the MAE is estimated using \Sexpr{n.folds}-fold cross-validation.
The other measures (NF, MEC, IA) are estimated in-sample.
We set the maximum iterations to \Sexpr{max.iters}.
For the parameters of the multi-objective optimization, we relied on the sensitive defaults provided by \citep{mlrMBO}.
TODO: We used sensible default setting from mbo author
TODO: Save package versions of all packages

% Question: What is the stop criterion? Could not find out with the help files.
% TODO: Write about stop criterion
% Question: Would it makes sense to have a stratified initial design to ensure that we sample each learner for sure?

<<mo-setup, dependson=c("params", "mo-learners", "mo-data")>>=
fn = function(x){
  # removes unused params
  x = x[!is.na(x)]
  lrn = setHyperPars(lrn.regr, par.vals = x)
  mae_loss = resample(learner = lrn, show.info = FALSE,
    task = task , resampling = rin ,
    measures = list(mlr::mae))$aggr
  mod = train(lrn, task)
  pred = Predictor$new(mod, task.dat, y = task$task.desc$target)
  imeasure = FunComplexity$new(pred, max_seg_cat = MAX_SEG_CAT, max_seg_num = MAX_SEG_NUM, epsilon = EPSILON, grid.size = GRID.SIZE)
  cat(".")
  c(round(mae_loss, 2),
    round(imeasure$c_wmean, 1),
    round(1 - imeasure$r2, 2),
    imeasure$n_features)
}

# Setup MBO
resampDescr = makeResampleDesc(method = "CV", iters = n.folds)
rin = makeResampleInstance(resampDescr, task)
obj.fun = makeMultiObjectiveFunction(fn = fn, par.set = ps.regr, n.objectives = 4, has.simple.signature = FALSE)
measure_names = c("MAE", "MEC", "IAS", "NF")
ctrl = makeMBOControl(n.objectives = 4L, y.name = measure_names)
ctrl = setMBOControlMultiObj(ctrl, method = "parego")
ctrl = setMBOControlTermination(ctrl, iters = max.iters)
@



<<mo-run, dependson=c("mo-setup"), results='hide'>>=
# Start MBO
mbo.iml = mbo(fun = obj.fun, control = ctrl)
saveRDS(mbo.iml, file = "mbo-results.RDS")
@

<<mo-results, dependson="mo-run">>=
mbo.iml = readRDS("mbo-results.RDS")
pareto.set = rbindlist(lapply(mbo.iml$pareto.set, data.frame))
best.models = data.frame(cbind(round(mbo.iml$pareto.front, 2), pareto.set))

# Remember the index so that models can later be retrieved from mbo object
best.models$index = 1:nrow(best.models)

# Some duplicates, because sometimes paramter changes, but all criteria remain the same.
# Only keep one of those solutions
dups = duplicated(best.models[,c("selected.learner", measure_names)])
best.models = best.models[!dups,]
@


<<mo-results-rename, dependson="mo-results", results = "asis">>=
#Improves the naming of the learners
best.models.print = best.models
log_params = c("regr.ksvm.C", "regr.ksvm.sigma", "regr.glmnet.lambda", "regr.xgboost.mod.eta")
best.models.print[, log_params] = 2^best.models.print[, log_params]
colnames(best.models.print) = gsub("regr.", "", colnames(best.models.print), fixed = TRUE)
colnames(best.models.print) = gsub("mod.", "", colnames(best.models.print), fixed = TRUE)
param_cols = setdiff(colnames(best.models.print), c(measure_names, "selected.learner", "index"))
numeric_params = sapply(best.models.print[,param_cols], function(x) is.numeric(x))
numeric_params = names(numeric_params[which(numeric_params)])
best.models.print = data.frame(best.models.print)
best.models.print[,numeric_params] = round(best.models.print[,numeric_params], 2)
best.models.print$selected.learner = gsub("regr.", "", best.models.print$selected.learner, fixed = TRUE)
best.models.print$selected.learner = gsub("xgboost.mod", "xgboost", best.models.print$selected.learner, fixed  = TRUE)

lrn_model_map = c("xgboost" = "gbt", "rpart" = "CART", "featureless" = "median", 
		  "ksvm" = "svm", "gamboost" = "gamb", "glmnet" = "ridge", "ranger" = "rf")

best.models.print$model = lrn_model_map[best.models.print$selected.learner]


# Shorter description of learner
best.models.print$descr = apply(best.models.print, 1, function(row) {
  lrn_name = row['model']
  params = row[param_cols]
  params = params[!is.na(params)]
  names(params) = gsub(sprintf("%s.", lrn_name), "", names(params), fixed = TRUE)
  param_string = sprintf("%s:%s", names(params), params)
  param_string = paste(param_string, collapse = ",")
  # When model has no parameters
  param_string = gsub("()", "", param_string, fixed = TRUE)
  sprintf("%s (%s)", lrn_name, param_string)
})


# Table of Pareto set + objective values
tab.cap = "Pareto front of models minimizing mean absolute error (MAE), number of features (NF), main effect complexity (MEC) and interaction strength (IA)."
to.print = best.models.print[order(best.models.print$MAE), c("descr", measure_names)]
rownames(to.print) = NULL
baseline_performance = measureMAE(task.dat[,task$task.desc$target], response = mean(task.dat[,task$task.desc$target]))
to.print = rbind(to.print, data.frame("descr" = "median", "MAE" = baseline_performance, "MEC" = 0, "IAS" = 0, "NF"=0))
xtable::xtable(to.print, caption = tab.cap, label = "tab:pareto")


# Compute scaled version of measures
best_performance = min(best.models.print$MAE)
best.models.print$sMAE = (best.models.print$MAE - best_performance) / (baseline_performance - best_performance)
best.models.print$sNF  = best.models.print$NF / sum(task$task.desc$n.feat)
best.models.print$sIAS  = best.models.print$IAS / max(best.models.print$IAS)
best.models.print$sMEC  = best.models.print$MEC / max(best.models.print$MEC)
# Because of featureless learner
best.models.print$sMEC = pmax(0, best.models.print$sMEC)
smeasure_names = c("sMAE", "sMEC", "sIAS", "sNF")
@


% =============================================================================
% Pareto Front Table
% =============================================================================
\subsubsection{Results.}
Table \ref{tab:pareto} shows the set of Pareto optimal models along with their MAE, NF, IAS and MEC measures.
For a more informative visualization, we propose to visualize the main effects along with the measures for a selection of models in Table~\ref{tab:spark-table-multiobj}.
To get an interesting selection of models, we chose 4 models with different tradeoffs between the 4 measures.
Table~\ref{tab:spark-table-multiobj} shows different tradeoffs.
% =============================================================================
% Best performing model
% =============================================================================

<<spark-table-multiobj, results='asis', dependson=c("mo-run", "mo-results-rename")>>=
single_obj = function(measure_df, lambdas){
  assert_numeric(lambdas, len = 4, null.ok = FALSE)
  assert_data_frame(measure_df, ncols = 4)
  rowSums(as.matrix(measure_df) %*% lambdas)
}

# Best models first
best.models.print = best.models.print[order(best.models.print$MAE),]
measure_df = best.models.print[smeasure_names]

best_perf = which.min(best.models.print$MAE)
# MAE-best model with bounded MEC
best_2 = which(best.models.print$MEC <= 2)[1]
# MAE-best model that models few interactions
best_3 = which(best.models.print$IA <= 0.1)[1]
# Best model that does not use all features
best_4 = which(best.models.print$NF <=  7)[1] 

indices = c(best_perf, best_2, best_3, best_4)
mbo_indices = best.models.print[indices,"index"]
tab = get_spark_table(mbo_obj = mbo.iml, indices =  mbo_indices, ylim = c(-2, 2), height = 5, width=10, log_params = log_params)
colnames(tab) = best.models.print$descr[indices]
# Because I use as.is later with xtable
colnames(tab) = gsub("_","\\_", colnames(tab), fixed = TRUE)
colnames(tab) = gsub("()", "",colnames(tab), fixed = TRUE)
rownames(tab) = c("MAE", "NF", "IA", "MEC", setdiff(colnames(task.dat), "quality"))

cap = sprintf("A selection of %i models from the Pareto optimal set. From left to right, the models with best MAE, best MAE when $MEC \\leq 2$, best MAE when $IA =\\leq 0.1$, best MAE with $NF \\leq 7$.", length(indices))
xtab = xtable(tab, caption = cap, label = "tab:spark-table-multiobj")
align(xtab) <- "l|p{2.2cm}p{2.2cm}p{2.2cm}p{2.2cm}"
print.xtable(xtab, sanitize.text.function = as.is)
@

% =============================================================================
% Visualization of Pareto Front
% =============================================================================
\subsubsection{Performance-Interpretability Tradeoff}
The complexity measures allow us to study how model classes and parameter settings trade off interpretability and performance.
Figure~\ref{fig:perf-interpret-tradeoff} shows each model with hyperparameter configuration from the Pareto set.
First we scaled all measures to the range of $[0,1]$ by scaling each measure with meaningful upper and lower bounds:

$$M_{scaled}(M) = \frac{M - M_{inf}}{M_{sup} - M_{inf}}$$

For MAE, we set $M_{inf}$ to the lowest MAE observed among all models and $M_{sup} = \sum_{i=1}^n|y_i - median(y)|$.
For NF, we set $M_{inf}=0$ and $M_{sup}=p$.
For MEC, we set $M_{inf}=0$ and $M_{sup}$ to the highest observed MEC among all models.
For IAS, we set $M_{inf}=0$ and $M_{sup}$ to the highest observed IAS among all models.


We sum the scaled measures and call it "Interpretability".
$$Interpretability = 3 - (NF_{scaled} + IAS_{scaled} + MEC_{scaled})$$

This weights all three (scaled) measures equally, other tradeoffs would have also been possible.
The maximal interpretability is 3 for the model that always predicts the median wine quality.
The theoretical minimal interpretability is 0 for a model that uses all features and has the highest interaction strength and main effect complexity measure among all Pareto optimal models.

<<perf-interpret-tradeoff, dependson="mo-results-rename", fig.cap="Performance vs. Interpretability tradeoff for predicting wine quality.",  fig.height=7, fig.width=12, fig.align="center", out.height="7cm", out.width="12cm">>=
ggplot(best.models.print,aes(x = 1 - sMAE, y = 3 - (sNF + sMEC + sIAS))) +
  geom_point(size = 4) + 
  geom_text_repel(aes(label = descr), size = 6) +
  scale_x_continuous("Performance", limits = c(NA, 1)) +
  scale_y_continuous("Interpretability", limits = c(0, 3))
@



% =============================================================================
% =============================================================================
% Further Material
% =============================================================================
% =============================================================================

<<plot-best-mbo, dependson="mo-results-rename", fig.cap = "Comparing models with different interpretability and performance tradeoffs.", eval = FALSE>>=
#param_cols = setdiff(colnames(best.models), c(measure_names, "selected.learner"))
plot.dat = melt(best.models.print, measure.vars = measure_names)
plot.dat = unique(plot.dat)
p = ggplot(plot.dat) +
  geom_col(aes(x = variable, y = value)) +
  facet_wrap("descr") +
  coord_flip()
print(p)

nudge_y = -0.25
p  = ggplot(mapping = aes(y = variable, x = descr, color = value)) +
  #geom_point(data = max.plot.dat, shape = 1) +
  geom_point(data = plot.dat, size = 15) +
  scale_size_continuous(range = c(0,10), guide = "none") +
  scale_x_discrete("") +
  scale_color_viridis(option = "plasma") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  geom_label(data = plot.dat[plot.dat$variable == "simplicity",],
    aes(label = sprintf("C:%.2f", MEC)), nudge_y = nudge_y, color = "black") +
  geom_label(data = plot.dat[plot.dat$variable == "additivity",],
    aes(label = sprintf("IA:%.2f", IAS)), nudge_y = nudge_y, color = "black") +
  geom_label(data = plot.dat[plot.dat$variable == "sparsity",],
    aes(label = sprintf("P:%.0f", NF)), nudge_y = nudge_y, color = "black") +
  geom_label(data = plot.dat[plot.dat$variable == "performance",],
    aes(label = sprintf("MAE:%.3f", MAE)), nudge_y = nudge_y, color = "black")
print(p)
@


