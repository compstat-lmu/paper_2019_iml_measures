\Sexpr{set_parent('paper.Rnw')}
\section{Optimization Performance and Interpretability}
\label{sec:multiobj}

<<params>>=
### Set parameters for application
DEBUG_MODE = FALSE
set.seed(42)
# Parameters for functional complexity
MAX_SEG_CAT = 9
MAX_SEG_NUM = 5
EPSILON = 0.05
GRID.SIZE = 50

# Parameters for mbo
n.folds = 5
surrogate.ntree = 200
n.initial.design = 200L
infill.criterion = crit.cb
n.cores = 1
n.augment = 30
@

<<mo-data>>=
wine = read.csv(here("./data/winequalityN.csv"))
wine = na.omit(wine)

# only use white wine
wine = wine[wine$type == "white", ]
wine$type = NULL

if(DEBUG_MODE) {
  WINE_SAMPLE = sample(1:nrow(wine), size = 1500)
} else {
  WINE_SAMPLE = sample(1:nrow(wine))
}

if(length(WINE_SAMPLE) < nrow(wine)) {
  warning("Not using all data points")
}
wine = wine[WINE_SAMPLE, ]

task = makeRegrTask(data = wine, target = "quality")
task.dat = getTaskData(task)
@

<<mo-learners>>=
base.learners.regr = list(
  makeLearner("regr.ksvm"),
  makeLearner("regr.kknn"),
  makeLearner("regr.glmnet"),
  makeLearner("regr.xgboost.mod"),
  makeLearner("regr.rpart"),
  makeLearner("regr.lm"),
  makeLearner("regr.featureless", method = "median")
)

lrn.regr = makeModelMultiplexer(base.learners.regr)

ps.regr = makeModelMultiplexerParamSet(lrn.regr,
  regr.ksvm = makeParamSet(
    makeNumericParam("C", lower = 0, upper = 100)
  ),
  regr.kknn = makeParamSet(
    makeIntegerParam("k", lower = 1, upper = 10),
    makeIntegerParam("distance", lower = 1, upper = 2)
  ),
  regr.xgboost.mod = makeParamSet(
    makeIntegerParam ("max_depth" , lower = 1, upper = 10),
    makeIntegerParam("nrounds", lower = 1, upper = 1000)
  ),
  regr.glmnet = makeParamSet(
    makeNumericParam("alpha", lower = 0, upper = 1),
    makeNumericParam("s", lower = 0, upper = 10)
  ),
  regr.rpart = makeParamSet(
    makeIntegerParam ("maxdepth" , lower = 1, upper = 6, requires = quote(booster == "gbtree"))
  )
)
@

% =============================================================================
% Motivation for application
% =============================================================================
As one of the main application of the functional complexity  measures, we demonstrate with the wine quality dataset how to optimize performance and interpretability in a multi-object optimization application.


% =============================================================================
% Wine Data
% =============================================================================
\subsubsection{Predicting Wine Quality.}
We used the wine quality dataset \citep{cortez2009modeling}, which contains physicochemical properties like alcohol and residual sugar of \Sexpr{nrow(wine)} white wines.
The goal is to predict the quality of the wineon a scale from 0 to 10, assessed by the median of three blind ratings.
The data is freely available online.

% =============================================================================
% Why like this?
% =============================================================================
\subsubsection{Motivation.}
As \citep{freitas2014comprehensible} points out, it is difficult to know before model what the tradeoffs between interpretability and performance are and suggests multi-objective optimization based on Pareto dominance.
This stands in contrast to apriori selecting an interpretable model class (e.g. decision rules)  and optimizing within this class or exclusively optimizing performance and applying post-hoc interpretations.
We suggest to search over a wide spectrum of different models and hyper parameters, present the set of Pareto optimal models and let the practitioner choose a suitable tradeoff between interpretability and performance.
Since we offer three measures of interpretability, we provide a more in-depth characterization of the model's behavior, allowing practitioners to make more informed decisions (e.g. how much does performance suffer when we use model with interactions?).

% =============================================================================
% Objectives and Models
% =============================================================================
\subsubsection{Objectives and Models.}
We apply the model-based optimization framework \citep{mlrMBO} to find the best model based on the following 4 objectives: number of features used by the model (NF), main effect complexity (MEC), interaction strength (IAS) and the mean absolute error (MAE) of the prediction.
We compare the following models: linear regression model, CART (tuning maximum tree-depth), weighted k-nearest neighbor classifier (tuning number of neighbors k and the distance function), a support vector machine (tuning complexity parameter C), sparse linear models glmnet (tuning alpha and s??), xgboost (tuning the maximum depth and number of rounds).


% =============================================================================
% MBO Setup
% =============================================================================
\subsubsection{Model-based Optimization Setup.}
We use the ParEGO algorithm \citep{knowles2006parego} for the multi-objective optimization.
We use a random forest with \Sexpr{surrogate.ntree} trees as a surrogate model to suggest new model/hyperparameter configurations to test with the confidence bound (automatically chosen lambda) as infill criterion.
The fitness function is the 4-dimensional vector  $(MAE, NF, MEC, IAS)$.
Within the fitness function,the MAE is estimated using \Sexpr{n.folds}-fold cross-validation.
The other measures (NF, MEC, IA) are estimated in-sample.
For the initial sample of models/hyperparameters we take a random Latin Hypercube sample of \Sexpr{n.initial.design}.

% Question: What is the stop criterion? Could not find out with the help files.
% TODO: Write about stop criterion
% Question: Would it makes sense to have a stratified initial design to ensure that we sample each learner for sure?


<<mo-run, dependson=c("mo-data", "mo-learners", "params"), results='hide'>>=
fn = function(x){
  # removes unused params
  x = x[!is.na(x)]
  lrn = setHyperPars(lrn.regr, par.vals = x)
  mae_loss = resample(learner = lrn, show.info = FALSE,
    task = task , resampling = rin ,
    measures = list(mlr::mae))$aggr
  mod = train(lrn, task)
  cat(".")
  pred = Predictor$new(mod, task.dat, y = task$task.desc$target)
  imeasure = FunComplexity$new(pred, max_seg_cat = MAX_SEG_CAT, max_seg_num = MAX_SEG_NUM, epsilon = EPSILON, grid.size = GRID.SIZE)
  cat(".")
  c(round(mae_loss, 2),
    round(imeasure$c_wmean, 1),
    round(1 - imeasure$r2, 2),
    imeasure$n_features)
}

# Setup MBO
resampDescr = makeResampleDesc(method = "CV", iters = n.folds)
rin = makeResampleInstance(resampDescr, task)
obj.fun = makeMultiObjectiveFunction(fn = fn, par.set = ps.regr, n.objectives = 4, has.simple.signature = FALSE)
measure_names = c("MAE", "MEC", "IAS", "NF")
ctrl = makeMBOControl(n.objectives = 4L, y.name = measure_names)
ctrl = setMBOControlInfill(ctrl, crit = infill.criterion)
ctrl = setMBOControlMultiObj(ctrl, method = "parego")
mbo.lrn = makeLearner("regr.randomForest", predict.type = "se", ntree = surrogate.ntree)
mbo.lrn = makeImputeWrapper(mbo.lrn, classes = list(numeric = imputeMax(2), factor = imputeConstant("__miss__")))
design = generateDesign(n = n.initial.design, par.set = ps.regr, fun = lhs::randomLHS, augment = n.augment)

# Start MBO
parallelStartMulticore(cpus = n.cores, show.info = TRUE)
mbo.iml = mbo(fun = obj.fun, design = design, learner = mbo.lrn, control = ctrl)
parallelStop()
saveRDS(mbo.iml, file = "mbo-results.RDS")


pareto.set = rbindlist(lapply(mbo.iml$pareto.set, data.frame))
best.models = data.frame(cbind(round(mbo.iml$pareto.front, 2), pareto.set))

# Remember the index so that models can later be retrieved from mbo object
best.models$index = 1:nrow(best.models)

# Some duplicates, because sometimes paramter changes, but all criteria remain the same.
# Only keep one of those solutions
dups = duplicated(best.models[,c("selected.learner", measure_names)])
best.models = best.models[!dups,]
@

<<mo-results-rename, dependson="mo-run", results = "asis">>=
# mbo.iml = readRDS("paper/mbo-results.RDS")
# Improves the naming of the learners
best.models.print = best.models
colnames(best.models.print) = gsub("regr.", "", colnames(best.models.print), fixed = TRUE)
colnames(best.models.print) = gsub("mod.", "", colnames(best.models.print), fixed = TRUE)
param_cols = setdiff(colnames(best.models.print), c(measure_names, "selected.learner", "index"))
numeric_params = sapply(best.models.print[,param_cols], function(x) is.numeric(x))
numeric_params = names(numeric_params[which(numeric_params)])
best.models.print = data.frame(best.models.print)
best.models.print[,numeric_params] = round(best.models.print[,numeric_params], 2)
best.models.print$selected.learner = gsub("regr.", "", best.models.print$selected.learner, fixed = TRUE)
best.models.print$selected.learner = gsub("xgboost.mod", "xgboost", best.models.print$selected.learner, fixed  = TRUE)

# Shorter description of learner
best.models.print$descr = apply(best.models.print, 1, function(row) {
  lrn_name = row['selected.learner']
  params = row[param_cols]
  params = params[!is.na(params)]
  names(params) = gsub(sprintf("%s.", lrn_name), "", names(params), fixed = TRUE)
  param_string = sprintf("%s:%s", names(params), params)
  param_string = paste(param_string, collapse = ",")
  # When model has no parameters
  param_string = gsub("()", "", param_string, fixed = TRUE)
  sprintf("%s (%s)", lrn_name, param_string)
})


# Table of Pareto set + objective values
tab.cap = "Pareto front of models minimizing mean absolute error (MAE), number of features (NF), main effect complexity (MEC) and interaction strength (IA)."
to.print = best.models.print[c("descr", measure_names)]
rownames(to.print) = NULL
xtable::xtable(to.print, caption = "tab.cap", label = "tab:pareto")


# Compute scaled version of measures
baseline_performance = measureMAE(task.dat[,task$task.desc$target], response = mean(task.dat[,task$task.desc$target]))
best_performance = min(best.models.print$MAE)
best.models.print$sMAE = (best.models.print$MAE - best_performance) / (baseline_performance - best_performance)
best.models.print$sNF  = best.models.print$NF / sum(task$task.desc$n.feat)
best.models.print$sIAS  = best.models.print$IAS / max(best.models.print$IAS)
best.models.print$sMEC  = (best.models.print$MEC - 1) / (max(best.models.print$MEC) - 1)
# Because of featureless learner
best.models.print$sMEC = pmax(0, best.models.print$sMEC)
smeasure_names = c("sMAE", "sMEC", "sIAS", "sNF")
@


% =============================================================================
% Pareto Front Table
% =============================================================================
\subsubsection{Results.}
Table \ref{tab:pareto} shows the set of Pareto optimal models along with their MAE, NF, IAS and MEC measures.
For a more informative visualization, we propose to visualize the main effects along with the measures for a selection of models in Table~\ref{tab:spark-table-multiobj}.
To get an interesting selection of models, we chose XXX models with different tradeoffs between the 4 measures.
First we scaled all measures to the range of $[0,1]$ by scaling each measure with meaningful upper and lower bounds:
$$M_{scaled}(M) = \frac{M - M_{inf}}{M_{sup} - M_{inf}}$$

For MAE, we set $M_{inf}$ to the lowest MAE observed among all models and $M_{sup} = \sum_{i=1}^n|y_i - median(y)|$.
For NF, we set $M_{inf}=0$ and $M_{sup}=p$.
For MEC, we set $M_{inf}=1$ and $M_{sup}$ to the highest observed MEC among all models.
For IAS, we set $M_{inf}=0$ and $M_{sup}$ to the highest observed IAS among all models.


$$\lambda_{MAE} MAE_{scaled} + \lambda_{NF} NF_{scaled} + \lambda_{MEC} MEC_{scaled} + \lambda_{IAS} IAS_{scaled}$$

By choosing values for $\lambda$'s the multi-objective function collapses to a single objective and we can select the model from the Pareto set that optimizes it.
Table~\ref{tab:spark-table-multiobj} shows different tradeoffs.
% =============================================================================
% Best performing model
% =============================================================================

<<spark-table-multiobj, results='asis', dependson=c("mo-run", "mo-results-rename")>>=
single_obj = function(measure_df, lambdas){
  assert_numeric(lambdas, len = 4, null.ok = FALSE)
  assert_data_frame(measure_df, ncols = 4)
  rowSums(as.matrix(measure_df) %*% lambdas)
}

lambda_perf = c(1,0,0,0)
best_perf = which.min(single_obj(best.models.print[smeasure_names], lambda_perf))
lambda_sparse = c(1,0.5,0,0)
best_sparse = which.min(single_obj(best.models.print[smeasure_names], lambda_sparse))
lambda_int = c(0.5,0.5,1,1)
best_int = which.min(single_obj(best.models.print[smeasure_names], lambda_int))
lambda_half = c(3,0.5,1,1)
best_half = which.min(single_obj(best.models.print[smeasure_names], lambda_half))

indices = c(best_perf, best_sparse, best_int, best_int)
mbo_indices = best.models.print[indices,"index"]

tab = get_spark_table(mbo_obj = mbo.iml, indices =  mbo_indices, ylim = c(-1, 1), height = 5, width=7)
colnames(tab) = best.models.print$descr[indices]
# Because I use as.is later with xtable
colnames(tab) = gsub("_","\\_", colnames(tab), fixed = TRUE)
colnames(tab) = gsub("()", "",colnames(tab), fixed = TRUE)
rownames(tab) = c("MAE", "NF", "IA", "AMEC", setdiff(colnames(task.dat), "quality"))

cap = sprintf("A selection of %i models from the Pareto optimal set. From left to right, the models optimize performance only (%s) performance and sparseness (%s), focus on interpretability (%s), more focus on performance (%s).",
  length(indices),
  paste(lambda_perf, collapse = ","),
  paste(best_sparse, collapse = ","),
  paste(lambda_int, collapse = ","),
  paste(lambda_half, collapse = ","))
xtab = xtable(tab)
align(xtab) <- "l|p{2.2cm}p{2.2cm}p{2.2cm}p{2.2cm}"
print.xtable(xtab, sanitize.text.function = as.is)
@

% =============================================================================
% Visualization of Pareto Front
% =============================================================================
\subsubsection{Performance-Interpretability Tradeoff}
The measures allow us to study how model classes and parameter settings trade off interpretebility and performance.
Figure~\ref{fig:perf-interpret-tradeoff} shows each model with hyperparameter configuration from the Pareto set.
We combine the three functional complexity measure and invert it, so that it can be interpreted as "Interpretability", with 3 being the theoretical interpretability maximum:

$$Interpretability = 3 - (NF_{scaled} + IAS_{scaled} + MEC_{scaled})$$

This weights all three (scaled) measures equally.
The theoretical maximal interpretability is 3 for a model predicting a constant value.
The theoretical minimal interpretability is 0 for a model that uses all features and has the highest interaction strength and main effect complexity measure among all Pareto optimal models.

<<perf-interpret-tradeoff, dependson="mo-results-rename", fig.cap="Performance vs. Interpretability tradeoff for predicting wine quality.">>=
ggplot(best.models.print,aes(x = 1 - sMAE, y = 3 - (sNF + sMEC + sIAS))) +
  geom_label(aes(label = descr)) +
  scale_x_continuous("Performance", limits = c(NA, 1)) +
  scale_y_continuous("Interpretability", limits = c(0, 3))
@



% =============================================================================
% =============================================================================
% Further Material
% =============================================================================
% =============================================================================

% Datasets:
%
% Biodegradability
% https://www.openml.org/d/1494
%
% Wind and Solar energy based on wheather
% https://github.com/hugorcf/Renewable-energy-weather/blob/master/renewable.ipynb

<<plot-best-mbo, dependson="mo-results-rename", fig.cap = "Comparing models with different interpretability and performance tradeoffs.", eval = FALSE>>=
#param_cols = setdiff(colnames(best.models), c(measure_names, "selected.learner"))
plot.dat = melt(best.models.print, measure.vars = measure_names)
plot.dat = unique(plot.dat)
p = ggplot(plot.dat) +
  geom_col(aes(x = variable, y = value)) +
  facet_wrap("descr") +
  coord_flip()
print(p)

nudge_y = -0.25
p  = ggplot(mapping = aes(y = variable, x = descr, color = value)) +
  #geom_point(data = max.plot.dat, shape = 1) +
  geom_point(data = plot.dat, size = 15) +
  scale_size_continuous(range = c(0,10), guide = "none") +
  scale_x_discrete("") +
  scale_color_viridis(option = "plasma") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  geom_label(data = plot.dat[plot.dat$variable == "simplicity",],
    aes(label = sprintf("C:%.2f", MEC)), nudge_y = nudge_y, color = "black") +
  geom_label(data = plot.dat[plot.dat$variable == "additivity",],
    aes(label = sprintf("IA:%.2f", IAS)), nudge_y = nudge_y, color = "black") +
  geom_label(data = plot.dat[plot.dat$variable == "sparsity",],
    aes(label = sprintf("P:%.0f", NF)), nudge_y = nudge_y, color = "black") +
  geom_label(data = plot.dat[plot.dat$variable == "performance",],
    aes(label = sprintf("MAE:%.3f", MAE)), nudge_y = nudge_y, color = "black")
print(p)
@


