\Sexpr{set_parent('paper.Rnw')}
\subsection{Measure ALE First Order Approximiation (Interaction)}
\label{sec:interaction}



% =============================================================================
% First order model
% =============================================================================
We propose to measure the level of additivity in the model by calculating the proportion of error for that is already covered by the main effects, when we are predicting the outcome of the model.
Important to note is that error here refers to the error for predicting the full model.

The general idea is that we have three models that are nested from left to right.
Baseline (e.g. mean of y), main effects model, full model.
The training data error decreases from left to right.
For the full model it is zero.

Based on the ALE decomposition, we can define an additive model that is composed of only the main  effects:

$$f_{1st,ALE}(x) = f_0 + f_{1,ALE}(x_1) + \ldots  + f_{p,ALE}(x_p)$$

$f_0$ is estimated using $f_0 = \frac{1}{n}\sum_{i=1}^n x_i$
The individual ALE components $f_{j,ALE}$ can be estimated as described in ALE section.
We can see this main effects model as a prediction model that is nested within the more complex complete model.
As common for linear models, we can analyze how much of the variance comes from certain components, here the main effects estimated with ALE.


% =============================================================================
% PRE / PRL
% =============================================================================
We use the proportionate reduction of error (related to  Proportional Reduction in Expected Loss \citep{cooil1994reliability}) as measure for how well the first-order model predictions approximates $f(x)$.
With some adaptions based on \citep{cooil1994reliability}, we define pre as:

$$pre(\fh, \fale, L) = 1 - \frac{\mathbb{E}(L(\fh, \fale))}{\mathbb{E}(L(\fh, c))}$$

Where $c$ is a constant that optimizes the loss function $L$, which is the mean for the L2 loss, and the median for the L1 loss for example.

If Interaction is 0, we can fully represent how the model works by showing the 1D-ALE plots.
IA can be a better measure than to only look at model parameters.
A decision tree might increase its depth, but interaction might not change or even decrease, because same feature is picked up again, or identical subtrees are fitted.

While the decomoosition of variance does not work with correlated features (1st order correlated with higher order) it should be still ok to just describe the SSE I get when using first order ALE model.


Making no assumptions, but simply describe how much residuals are left after modeling with first order ALE.
Note that our interaction strength measure does not account for the complexity of the interaction, just how much the main effect model leaves to be explained by the interactions.
It's not the depth of the interaction.
E.g. a tree might have a low interaction depth, but a high interaction strength and another tree might have a high interaction depth, but the main effects model already explains a lot.

% =============================================================================
% R2 special case
% =============================================================================
The $R^2$-measure is a special case of the PRL using the squared loss, which we estimate with:

$$R^2 = 1 - \frac{SSE}{SST} = \frac{\sum_{i=1}^n (f(x^{(i)}) - f_{1st,ALE}(x^{(i)}))^2}{\sum_{i=1}^n (f(x^{(i)}) -  \frac{1}{n}\sum_{i=1}^n f(x^{(i)}))^2}$$

Using the R-squared measure, has the interpretation of the variance, so we use this further on and for the examples.
When a model is optimized with a different loss, it might make sense to measure the PRL also with another loss to quantify how appropriate an approximation with a first order ALE model is.

% =============================================================================
% pre algorithm
% =============================================================================
The estimation of the pre of the main effects ALE model is described with the following algorithm.
\begin{algorithm}
\caption{Estimate PRL of ALE main effects model}\label{algo:ale1st}
\begin{enumerate}
\item Given: model $\fh$, dataset $\D$, loss function $L$
\item Estimate $\falej$ for all features $j\in\{1,\ldots,p\}$
\item Estimate $\fzero = \sum_{i=1}^n \fh(\xi)$
\item Define: $\fale(x) = \fzero + \sum_{j=1}^p \falej(x)$
\item Calculate $PRL(L, \fh, \fale)$
\end{enumerate}
\end{algorithm}



% =============================================================================
% Further approaches
% =============================================================================
There are other ways to estimate the additivity of a model.
One is via Sobol interaction strength \citep{sobol1993sensitivity} or Shapley \citep{owen2014sobol}.
We decided against Sobol and Shapley since they were computationally expensive and varied a lot from run to run.
Additonally it makes more sense to look at the PRL/PRE of the main effects of the decomposition, since then it is coherent with the plots that are shown and with the complexity, which is presented next.
