\Sexpr{set_parent('paper.Rnw')}
\section{Optimization of Performance and Interpretability}
\label{sec:multiobj}

<<params>>=
### Set parameters for application
DEBUG_MODE = FALSE 
set.seed(42)
# Parameters for functional complexity
MAX_SEG_CAT = 9
MAX_SEG_NUM = 5
EPSILON = 0.05
GRID.SIZE = 200

# Parameters for mbo
n.folds = 5
# Number of iterations should be at least 20 times number of hyperparameters
max.iters = 350
@

<<mo-data>>=
wine = read.csv(here("./data/winequalityN.csv"))
wine = na.omit(wine)

# only use white wine
wine = wine[wine$type == "white", ]
wine$type = NULL

task = makeRegrTask(data = wine, target = "quality")
task.dat = getTaskData(task)
@

<<mo-learners>>=
base.learners.regr = list(
  makeLearner("regr.ksvm"),
  makeLearner("regr.glmnet"),
  makeLearner("regr.xgboost.mod"),
  makeLearner("regr.rpart"),
  makeLearner("regr.ranger"),
  makeLearner("regr.gamboost")
)

lrn.regr = makeModelMultiplexer(base.learners.regr)

ps.regr = makeModelMultiplexerParamSet(lrn.regr,
  regr.ksvm = makeParamSet(
    makeNumericParam("C", lower = -15, upper = 15, trafo = function(x) 2^x),
    makeNumericParam("sigma", lower = -15, upper = 15, trafo = function(x) 2^x)
  ),
  regr.xgboost.mod = makeParamSet(
    makeIntegerParam ("max_depth" , lower = 1, upper = 10),
    makeIntegerParam("nrounds", lower = 1, upper = 1000)
  ),
  regr.glmnet = makeParamSet(
    makeNumericParam("alpha", lower = 0, upper = 1),
    makeNumericParam("lambda", lower = -15, upper = 15, trafo = function(x) 2^x)
  ),
  regr.rpart = makeParamSet(
    makeIntegerParam("maxdepth", lower = 1, upper = 20),
    makeNumericParam("cp", lower = 0, upper = 1)
  ),
  regr.gamboost = makeParamSet(
    makeIntegerParam("mstop", lower = 1, upper = 500)
  ),
  regr.ranger = makeParamSet(
    makeIntegerParam("mtry", lower = 1, upper = 11)
  )
)

@

% =============================================================================
% Motivation for application
% =============================================================================
As one of the main applications of the proposed interpretability measures, we demonstrate model selection for performance and interpretability in a multi-objective optimization approach.

% =============================================================================
% Wine Data
% =============================================================================
\subsubsection{Predicting Wine Quality.}
We used the wine quality dataset \citep{cortez2009modeling} which contains physical-chemical properties such as alcohol and residual sugar of \Sexpr{nrow(wine)} white wines.
The goal was to predict wine quality on a scale of 0 to 10, assessed by the median of three blind ratings.

% =============================================================================
% Why like this?
% =============================================================================
\subsubsection{Motivation.}
As \citep{freitas2014comprehensible} emphasizes, it is difficult to know the desired compromise between interpretability and performance before modeling the data and suggests multi-objective optimization.
This stands in contrast to a priori selecting an interpretable model class (e.g. decision rules) and optimizing within this class or exclusively optimizing performance and applying post-hoc interpretations.
We suggest searching over a wide spectrum of model classes and hyper parameter settings, presenting the set of Pareto optimal models, and allowing the practitioner to choose a suitable compromise between interpretability and performance.
The three measures of interpretability provide a detailed characterization of machine learning models, which enables making informed decisions (e.g. how much does performance suffer if we use a model without interactions?).

% =============================================================================
% Objectives and Models
% =============================================================================
\subsubsection{Optimization Setup.}
We used the mlrMBO model-based optimization framework \citep{horn2016multi} to find the best model based on four objectives: number of features used by the model (NF), main effect complexity (MEC), interaction strength (IAS) and the cross-validated mean absolute error (MAE).
We optimized over the space of following model classes (and hyperparameters): \textbf{CART} (maximum tree-depth and pruning cp), \textbf{s}upport \textbf{v}ector \textbf{m}achine (cost C and sigma), \textbf{elastic net} regression (regularization alpha and penalization lambda), \textbf{g}radient \textbf{b}oosted \textbf{t}rees (maximum depth, number of iterations), gradient \textbf{boost}ed \textbf{g}eneralized \textbf{a}dditive \textbf{m}odel (number of iterations) and \textbf{r}andom \textbf{f}orest (mtry).


\subsubsection{Model-based Optimization Setup.}
We used the ParEGO algorithm \citep{knowles2006parego} for multi-objective optimization.
Within the fitness function, the MAE was estimated using \Sexpr{n.folds}-fold cross-validation and the other measures (NF, MEC, IAS) were estimated using all data instances.
We set the number of iterations of ParEGO to \Sexpr{max.iters}.
For all other parameters of the model-based, multi-objective optimization, we relied on the sensitive defaults provided by \citep{mlrMBO}.

<<mo-setup, dependson=c("params", "mo-learners", "mo-data")>>=
fn = function(x){
  # removes unused params
  x = x[!is.na(x)]
  lrn = setHyperPars(lrn.regr, par.vals = x)
  mae_loss = resample(learner = lrn, show.info = FALSE,
    task = task , resampling = rin ,
    measures = list(mlr::mae))$aggr
  mod = train(lrn, task)
  pred = Predictor$new(mod, task.dat, y = task$task.desc$target)
  imeasure = FunComplexity$new(pred, max_seg_cat = MAX_SEG_CAT, max_seg_num = MAX_SEG_NUM, epsilon = EPSILON, grid.size = GRID.SIZE)
  cat(".")
  c(round(mae_loss, 2),
    round(imeasure$c_wmean, 1),
    round(1 - imeasure$r2, 2),
    imeasure$n_features)
}

# Setup MBO
resampDescr = makeResampleDesc(method = "CV", iters = n.folds)
rin = makeResampleInstance(resampDescr, task)
obj.fun = makeMultiObjectiveFunction(fn = fn, par.set = ps.regr, n.objectives = 4, has.simple.signature = FALSE)
measure_names = c("MAE", "MEC", "IAS", "NF")
ctrl = makeMBOControl(n.objectives = 4L, y.name = measure_names)
ctrl = setMBOControlMultiObj(ctrl, method = "parego")
ctrl = setMBOControlTermination(ctrl, iters = max.iters)
@



<<mo-run, dependson=c("mo-setup"), results='hide', eval = TRUE>>=
# Start MBO
mbo.iml = mbo(fun = obj.fun, control = ctrl)
saveRDS(mbo.iml, file = "mbo-results.RDS")
@

<<mo-results, dependson="mo-run">>=
mbo.iml = readRDS("mbo-results.RDS")
pareto.set = rbindlist(lapply(mbo.iml$pareto.set, data.frame))
best.models = data.frame(cbind(round(mbo.iml$pareto.front, 2), pareto.set))

# Remember the index so that models can later be retrieved from mbo object
best.models$index = 1:nrow(best.models)

# Some duplicates, because sometimes paramter changes, but all criteria remain the same.
# Only keep one of those solutions
dups = duplicated(best.models[,c("selected.learner", measure_names)])
best.models = best.models[!dups,]

# We are not interested in constant models
best.models = best.models[best.models$MEC != 0,]
@


<<mo-results-rename, dependson="mo-results", results = "asis">>=
#Improves the naming of the learners
best.models.print = best.models
log_params = c("regr.ksvm.C", "regr.ksvm.sigma", "regr.glmnet.lambda")
best.models.print[, log_params] = 2^best.models.print[, log_params]
colnames(best.models.print) = gsub("regr.", "", colnames(best.models.print), fixed = TRUE)
colnames(best.models.print) = gsub("mod.", "", colnames(best.models.print), fixed = TRUE)
param_cols = setdiff(colnames(best.models.print), c(measure_names, "selected.learner", "index"))
numeric_params = sapply(best.models.print[,param_cols], function(x) is.numeric(x))
numeric_params = names(numeric_params[which(numeric_params)])
best.models.print = data.frame(best.models.print)
best.models.print[,numeric_params] = round(best.models.print[,numeric_params], 4)
best.models.print$selected.learner = gsub("regr.", "", best.models.print$selected.learner, fixed = TRUE)
best.models.print$selected.learner = gsub("xgboost.mod", "xgboost", best.models.print$selected.learner, fixed  = TRUE)

lrn_model_map = c("xgboost" = "gbt", "rpart" = "CART", "featureless" = "median", 
		  "ksvm" = "svm", "gamboost" = "gamb", "glmnet" = "elastic net",
		  "ranger" = "rf")

best.models.print$model = lrn_model_map[best.models.print$selected.learner]


# Shorter description of learner
best.models.print$descr = apply(best.models.print, 1, function(row) {
  lrn_name =   params = row[param_cols]
  params = params[!is.na(params)]
  names(params) = gsub(sprintf("%s.", row['selected.learner']), "", names(params), fixed = TRUE)
  param_string = sprintf("%s:%s", names(params), params)
  param_string = paste(param_string, collapse = ",")
  # When model has no parameters
  param_string = gsub("()", "", param_string, fixed = TRUE)
  sprintf("%s (%s)", row["model"], param_string)
})


# Table of Pareto set + objective values
tab.cap = "Pareto front of models minimizing mean absolute error (MAE), number of features (NF), main effect complexity (MEC) and interaction strength (IAS)."
to.print = best.models.print[order(best.models.print$MAE), c("descr", measure_names)]
rownames(to.print) = NULL
baseline_performance = measureMAE(task.dat[,task$task.desc$target], response = mean(task.dat[,task$task.desc$target]))
to.print = rbind(to.print, data.frame("descr" = "median", "MAE" = baseline_performance, "MEC" = 0, "IAS" = 0, "NF"=0))
colnames(to.print)[1] = "Model (Hyperparameters)"
tab = xtable::xtable(to.print, caption = tab.cap, label = "tab:pareto")
print(tab, caption.placement = "top")

# Compute scaled version of measures
best_performance = min(best.models.print$MAE)
best.models.print$sMAE = (best.models.print$MAE - best_performance) / (baseline_performance - best_performance)
best.models.print$sNF  = best.models.print$NF / sum(task$task.desc$n.feat)
best.models.print$sIAS  = best.models.print$IAS / max(best.models.print$IAS)
best.models.print$sMEC  = best.models.print$MEC / max(best.models.print$MEC)
# Because of featureless learner
best.models.print$sMEC = pmax(0, best.models.print$sMEC)
smeasure_names = c("sMAE", "sMEC", "sIAS", "sNF")

# Best models first
best.models.print = best.models.print[order(best.models.print$MAE),]

# Model description with reference to the table
best.models.print$descr.ref = sprintf("%s [row %i]", best.models.print$model, 1:nrow(best.models.print))
@


% =============================================================================
% Pareto Front Table
% =============================================================================
\subsubsection{Results.}
Table \ref{tab:pareto} shows the set of Pareto-optimal models along with their MAE, NF, IAS and MEC.
If two models from the same class with different hyperparameter values had exactly the same MAE, NF, IAS and MEC, we kept only one and dropped the others.
We also dropped constant models (e.g. elastic net regression with strong penalization), with exception of the median model.
For a more informative visualization, we propose to visualize the main effects together with the measures in Table~\ref{tab:spark-table-multiobj}.
The four selected models show different trade-offs between the four measures.
% =============================================================================
% Best performing model
% =============================================================================

<<spark-table-multiobj, results='asis', dependson=c("mo-run", "mo-results-rename")>>=
single_obj = function(measure_df, lambdas){
  assert_numeric(lambdas, len = 4, null.ok = FALSE)
  assert_data_frame(measure_df, ncols = 4)
  rowSums(as.matrix(measure_df) %*% lambdas)
}

measure_df = best.models.print[smeasure_names]

best_perf = which.min(best.models.print$MAE)
# MAE-best model with bounded MEC
best_2 = which(best.models.print$MEC < 1.01)[1]
# MAE-best model that models few interactions
best_3 = which(best.models.print$IA <= 0.1)[1]
# Best model that does not use all features
best_4 = which(best.models.print$NF <=  7)[1] 

indices = c(best_perf, best_2, best_3, best_4)
mbo_indices = best.models.print[indices,"index"]
tab = get_spark_table(mbo_obj = mbo.iml, indices =  mbo_indices, height = 3, width=10, log_params = log_params)
colnames(tab) = best.models.print$descr.ref[indices]
# Because I use as.is later with xtable
colnames(tab) = gsub("_","\\_", colnames(tab), fixed = TRUE)
colnames(tab) = gsub("\\(\\)", "",colnames(tab), fixed = TRUE)
rownames(tab) = c("MAE", "MEC", "IAS", "NF", setdiff(colnames(task.dat), "quality"))

cap = sprintf("A selection of four models from the Pareto optimal set, along with their ALE main effect curves. From left to right, the columns show models with 1) lowest MAE, 2) lowest MAE when $MEC=1$, 3) lowest MAE when $IAS =\\leq 0.1$, and 4) lowest MAE with $NF \\leq 7$. Corresponding hyperparameters can be found in Table~\\ref{tab:pareto}.", length(indices))
xtab = xtable(tab, caption = cap, label = "tab:spark-table-multiobj")
align(xtab) <- "l|p{2.2cm}p{2.2cm}p{2.2cm}p{2.2cm}"
print.xtable(xtab, sanitize.text.function = as.is, caption.placement = "top")
@

% =============================================================================
% Visualization of Pareto Front
% =============================================================================
\subsubsection{Performance-Interpretability Trade-off.}
The complexity measures allow to study the trade-off between interpretability and performance across different model classes and hyperparameter settings.
We mapped each measure to the interval $[0,1]$ by scaling each measure $M$ with meaningful upper and lower bounds:
%
$$M_{scaled}(M) = \frac{M - M_{inf}}{M_{sup} - M_{inf}}$$
%
For MAE, we set $M_{inf}$ to the lowest observed MAE of all models and $M_{sup} = \frac{1}{n}\sum_{i=1}^n|\yi - median(y)|$.
For NF, we set $M_{inf}=0$ and $M_{sup}=p$.
For MEC, we set $M_{inf}=0$ and $M_{sup}$ to the highest observed MEC of all models.
For IAS, we set $M_{inf}=0$ and $M_{sup}$ to the highest observed IAS of all models.
To combine the three measures in a single dimension, we mapped interpretability ad-hoc as: $Interpretability = 3 - (NF_{scaled} + IAS_{scaled} + MEC_{scaled})$.
This weights all three (scaled) measures equally.
The maximum interpretability is 3 for the constant model that always predicts the median wine quality.
The theoretical minimum interpretability is 0 for a model that uses all features and has the highest interaction strength and highest main effect complexity measure among all Pareto optimal models.
Figure~\ref{fig:perf-interpret-tradeoff} maps each model and hyperparameter configuration from the Pareto set to the performance / interpretability space.

<<perf-interpret-tradeoff, dependson="mo-results-rename", fig.cap="Performance vs. interpretability tradeoff for predicting wine quality. Corresponding hyperparameters and measures are shown in Table~\\ref{tab:pareto}.",  fig.height=7, fig.width=12, fig.align="center", out.height="7cm", out.width="12cm">>=
plot.dat = best.models.print[c("sMAE", "sNF", "sMEC", "sIAS", "descr.ref")]
plot.dat = rbind(plot.dat, data.frame(sMAE = 1, sNF = 0, sMEC = 0, sIAS = 0, descr.ref = "median"))
ggplot(plot.dat,aes(x = 1 - sMAE, y = 3 - (sNF + sMEC + sIAS))) +
  geom_point() + 
  geom_text_repel(aes(label = descr.ref), size = 6) +
  scale_x_continuous("Performance", limits = c(NA, 1)) +
  scale_y_continuous("Interpretability", limits = c(0, 3))
@



% =============================================================================
% =============================================================================
% Further Material
% =============================================================================
% =============================================================================

<<plot-best-mbo, dependson="mo-results-rename", fig.cap = "Comparing models with different interpretability and performance tradeoffs.", eval = FALSE>>=
#param_cols = setdiff(colnames(best.models), c(measure_names, "selected.learner"))
plot.dat = melt(best.models.print, measure.vars = measure_names)
plot.dat = unique(plot.dat)
p = ggplot(plot.dat) +
  geom_col(aes(x = variable, y = value)) +
  facet_wrap("descr") +
  coord_flip()
print(p)

nudge_y = -0.25
p  = ggplot(mapping = aes(y = variable, x = descr, color = value)) +
  #geom_point(data = max.plot.dat, shape = 1) +
  geom_point(data = plot.dat, size = 15) +
  scale_size_continuous(range = c(0,10), guide = "none") +
  scale_x_discrete("") +
  scale_color_viridis(option = "plasma") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  geom_label(data = plot.dat[plot.dat$variable == "simplicity",],
    aes(label = sprintf("C:%.2f", MEC)), nudge_y = nudge_y, color = "black") +
  geom_label(data = plot.dat[plot.dat$variable == "additivity",],
    aes(label = sprintf("IA:%.2f", IAS)), nudge_y = nudge_y, color = "black") +
  geom_label(data = plot.dat[plot.dat$variable == "sparsity",],
    aes(label = sprintf("P:%.0f", NF)), nudge_y = nudge_y, color = "black") +
  geom_label(data = plot.dat[plot.dat$variable == "performance",],
    aes(label = sprintf("MAE:%.3f", MAE)), nudge_y = nudge_y, color = "black")
print(p)
@


