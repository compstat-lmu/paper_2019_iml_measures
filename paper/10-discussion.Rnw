\Sexpr{set_parent('paper.Rnw')}
\section{Discussion}
\label{sec:discussion}


% =============================================================================
% Key findings
% =============================================================================
In this paper, we proposed three model-agnostic measures of prediction function complexity based on functional decomposition: number of features, interaction strength and main effect complexity.
The measures allow model comparison across different model classes due to their model-agnostic nature.
We argued that minimizing the number of features, interaction strength and main effect complexity improves the post-hoc interpretation.
We demonstrated that these measures can be explicitly optimized with multi-objective optimization, to make the trade-off between interpretability and performance explicit and allow practitioners to select a model that is optimal for their application.


% =============================================================================
% Strengths and Limitations
% =============================================================================
\subsubsection{Strengths.}
Our proposed measures can be applied for many models since they are model-agnostic, work for regression and binary classification (based on probabilities) and work for categorical and numerical features.
\subsubsection{Limitations.}
The way we decomposed the prediction function and defined the complexity measures will not be suitable for every situation.
For example all higher order effects are combined into a single interaction strength measure, but we don't distinguish whether a model models only a single 2-way interaction or more complex ones.
However, estimating 2nd-order ALE can fix this problem to some degree.
Also the main effect complexity measure is limited, because we only regard linear segments, but don't check for seasonal components or other structures. 
Furthermore, the complexity measures quantify the prediction function from outside, ignoring the way a model is represented.
While this allows these measures to be model-agnostic, which is a strength, it also ignores how the model can be represented and how well humans can understand it.

% =============================================================================
% What's next
% =============================================================================
\subsubsection{The bigger picture.}
The proposed measures do not all-encompassingly define interpretability, but are a step forward in providing quantifiable properties of machine learning models towards more rigorous science in interpretable machine learning, as \citep{doshi2017towards,Lipton2016} urge.
We believe that interpretability can't be measured with a single number, but is a high-dimensional concept (sparsity, additivity, fidelity, human simulatability, ...) and we need many ways to measure interpretability.
An example for such a measure that complements our work is \citep{friedler2019assessing} who propose the number of computational steps necessary for a human to emulate the model prediction as measure for interpretability.
This matches \citep{rudin2018please} notion that interpretability depends on the application and the audience and we think this can be seen as putting different weights on the different interpretability dimension.
In some situations we might prefer sparseness and a lack of interactions, in other it's important that we can represent the model as a decision list.
As \citep{doshi2017towards} mentions this quantification has to happen on different levels: on the functional level (what we did), with humans and with experts.

We believe that a plethora of measurements for interpretability are necessary to solve the lack of definition of interpretability.
This allows researchers to make quantified, verifyable claims when proposing "interpretable" models or methods.

% =============================================================================
% Implementation Note
% =============================================================================
\subsubsection{Implementation.}
The source code is available along with the paper on Github: LINK.
The measure are implemented in the iml package \citep{iml2018} which is available on CRAN.
All examples are done with mlr package \citep{JMLR:v17:15-066} in R \citep{r2018}.
% =============================================================================
% Funding
% =============================================================================
\subsubsection{Funding.}
This work is funded by the Bavarian State Ministry of Science and the Arts in the framework of the Centre Digitisation.Bavaria (ZD.B)<



% =============================================================================
% =============================================================================
% Further material
% =============================================================================
% =============================================================================

% =============================================================================
% REmarks
% =============================================================================
% The approach we take (functional decomposition) is flexible enough to adapt to different desideratea (e.g. favor different functional forms over others).
% All the three measures would also work with any other decomposition like fANOVA or PDP.
% It would even work with nonsensical decomposition, e.g.
% true model for two featuresis $\fh(x) = x_1$, but we decompose as $f_{nonsense}(x) = x_1 + x_2 + Rest$ and we show plots of $x_1$ and $x_2$.
% Complexity measure will not be so meaningful, but n.features would remain the same, and the R squared measure would be better for the first decomposition.
% As such the measures (IA and C) are intertwined between the model and the decomposition we choose, which makes sense.
% Problem with surrogate model: might approximate well, but not perfectly describe the model behaviour.
% e.g. x1, x2 strongly correlated, black box uses only x1.
% surrogate model uses x2.
% then perfect fidelity, but not true effects.
% All three measures would also work with surrogate models, because we can show how big their $R^2$ is for predicting the black box predictions and we can measure the complexity of the surrogate by using it's own C.
% For n.features we have now two options: we could use number of features in black box model or number of features in the surrogate model.
% What if the decomposition is bad?
% Well, for ALE we can show that it's a desirable decomposition.
% But for any other, the $R^2$ will be very bad if the approximation is unfaithful.

% =============================================================================
% Areas to improve
% =============================================================================
% The proposed measure for the main effect complexity could be improved by considering shapes different from linear segments.
% For example quadratic components or seasonal components, which make sense when the feature is time, e.g. years and the main effect captures some seasonal effect.

%Problems:
%- Unclear how many intervals
%- Weighted by data sensity (can be unintuitive when looking at plot) or all plot point same weight (probably very wrong). Solution: Make clear in ALE plot where most data is with rug or alpha.
