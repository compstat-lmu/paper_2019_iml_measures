\Sexpr{set_parent('paper.Rnw')}
\section{Functional complexity}
\label{sec:measures}

% =============================================================================
% General idea in words
% =============================================================================
In this section, we motivate complexity measures based on the functional decomposition: number of features (NF) used (Section~\ref{sec:nfeatures}), interaction strength (IA) (Section~\ref{sec:interaction} and the average main effect complexity (MEC) (Section \ref{sec:curve}.

% =============================================================================
% Decomposition and Simplification
% ============================================================================
Based on Equation~\ref{eqn:decomp}, we decompose the prediction function into a constant (estimated as $f_0 = \frac{1}{n}\sum_{i=1}^n \hat{f}(\xi)$), plus the main effects (estimate with ALE), and a rest containing interactions.

\begin{eqnarray} f(x) % = &\overbrace{f_0}^\text{Intercept} + \overbrace{\sum_{j=1}^p f_j(x_j)}^\text{1st order effects} + \overbrace{\sum_{S \subseteq \{1,\ldots,p\},|S| \geq 2} f_{S}(x_S)}^{\text{Higher order effects}} \\
=&\underbrace{f_0 + \sum_{j=1}^p \overbrace{\falej(x_j)}^\text{AMEC: How complex?} + \overbrace{IA(x)}^{\text{IA: Interaction strength?}}}_{\text{NF: How many features were used}}
\end{eqnarray}
The arrangement of components emphasizes the prediction function as the sum of an additive main effect model and an interactions rest.
In isolation from the interactions, the main effect model constitutes a prediction function and we can analyze how well it already approximates $f$, which is the idea behind the interaction measure IAS.
The average main effect complexity (AMEC) tries to capture how many parameters we need to describe the one-dimensional main effects on average.

% =============================================================================
% Why minimize proposed measures?
% =============================================================================
%When the three measures are minimized, the following improvements of interpretability will be reached.
%Minimizing the number of features to be used directly improves the sparsity of the model.
%The less features are used, the less plots have to be looked at and the less numbers are needed to describe e.g. the feature importance and so on.
%Minimizing the strength of interactions increases how much of the prediction variance the main effects explain.
%If the interaction measure is zero, the ALE plots will explain all of the models variance.
%Minimizing the complexity of the first order effects ensures that we need less parameters to (approximately) describe the main effects.
%A complexity of 1 means that we only need a single number to describe the relationship.




% -----------------------------------------------------------------------------
% Material
% -----------------------------------------------------------------------------
%These measures quantify the complexity of a machine learning model from an external view onto the shape of the prediction function and ignores the actual representation of the model (e.g. tree structure).


% We do the following approximation:
%
% \begin{eqnarray*}
% f(x)  =& \overbrace{f_0}^\text{Intercept} + \overbrace{\sum_{j=1}^p f_j(x_j)}^\text{1st order effects} + \overbrace{\sum_{j\neq k}^p f_{jk}(x_{jk})}^\text{2nd order effects} + \ldots + \overbrace{f_{1,\ldots,p}(x_{1,\ldots,p})}^\text{p-th order effect}\\
%      =& f_0 + \sum_{j=1}^{p} f_{j}(x_j) + \sum_{S \subseteq \{1,\ldots,p\}} f_{S}(x_S)\\
%      =& \bar{f}(x) + \sum_{j=1}^{p} f_{j,ALE}(x_j)  + IA(x) \\
%      =& \bar{f}(x) + \sum_{j=1}^{p} (\tilde{f}_{j,ALE}(x_j) + \epsilon_{j}(x_j)) + IA(x) \\
% \end{eqnarray*}
%
% $\tilde{f}_{j,ALE}(x_j) $ is the approximation of the j-th main effect.\\
% $\epsilon_{j}(x_j)$ is the approximiation error $f_{j,ALE}(x_j) - \tilde{f}_{j,ALE}(x_j)$\\
% $IA(x)$ the interaction terms.\\


% \citep{murdoch2019interpretable} defines the following desiderata for an interpretability measure:
% Accuracy
% \begin{itemize}
% \item Accuracy (of interpretation method), which matches that we look at R squared. Predictive accuracy is measured as usual. Descriptive accuracy is measured with novel measures
% \item Relevancy: Show only relevant information. With our measures we can decide which plots to show. Remove when effect is zero. Also we can measure variance of each of the 1st order components and only show the most relevant ones.
% \item Sparsity: Directly optimized with our measures
% \item Simulatability: Can human internally simulate and reason about
% \item Modularity: Can model parts be interpreted independently? Interaction measure allows us to determine how independently we can analyze the individual features with their ALE plots
% \item They also say: "Moreover, it is unclear if any of the current interpretation forms can fully capture a modelâ€™s behaviour, or if a new format altogether is needed. How to close that gap, while producing outputs relevant to a particular audience/problem, is an open problem."
% \end{itemize}
%
% We claim that optimizing the two measures we will propose will improve those desiderata:
% Decreasing interactions will improve the accuracy of the view of ALE first order plots.
% modularity is achieved by doing a decomposition.
% With interaction measure you can see how much the first order ALE plots of the variance already explain.


