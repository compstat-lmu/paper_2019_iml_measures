\Sexpr{set_parent('paper.Rnw')}
\section{Interpretability measures}
\label{sec:measures}


With a decomposition, we can identify individual parts of the prediction function and quantify there complexity.
Goal: Measure the complexity of the model via the decomposition

We propose the following view of the decomposition:

$$f(x) = \underbrace{f_0}_{mean} + \underbrace{f_{1, ALE}(x_1) + \ldots f_{p, ALE}(x_p)}_\text{1st order effects} + \underbrace{\sum_{S \subseteq \{1,\ldots,p\},|S| \geq 2} f_{S, ALE}(x_S)}_{\text{Higher order effects}}$$

Now we have to parts:
\begin{itemize}
\item Main effects
\item 2nd and higher order effects (interactions)
\end{itemize}

The main effects can be arbitrarily complex one-dimensional functions.
But we can always visualize them easily with ALE plots.
And then we can try to quantify how complex those look.
The second part is tougher to visualize or to grasp, as it contains arbitrary higher order interactions.
We propose to simply measure how much of the variance of the predictions can be attributed to the whole sum of interactions.

This decomposition allows us to explain a model by plotting all of its main effects and quantifying how much we have (or have not) captured of the variance of the predictions.

We propose two measures to capture both parts of this decomposition:

\begin{itemize}
\item LinSeg epsilon approximation to quantify the total complexity of the main effects model.
\item Variance explained by the interactions.
\end{itemize}

The approach we take (functional decomposition) is flexible enough to adapt to different desideratea (e.g. favor different functional forms over others).

We do the following approximation:

\begin{eqnarray*}
f(x)  =& \overbrace{f_0}^\text{Intercept} + \overbrace{\sum_{j=1}^p f_j(x_j)}^\text{1st order effects} + \overbrace{\sum_{j\neq k}^p f_{jk}(x_{jk})}^\text{2nd order effects} + \ldots + \overbrace{f_{1,\ldots,p}(x_{1,\ldots,p})}^\text{p-th order effect}\\
     =& f_0 + \sum_{j=1}^{p} f_{j}(x_j) + \sum_{S \subseteq \{1,\ldots,p\}} f_{S}(x_S)\\
     =& \bar{f}(x) + \sum_{j=1}^{p} f_{j,ALE}(x_j)  + IA(x) \\
     =& \bar{f}(x) + \sum_{j=1}^{p} (\tilde{f}_{j,ALE}(x_j) + \epsilon_{j}(x_j)) + IA(x) \\
\end{eqnarray*}

$\tilde{f}_{j,ALE}(x_j) $ is the approximation of the j-th main effect.\\
$\epsilon_{j}(x_j)$ is the approximiation error $f_{j,ALE}(x_j) - \tilde{f}_{j,ALE}(x_j)$\\
$IA(x)$ the interaction terms.\\


Paper \citep{murdoch2019interpretable} says that desiderata are
\begin{itemize}
\item Accuracy (of interpretation method), which matches that we look at R squared. Predictive accuracy is measured as usual. Descriptive accuracy is measured with novel measures
\item Relevancy: Show only relevant information. With our measures we can decide which plots to show. Remove when effect is zero. Also we can measure variance of each of the 1st order components and only show the most relevant ones.
\item Sparsity: Directly optimized with our measures
\item Simulatability: Can human internally simulate and reason about
\item Modularity: Can model parts be interpreted independently? Interaction measure allows us to determine how independently we can analyze the individual features with their ALE plots
\item They also say: "Moreover, it is unclear if any of the current interpretation forms can fully capture a modelâ€™s behaviour, or if a new format altogether is needed. How to close that gap, while producing outputs relevant to a particular audience/problem, is an open problem."
\end{itemize}

We claim that optimizing the two measures we will propose will improve those desiderata:
Decreasing interactions will improve the accuracy of the view of ALE first order plots.
modularity is achieved by doing a decomposition.
With interaction measure you can see how much the first order ALE plots of the variance already explain.


Desiderata that we define:
\begin{itemize}
\item Information should be contained in as few plots as possible.
\item We prefer marginal relationships between features and prediction that can be explained with few parameters
\item Plots and summary statistics should show as much information as possible of the black box
\end{itemize}




All the three measures would also work with any other decomposition like fANOVA or PDP.
It would even work with nonsensical decomposition, e.g.
true model for two featuresis $\fh(x) = x_1$, but we decompose as $f_{nonsense}(x) = x_1 + x_2 + Rest$ and we show plots of $x_1$ and $x_2$.
Complexity measure will not be so meaningful, but n.features would remain the same, and the R squared measure would be better for the first decomposition.
As such the measures (IA and C) are intertwined between the model and the decomposition we choose, which makes sense.


All three measures would also work with surrogate models, because we can show how big their $R^2$ is for predicting the black box predictions and we can measure the complexity of the surrogate by using it's own C.
For n.features we have now two options: we could use number of features in black box model or number of features in the surrogate model.
