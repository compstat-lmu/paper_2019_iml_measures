\Sexpr{set_parent('paper.Rnw')}
\subsection{Measure ALE First Order Complexity}
\label{sec:curve}

We propose a measure that, for each of the main effects, approximates the ALE curve with linear segments and adds up the parameters needed if we were to model all those segments in a linear regression model.

When complexity measure is minimized, we get linear effects or binary split.

Recipe:


\begin{algorithm}
\caption{Estimate ALE curve complexity}\label{algo:ale-complexity}
\begin{enumerate}
\item Input: Model $\fh$, PRL-bound $\epsilon$, complexity limit $max_{seg,j}$
\item Output: Weighted average complexity $C$
\item For each $j \in \pset$ do:
\begin{enumerate}
\item Estimate ALE $\falej$
\item If $\falej(x_j) = 0$ for all $x$, stop and return $\galej(x) = 0$
\item Set $K=0$ and $\galej = 0$
\item While $K < max_{seg}$ AND $PRL(\falej, \galej, L) < 1- \epsilon$:
\begin{itemize}
\item $\hat{z}_1,\ldots, \hat{z}_K = argmin_{z_1,\ldots,z_K} \sum_{k=1}^{K+1} \sum_{\xi\in Z_k}\left(\falej(\xij) - (\beta_{j0}^k + \beta_{j1}^k\xij)\right)^2$ where $Z_1 = [min(\xj), z_1], \ldots, Z_k = [z_{k-1}, z_k],\ldots,Z_{K+1} = [z_K,max(\xj)]$ and $\beta_{j0}^k$ and $\beta_{j1}^k$ are the least square estimates. If $K=0$, then $Z_1 = [min(\xj), max(\xj)]$. In case of categorical, $\beta_j$ is set to zero. Solve optimization with Generalized Simulated annealing (package GenSA).
\item $\galej(\xj) = \sum_{k=1}^{K+1} \I_{\xi\in Z_k}\left(\hat{\beta}_0^k + \hat{\beta}_j^k\xij\right)$. For categorical, $\hat{\beta}_j = 0$
\end{itemize}
\item For numeric: Set slopes to zero. For each segment $k \in 1,\ldots,K$:
\begin{enumerate}
\item Set $\beta_{j0}^k = \frac{1}{n_k}\sum_{\xi\in Z_k}\left \falej(\xij)$ and $\beta_{j1}^k$ = 0
\item If still $R^2 > 1 - \epsilon$, overwrite both $\beta$.
\end{enumerate}
\item $C_j = K + \sum_{k=1}^K \mathbb{I}_{\beta_k > 0}$ for numerical; $C_j = K$ for categorical
\item ALE variance $V_j = \frac{1}{n}\sum_{i=1}^n \hat{f}^{ALE}_j(x^{(i)})$
\end{enumerate}
\item $C = \frac{1}{\sum_{j=1}^pV_j}\sum_{j=1}^p V_jc_j$.
\end{enumerate}
\end{algorithm}




Optimization problem for numerical features:

Given a fixed number $K$ of intervals $I_k$ defined by K-1 break points $z_k$: $[\min(x_j), z_1], ]z_1, z_2], \ldots ] z_k,\max(x_j)]$

$$\argmin{z_1,\ldots,z_{K-1}} \sum_{k=1}{K}\sum_{x^{(i)} \in I_k} (f_{j,ALE}(x^{(i)}) - (\beta_{0,k} + \beta_{1,k} x^{(i)}))^2$$

where $\beta_{0,k}$ and $\beta_{1,k}$ are the ordinary least square estimates in interval $I_k$.


Optimization problem for numerical features:
Given a fixed number $K$ of intervals $I_k$ defined by K-1 break points $z_k$:

$$x^{(i)} \in I_k: z_{k-1} \leq f_{j,ALE}(x^{(i)}) < z_{k}$$
where  $z_0 = min(f_{j,ALE}(x^{(i)})) i=1,\ldots,n$ and $z_k = max(f_{j,ALE}(x^{(i)})) i=1,\ldots,n$
$$\argmin{z_1,\ldots,z_{K-1}} \sum_{k=1}{K}\sum_{i: x^{(i)} \in I_k} (f_{j,ALE}(x^{(i)}) - c)^2$$
where $c = \frac{1}{n}\sum_{i: x^{(i)} \in I_k} f_{j,ALE}(x^{(i)})$ is the mean  ALE in that interval.



For categorical features we do an exhaustive search with through all possible n_breaks:






We look at each of the ALE plots and  answer: How many weights would we need if we were to approximate all of the ALE plots with a linear model

The following figure shows an ALE plot for a feature from the Friedman2 formula from mlbench TODO:CITE Friedman paper.
We generated 500 data points from this formula an trained a random forest to predict.

$$y = (x_1^2 + (x_2 \cdot x_3 - (1/(x_2 \cdot x_4)))^2)^{0.5} + e$$

Input are uniformly distributed in ranges:
$$0\leq x_1 \leq 100$$
$$ 40\pi \leq x_2 \leq 560 \pi$$
$$ 0 \leq x_3 \leq 1$$
$$ 1 \leq x_4 \leq 11$$

and

$$ e \sim N(0,125)$$
<<fun-complex>>=
n = 1000
grid.size = 50
dat1 = mlbench::mlbench.friedman2(n)
dat1 = data.frame(cbind(dat1$x, y = dat1$y))

dat1$V1 = cut(dat1$V1, breaks = 100 * c(0, 0.2, 0.4, 0.6, 0.8, 1), include.lowest = TRUE)
dat1$V2 = cut(dat1$V2, breaks = quantile(dat1$V2, probs = c(0, 0.2, 1)), include.lowest = TRUE)
#dat1$V3 = cut(dat1$V3, breaks = quantile(dat1$V3, probs = c(0, 0.2, 0.4, 0.6, 0.8, 0.9, 0.95, 1)), include.lowest = TRUE)
#dat1$V4 = cut(dat1$V4, breaks = quantile(dat1$V4, probs = c(0, 0.2, 0.4, 0.6, 0.8, 1)), include.lowest = TRUE)

tsk = makeRegrTask(data = dat1, target = "y")
lrn = makeLearner("regr.ksvm")
mod = train(lrn, tsk)
pred.ksvm = iml::Predictor$new(mod, dat1)
fc = FunComplexity$new(pred.ksvm, epsilon = 0.05, max_c = 10, grid.size = grid.size)
#FeatureEffects$new(pred.ksvm, grid.size = grid.size)$plot(nrow = 1)
plot(fc, nrow = 1)
fc$approx_models$V1$plot()
FeatureEffect$new(pred.ksvm, "V1")$plot()
# Introduce second epsilon
fc$plot(fc$relevant_features)

fc$c_wmean


# order by increasing y
dat2 = impact_order_all(dat1, y = dat1$y)
pred.ksvm = iml::Predictor$new(mod, dat2)
fe = FeatureEffect$new(pred.ksvm, feature  = "V4", method = "ale")
fe$plot()
@


Best step and best piece-wise linear models for $epsilon = 0.05$

Categorical feature
<<dependson = "fun-complex">>=
plot(fc$approx_models$V1)
@


Numerical feature
<<dependson = "fun-complex">>=
plot(fc$approx_models$V4)
@



The effect of different values for $epsilon$, here on the piece-wise linear model:
<<depends = "fun-complex">>=
fc = FunComplexity$new(pred.ksvm, epsilon = 0.05, max_c  = 10, grid.size = grid.size)
p1 = plot(fc$approx_models$V4)

fc = FunComplexity$new(pred.ksvm, epsilon = 0.001, max_c = 10, grid.size = grid.size)
p2 = plot(fc$approx_models$V4)

fc = FunComplexity$new(pred.ksvm, epsilon = 0.05, max_c = 10, grid.size = grid.size)
p3 = plot(fc$approx_models$V1)

fc = FunComplexity$new(pred.ksvm, epsilon = 0.001, max_c = 10, grid.size = grid.size)
p4 = plot(fc$approx_models$V1)

gridExtra::grid.arrange(p1, p2, p3, p4, nrow = 2, ncol = 2)

@


<<depends = "fun-complex">>=
fc.ksvm = FunComplexity$new(pred.ksvm, epsilon = 0.05, max_c = 10, grid.size = grid.size)
p1 = plot(fc.ksvm, nrow = 1)

lrn = makeLearner("regr.rpart")
#lrn = makeLearner("regr.kknn")

mod = train(lrn, tsk)
pred.rpart = iml::Predictor$new(mod, dat1)
fc.rpart = FunComplexity$new(pred.rpart, epsilon = 0.05, max_c = 10, grid.size = grid.size)

p2 = plot(fc.rpart, nrow = 1)

gridExtra::grid.arrange(p1, p2, row = 2)
@



Things that did not work out:
\begin{itemize}
\item Version for cubic splines approx
\item Version for tree split approx
\end{itemize}


Alternatives that can also be measured with this approach:
\begin{itemize}
\item complexity can be computed differently with this ALE plot approximation. e.g. without weighting
\item Is linear yes or no, in a soft version so that we allow a bit zick-zack
\item Is monotone yes or no
\end{itemize}


This measure also enhances ALE plots even when not used to measure model.
For cheap cost, because we might already have ALE plots.

\begin{itemize}
\item We can use variance to sort ALE plots to show the ones where more is happening first.
\item We can compute Average Marginal Effects from the approximation and report those
\item We can sort ALE plots by complexity c and show the ones with similar complexity ("first row shows the plots with linear relationships, second with complexity of two, ...")
\item With the feature.used approach we can omit computation of ALE plots of features that are not used.
\item We can report the 1st-order fit measure along with the plots to assess how much they are responsible for the total prediction.
\end{itemize}



\subsubsection{LIME unreliable when C high}

Quick Idea: Maybe LIME models match the ALE curves roughly?


Show that $R^2$ of LIME suffers with increasing in C.

GAmboost for friedman2 with increasing m.


<<>>=
library("lime")
set.seed(123)
n = 500
dat = mlbench::mlbench.friedman3(n, sd = 0.3)
datx = data.frame(cbind(dat$x, "y" = dat$y))
task = makeRegrTask(data  = datx, target = "y")

for(pn in c(-1,0,1, 2, 3)) {
  lrn = makeLearner("regr.earth", penalty  = pn)
  mod = train(lrn, task)
  pred = Predictor$new(mod, datx, class = 1)
  fc = FunComplexity$new(pred, grid.size = 100)
  c = fc$c_wmean
  print(c)
  print(fc$r2)
  fe = FeatureEffects$new(pred, method = "pdp+ice", center.at = 0)
  # TODO: Fit LIME models
  exp = lime(datx, mod)
  x = explain(datx[1:50,], exp, n_features = 2)
  print(median(x$model_r2))
}

@

\subsubsection{Correlation between GAM degrees of freedom and C??}

<<gam-approx>>=
set.seed(123)
n = 500
dat = mlbench::mlbench.friedman3(n, sd = 0.3)
datx = data.frame(cbind(dat$x, "y" = dat$y))
task = makeRegrTask(data  = datx, target = "y")
library("mgcv")


gammas = c(0.1, 0.5, 1, 2, 5, 10, 20, 100)
edf = vector()
cs = vector()
epsilons = vector()
for (eps in c(0.1, 0.05, 0.01, 0.001)) {
  for(gamma in gammas) {
    gm = gam(y ~ s(V1) + s(V2) + s(V3) + s(V4), data = datx, gamma = gamma)
    edf  = c(edf, sum(gm$edf)/4)
    pred = Predictor$new(gm, datx)
    fc = FunComplexity$new(pred, epsilon = eps)
    cs = c(cs, fc$c_wmean)
    epsilons = c(epsilons, eps)
  }
}
ggplot(data.frame(x = edf, y = cs, e = factor(epsilons))) +
  geom_line(aes(x=x,y=y, group = e, color = e))  +
  scale_x_continuous(limits = c(0,NA)) +
  scale_y_continuous(limits = c(0,NA))
@



