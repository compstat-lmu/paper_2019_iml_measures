\Sexpr{set_parent('paper.Rnw')}
\subsection{Main Effect Complexity (MEC)}
\label{sec:curve}

% =============================================================================
% Measure Intro
% =============================================================================
<<c-demo-prepare>>=
n = 300
x = runif(n = n)
y = log(x)
dat = data.frame(y = y, x = x, e = rnorm(n, sd = 0.1))
mod = mgcv::gam(y ~ s(x), data = dat)
pred = Predictor$new(mod, dat)
fe = FeatureEffect$new(pred, "x")
app = AleNumApprox$new(ale = fe, max_seg = 2, epsilon = 0.05)
@
To determine the shape complexity of ALE main effects $\falej$, we propose the main effect complexity (MEC) measure.
For single ALE main effect, $\text{MEC}_j$ is the number of parameters needed to approximate the curve with linear segments.
For the entire model, MEC is the average $\text{MEC}_j$ over all main effects, weighted with their variance.
Figure~\ref{fig:c-demo} shows an ALE plot (= main effect) and its approximation with two linear segments.
In the remainder, main effect ALE curves are represented by sparklines, e.g. Figure~\ref{fig:c-demo} by \Sexpr{spark(app)}.
Vertical bars -- if present -- show intervals between segments of the approximation with linear segments.

<<epsilons,  fig.cap = "Effect of different epsilons on complexity of approximation. On the left the ALE plot can be approximated with a single linear segment for the given epsilon of 0.05. On the right two segments are needed to approximate the ALE curve when epsilon is set to 0.001.">>=
n = 1000
grid.size = 50
dat1 = mlbench::mlbench.friedman2(n)
dat1 = data.frame(cbind(dat1$x, y = dat1$y))
tsk = makeRegrTask(data = dat1, target = "y")
lrn = makeLearner("regr.ranger", num.trees = 30)
mod = train(lrn, tsk)
pred.ksvm = iml::Predictor$new(mod, dat1)
fc = FunComplexity$new(pred.ksvm, epsilon = 0.01,grid.size = grid.size)

eps1 = 0.05
fc1 = FunComplexity$new(pred.ksvm, epsilon = eps1,  grid.size = grid.size)
#p1 = plot(fc$approx_models$V3)

eps2 = 0.005
fc2 = FunComplexity$new(pred.ksvm, epsilon = eps2, grid.size = grid.size, max_seg_num = 10)

eps3 = 0.001
fc3 = FunComplexity$new(pred.ksvm, epsilon = eps3, grid.size = grid.size, max_seg_num = 10)
#p2 = plot(fc$approx_models$V3)
#gridExtra::grid.arrange(p1, p2, nrow = 1)
@

<<c-demo2, fig.cap = "Two identical ALE curves with the difference that one has a jump at 0.5. The measures complexity is higher for the left curve, since we need more segments to describe the variance of the curve. Both have an R squared above 0.95">>=
n = 3000
grid.size = 100
x = runif(n = n)
y = 0.5 * sin(15 * x)
dat = data.frame(y = y, x = x, e = rnorm(n, sd = 0.1))
mod = mgcv::gam(y ~ s(x), data = dat)
pred = Predictor$new(mod, dat)
fe = FeatureEffect$new(pred, "x", grid.size = grid.size)
app1_wiggle = AleNumApprox$new(ale = fe, max_seg = 6, epsilon = 0.05)
#p1 = plot(app1)  + scale_y_continuous(limits = c(-7,7))

# add
dat2 = dat
dat2$y = 0.5 * sin(20 * dat$x) + 10 * as.numeric(dat$x >0.5)

mod = mgcv::gam(y ~ s(x) + I(x >0.5), data = dat2)
pred = Predictor$new(mod, dat2)
fe = FeatureEffect$new(pred, "x", grid.size = grid.size)
app2_jump = AleNumApprox$new(ale = fe, max_seg = 5, epsilon = 0.05)
#p2 = plot(app2) + scale_y_continuous(limits = c(-7,7))
#grid.arrange(p1, p2, nrow = 1)
@


<<c-demo3, fig.cap = "Three ALE curves that only differ by a scaling factor in  dicrection of the y-axis. All three need the same amount of segments to be approximated with R squared above 0.95">>=
n = 3000
grid.size = 100
x = runif(n = n)
y = 0.2 * sin(15 * x)
dat = data.frame(y = y, x = x, e = rnorm(n, sd = 0.1))
mod = mgcv::gam(y ~ s(x), data = dat)
pred = Predictor$new(mod, dat)
fe = FeatureEffect$new(pred, "x", grid.size = grid.size)
app1 = AleNumApprox$new(ale = fe, max_seg = 6, epsilon = 0.05)
p1 = plot(app1)  + scale_y_continuous(limits = c(-7,7))

y = 2 * sin(15 * x)
dat = data.frame(y = y, x = x, e = rnorm(n, sd = 0.1))
mod = mgcv::gam(y ~ s(x), data = dat)
pred = Predictor$new(mod, dat)
fe = FeatureEffect$new(pred, "x", grid.size = grid.size)
app2 = AleNumApprox$new(ale = fe, max_seg = 6, epsilon = 0.05)
p2 = plot(app2)  + scale_y_continuous(limits = c(-7,7))

y = 6 * sin(15 * x)
dat = data.frame(y = y, x = x, e = rnorm(n, sd = 0.1))
mod = mgcv::gam(y ~ s(x), data = dat)
pred = Predictor$new(mod, dat)
fe = FeatureEffect$new(pred, "x", grid.size = grid.size)
app3 = AleNumApprox$new(ale = fe, max_seg = 6, epsilon = 0.05)
p3 = plot(app3)  + scale_y_continuous(limits = c(-7,7))
#grid.arrange(p1, p2, p3, nrow  = 1)
@


<<c-demo, fig.cap = "ALE curve (black line) approximated by two linear segments. Segments are vertical lines, approximation the doted lines.", out.height ="4cm", out.width="10cm", fig.height=4, fig.width=10, fig.align="center", dependson="c-demo-prepare">>=
plot(app) + ggtitle("") + scale_y_continuous("ALE")
@
% =============================================================================
% Measure Intuition
% =============================================================================
Through the approximation with linear segments, the degrees of freedom required for describing a main effect curve becomes measurable.
The approximiation allows some error, e.g. an almost linear main effect like \Sexpr{spark(fc1$approx_models$V3$ale)}  has $\text{MEC}_j=1$, even if dozens of parameters would be needed to describe it perfectly. 
The approximation quality is measured with R-squared.
An approximation has to reach an R-squared of at least $1-\epsilon$, where $\epsilon$ is the user defined maximum approximation error.
We also introduced parameter $max_{seg}$, the maximum number of segments.
In the case that an approximation cannot reach an R-squared above $1-\epsilon$ with a given $max_{seg}$, $\text{MEC}_j$ is computed with the maximum number of segments.
The selected maximum approximation error $\epsilon$ should be small, but not too small.
We found $\epsilon$ between $0.01$ and $0.1$ visually meaningful.
% =============================================================================
% Measure Sum Up and Fix
% =============================================================================
$\text{MEC}_j$ is averaged over all features to obtain the main effect complexity for the model.
Each $\text{MEC}_j$ is weighted with the variance of the corresponding ALE main effect to give more weight to features that contribute more to the prediction.
%For example, the two ALE curves  \Sexpr{spark(app1_wiggle$ale, c(-7, 7), width = 5)} and \Sexpr{spark(app2_jump$ale, c(-7, 7), width = 5)} are identical except for the jump at 0.5 in the second curve.
%We need more linear segments to get the same approximation $R^2$ as for the second curve.
%One way to mitigate the problem of getting high MEC estimates through wiggly main effects that are irrelevant for the prediction is to weight each main effect by its variance.
For example, the three main effect curves \Sexpr{spark(app1$ale, c(-6, 6), width = 5)}, \Sexpr{spark(app2$ale, c(-6, 6), width = 5)} and \Sexpr{spark(app3$ale, c(-6, 6), width = 5)} differ from each other by a scaling factor in the y-axis, but each requires five segments to be approximated with $R^2 >= 0.95$:
 \Sexpr{spark(app1, c(-6, 6), width = 5)}, \Sexpr{spark(app2, c(-6, 6), width = 5)} and \Sexpr{spark(app3, c(-6, 6), width = 5)}
When weighted with the variance,  \Sexpr{spark(app1$ale, c(-6, 6), width = 5)} gets a weight of \Sexpr{sprintf("%.2f", app1$var)}, \Sexpr{spark(app2$ale, c(-6, 6), width = 5)} a weight of \Sexpr{sprintf("%.2f", app2$var)} and \Sexpr{spark(app3$ale, c(-6, 6), width = 5)} a weight of \Sexpr{sprintf("%.2f", app3$var)}.
Algorithm~\ref{algo:AMEC} describes the MEC computation in detail.

% =============================================================================
% Algorithm for complexity
% =============================================================================
\begin{algorithm}
\caption{Main Effect Complexity (MEC).}\label{algo:AMEC}
	\KwInput{$\fh$, $\epsilon$, $max_{seg}$, $\D$}
	\For{$j \in \pset$}{
Estimate ALE $\falej$\;
	%\If{$\falej(x_j) = 0$ for all $x$}{
	%	$MEC_j = 0, V_j=0$\;
	%	Continue with next feature\;
	%	}
	Approximate $\falej$ with $g_j(x_j) = \beta_0 + \beta_1 x_j$\;
	Compute $R^2$ of $g_j(x_j)$\;
	Set $K=1$ \tcp{Number of breakpoints}
	\While{$K < max_{seg}$ AND $R^2 < (1- \epsilon)$}{
		\tcp{Optimize intervals $Z_k$ via generalized simulated annealing, $\beta$'s per segment with ordinary least squares}
		\tcp{For categorical feature, fix slope to zero} 
		$g_j(x_j) = \sum_{k=1}^{K+1} \I_{\xi\in Z_k}\cdot\left(\hat{\beta}_0^k + \hat{\beta}_j^k\xij\right)$

		Compute $R^2$ of $g_j$\;
		Set $K = K+1$ \;
	}
	\tcp{Post-processing of slopes}
	\For{$k \in 1,\ldots,K$}{
		Set $\beta_{0}^k = \frac{1}{\sum_{i=1}^n\I_{\xi\in Z_k}}\sum_{i=1}^n\I_{\xi\in Z_k}\cdot\falej(\xij)$ and $\beta_{j1}^k = 0$ in $g_j$\;
		\lIf{$R^2(g_j) > (1-\epsilon)$}
		{
			Use new $\hat{\beta}_{j0}^k$, $\hat{\beta}_{j1}^k$ in $g_j$
		}
		\lElse{
			Keep old $\hat{\beta}_{0}^k$, $\hat{\beta}_{1}^k$ in $g_j$	
			}
			}
\tcp{Sum of non-zero coefficients minus first intercept}
$MEC_j = K - 1+ \sum_{k=1}^K \mathbb{I}_{\beta_j^k > 0}$\;
$V_j = \frac{1}{n}\sum_{i=1}^n \left(\hat{f}^{ALE}_j(x^{(i)})\right)^2$\;
}
	\Return{$MEC = \frac{1}{\sum_{j=1}^pV_j}\sum_{j=1}^p V_jMEC_j$\;}
\end{algorithm}






% =============================================================================
% AMEC ignores internal model complexity and uses external complexity
% =============================================================================
<<tree-increase-data>>=
set.seed(42)
library(rpart)
n = 1000
dat = data.frame(x = runif(n), x2 = rnorm(n))
nsd = 0.4
dat$y = 2 * dat$x + rnorm(n, sd = nsd)

grid.size = 100
epsilon = 0.05
@


% =============================================================================
% =============================================================================
% =============================================================================
% Material
% =============================================================================
% =============================================================================
% =============================================================================

%If all features in the data are uncorrelated, this has the nice interpretation that each feature main effect complexity (MEC) is weighted by the percentage it contributed to the variance of the complete main effects model.
%In case of correlated features, it's more complicated.
%If we were to fairly assign to each feature the share of the first order model variance, we would need an attribution method such as the Shapley value.
%The big problem is that the contribution can be negative when features are correlated and one of the correlated features has a much stronger effect.
%This can happen when the covariance is negative.
%The interpretation is not very intuitive then.
%An advantage of weighting by the individual variances is that it has a visual connection:
%The smaller the variance, the lower the amplitude of the curve in y-direction.
%And we want that weighted by the data distribution, so taking the minimum and maximu for scaling is not an option.


% =============================================================================
% Find best splits
% =============================================================================
%\begin{algorithm}
%\caption{ApproxLinearSegments}\label{algo:aleapprox}
%	\KwInput{K, $\falej$, $\epsilon$}
%	\KwOutput{$\beta \in \mathbb{R}^k, R^2$}
%	Define equidistant initial breakpoints: $\hat{z}_1,\ldots, \hat{z}_K$\;
%	Define segments $Z_1 = [min(\xj), z_1], \ldots, Z_k = [z_{k-1}, z_k],\ldots,Z_{K+1} = [z_K,max(\xj)]$ \\
%If $K=0$, then $Z_1 = [min(\xj), max(\xj)]$\;
%
%	Optimize break points: $\hat{z}_1,\ldots, \hat{z}_K = argmin_{z_1,\ldots,z_K} \sum_{k=1}^{K+1} \sum_{\xi\in Z_k}\left(\falej(\xij) - (\beta_{j0}^k + \beta_{j1}^k\xij)\right)^2$ where and $\beta_{j0}^k$ and $\beta_{j1}^k$ are the least square estimates. 
%Optimize with Generalized Simulated annealing.
%
%$\galej(\xj) = \sum_{k=1}^{K+1} \I_{\xi\in Z_k}\left(\hat{\beta}_0^k + \hat{\beta}_j^k\xij\right)$.
%Set slopes to zero. For each segment $k \in 1,\ldots,K$:
%\begin{itemize}
%\item Set $\beta_{j0}^k = \frac{1}{n_k}\sum_{\xi\in Z_k}\falej(\xij)$ and $\beta_{j1}^k$ = 0
%\item If still $R^2 > 1 - \epsilon$, overwrite both $\beta$.
%\end{itemize}
%
%* If the feature is numeric, fix all $\beta_{j1}^k$ at zero and we applied random search of break points instead of GenSA.
%\end{algorithm}


% =============================================================================
% C example
% =============================================================================
% We illustrate the measure with a short example.
% We simulated 500 data points with 4 features as a regression problem \citep{friedman1991multivariate}.
% The features are uniformly distributed in the following intervals: $0\leq x_1 \leq 100$, $ 40\pi \leq x_2 \leq 560 \pi$, $ 0 \leq x_3 \leq 1$, $ 1 \leq x_4 \leq 11$.
% The regression target was simulated as:
%
% $$y = (x_1^2 + (x_2 \cdot x_3 - (1/(x_2 \cdot x_4)))^2)^{0.5} + e$$
% where $ e \sim N(0,125)$.
%
% We trained a random forest, plotted the ALEs and calculated the complexities with the parameters $\epsilon = 0.95$, $max_{segnum} = 5$ and $max_{segcat} = 9$


% Since the model relied the most on $x2$ and $x3$ as main effects, the weighted main effect complexity is \Sexpr{fc$c_wmean}.
% Both features main effects can be approximated with a linear segment.
% The other two feature have a very low variance and get a low weight.

<<spark-table, dependson="epsilons", results="asis", eval=FALSE>>=
dat = data.frame(test = c(1,2), sp = c(spark(fc$approx_models$V1), spark(fc$approx_models$V2)), stringsAsFactors = FALSE)
print.xtable(xtable(dat), sanitize.text.function = as.is)
@
% =============================================================================
% Different epsilons
% =============================================================================
%The choice of $\epsilon$ affects the complexity measures:
%This ALE main effect \Sexpr{spark(fc1$approx_models$V3$ale)} can be approximated with a linear model \Sexpr{spark(fc1$approx_models$V3, approx = TRUE)} when $\epsilon = \Sexpr{eps1}$ but needs \Sexpr{(fc3$approx_models$V3$n_coefs + 1) / 2} segments when $\epsilon$ is decreased to \Sexpr{eps3}: \Sexpr{spark(fc3$approx_models$V3, approx = TRUE)}.



% =============================================================================
% Did not work out
% =============================================================================
% We tried a few alternatives to approximate the ALE plots for measuring their complexity, which turned out to be impractical.
% We tried approximation with cubic (overlapping) splines and measuring the complexity as degrees of freedom.
% The degrees of freedom exploded when the ALE plot was not quite linear, but almost, yielding an unintuitive measure.
% Tree splits did not work, like for example model based partitioning TODO: CITE mob.
% One reason is that the greedy approach did not work well and we found it better to search per number of splits.

%The complexity measures ignores how complex the internal structure of the model is, e.g. it does not matter whether the linear relationship comes from a single linear model or a blend of hundreds of linear models.
%We demonstrate how the number of nodes and complexity measure C increase with increasing model performacne, but at some point the complexity C decreases again.
%For this example, \Sexpr{n} data points were drawn from a uniform distribution between 0 and 1, and the target is simulated as $2 * x + \epsilon$, where $\epsilon \sim N(0,\Sexpr{nsd})$.
%We trained three trees with different maximal depths, as shown in Table~\ref{tab:tree}.

<<tree-increase, dependson="tree-increase-data",results="asis", eval=FALSE>>=
rp_width = function(rp) sum(rp$frame$var == "<leaf>")

rp1 = rpart(y ~ ., data = dat, control = rpart.control(maxdepth = 1))
pred1 = Predictor$new(rp1, data = dat)
fc1 = FunComplexity$new(pred1, grid.size = grid.size, epsilon = epsilon)
#p1 = fc1$approx_models$x$plot() +
#  ggtitle(sprintf("Leaf nodes: %i, C: %i", rp_width(rp1), fc1$c_wmean))

rp2 = rpart(y ~ ., data = dat, control = rpart.control(maxdepth = 2))
pred2 = Predictor$new(rp2, data = dat)
fc2 = FunComplexity$new(pred2, grid.size = grid.size, epsilon = epsilon)
#p2 = fc2$approx_models$x$plot() +
#  ggtitle(sprintf("Leaf nodes: %i, C: %i", rp_width(rp2), fc2$c_wmean))

rp3 = rpart(y ~ ., data = dat, control = rpart.control(maxdepth = 3))
pred3 = Predictor$new(rp3, data = dat)
fc3 = FunComplexity$new(pred3, grid.size = grid.size, epsilon = epsilon)
#p3 = fc3$approx_models$x$plot() +
#  ggtitle(sprintf("Leaf nodes: %i, C: %i", rp_width(rp3), fc3$c_wmean))

#grid.arrange(p1, p2, p3, nrow = 1)
n_terminal = function(tree) {
  nrow(tree$frame[tree$frame$var == "<leaf>",])
}

tab = data.frame(
  "Tree Depth" = c(1,2,3),
  "Terminal nodes" = c(n_terminal(rp1), n_terminal(rp2),n_terminal(rp3)),
  "ALE" = c(spark(fc1$approx_models$x),spark(fc2$approx_models$x),spark(fc3$approx_models$x)),
  "Approximation" = c(
    spark(fc1$approx_models$x, approx = TRUE),
    spark(fc2$approx_models$x, approx = TRUE),
    spark(fc3$approx_models$x, approx = TRUE)),
  "AMEC" = c(fc1$c_wmean, fc2$c_wmean, fc3$c_wmean))

caption = "Increasing the tree depth always increases number of terminal nodes, but the complexity measure increases at some point because the prediction function can be approximated by a single linear function."
print.xtable(xtable(tab, caption = caption), sanitize.text.function = as.is, booktabs = TRUE, label = "tab:tree")
@



