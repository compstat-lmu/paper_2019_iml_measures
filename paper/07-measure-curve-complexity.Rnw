\Sexpr{set_parent('paper.Rnw')}
\subsection{Measure ALE First Order Complexity}
\label{sec:curve}

% =============================================================================
% Measure Intro
% =============================================================================
We propose a measure that, for each of the main effects $\falej$, approximates the ALE curve with linear segments and adds up the parameters needed if we were to model all those segments in a segmented linear regression model, which models the (approximate) ALE curve as a function of the single feature.
When complexity measure is minimized, we get linear effects or binary split.


% =============================================================================
% Measure Intuition
% =============================================================================
<<c-demo, fig.cap = "An ALE curve (black line) approximated by two linear segments. Segments are vertical lines, approximation the doted lines.">>=
n = 300
x = runif(n = n)
y = log(x)
dat = data.frame(y = y, x = x, e = rnorm(n, sd = 0.1))
mod = mgcv::gam(y ~ s(x), data = dat)
pred = Predictor$new(mod, dat)
fe = FeatureEffect$new(pred, "x")
app = AleNumApprox$new(ale = fe, max_seg = 2, epsilon = 0.05)
plot(app) + ggtitle("")
@

Before we dive into the algorithm we show an example.
Figure \ref{fig:c-demo} shows an ALE plot and an approximation with linear segments.
Throughout the paper we will often represent those main effect ALE curves with sparklines, e.g. Figure \ref{fig:c-demo} looks like this in-text \Sexpr{spark(app)}.
The dots represent the start and end of the curve, the vertical bars -- if present -- the segments the curve was divided into.
The ALE curve can be complex, only limited by the number of intervals used by the ALE method.
The goal is to replace the ALE curve by something where we know how complex it is.
We decided to use linear segments.
Another goal is that if a curve is approximately linear but with some additional ups and downs, we would want to approximate the curve with only a linear function.
Therefore we have to introduce some allowed error.
To account for possible constant ALE plots (e.g. for a tree, where there might be only one jump between two constant pieces), we additionally have built in some ways to try to approximate the curve with constant segments.
We measured the approximation with $R^2$ of the approximation vs. the full ALE curve.


%TODO: Fix max_seg = 1 only alllowed to spit out one segment


% =============================================================================
% Measure Sum Up and Fix
% =============================================================================
For computing the complexity per feature, we only look at this feature.
But we have to aggregate these measures over all feature got get one number.
Looking only at one feature at a time has one problem:
The features might have different overall impact on prediction (some close to none), but the complexity measures don't reflect this and tend to be higher the lower the variance of the ALE plot is.


<<c-demo2, fig.cap = "Two identical ALE curves with the difference that one has a jump at 0.5. The measures complexity is higher for the left curve, since we need more segments to describe the variance of the curve. Both have an R squared above 0.95">>=
n = 3000
grid.size = 100
x = runif(n = n)
y = 0.5 * sin(15 * x)
dat = data.frame(y = y, x = x, e = rnorm(n, sd = 0.1))
mod = mgcv::gam(y ~ s(x), data = dat)
pred = Predictor$new(mod, dat)
fe = FeatureEffect$new(pred, "x", grid.size = grid.size)
app1 = AleNumApprox$new(ale = fe, max_seg = 6, epsilon = 0.05)
p1 = plot(app1)  + scale_y_continuous(limits = c(-7,7))

# add
dat2 = dat
dat2$y = 0.5 * sin(20 * dat$x) + 10 * as.numeric(dat$x >0.5)

mod = mgcv::gam(y ~ s(x) + I(x >0.5), data = dat2)
pred = Predictor$new(mod, dat2)
fe = FeatureEffect$new(pred, "x", grid.size = grid.size)
app2 = AleNumApprox$new(ale = fe, max_seg = 5, epsilon = 0.05)
p2 = plot(app2) + scale_y_continuous(limits = c(-7,7))
#grid.arrange(p1, p2, nrow = 1)
@

Two identical ALE curves \Sexpr{spark(app1$ale, c(-7, 7), width = 5)} and \Sexpr{spark(app2$ale, c(-7, 7), width = 5)} with the difference that one has a jump at 0.5. The measures complexity is higher for the left curve, since we need more segments to describe the variance of the curve. Both have an R squared above 0.95

Also at what point would we decide between "this is a noisy ALE plot" and "we have to approximate this"?

<<c-demo3, fig.cap = "Three ALE curves that only differ by a scaling factor in  dicrection of the y-axis. All three need the same amount of segments to be approximated with R squared above 0.95">>=
n = 3000
grid.size = 100
x = runif(n = n)


y = 0.2 * sin(15 * x)
dat = data.frame(y = y, x = x, e = rnorm(n, sd = 0.1))
mod = mgcv::gam(y ~ s(x), data = dat)
pred = Predictor$new(mod, dat)
fe = FeatureEffect$new(pred, "x", grid.size = grid.size)
app1 = AleNumApprox$new(ale = fe, max_seg = 6, epsilon = 0.05)
p1 = plot(app1)  + scale_y_continuous(limits = c(-7,7))

y = 2 * sin(15 * x)
dat = data.frame(y = y, x = x, e = rnorm(n, sd = 0.1))
mod = mgcv::gam(y ~ s(x), data = dat)
pred = Predictor$new(mod, dat)
fe = FeatureEffect$new(pred, "x", grid.size = grid.size)
app2 = AleNumApprox$new(ale = fe, max_seg = 6, epsilon = 0.05)
p2 = plot(app2)  + scale_y_continuous(limits = c(-7,7))

y = 6 * sin(15 * x)
dat = data.frame(y = y, x = x, e = rnorm(n, sd = 0.1))
mod = mgcv::gam(y ~ s(x), data = dat)
pred = Predictor$new(mod, dat)
fe = FeatureEffect$new(pred, "x", grid.size = grid.size)
app3 = AleNumApprox$new(ale = fe, max_seg = 6, epsilon = 0.05)
p3 = plot(app3)  + scale_y_continuous(limits = c(-7,7))
#grid.arrange(p1, p2, p3, nrow  = 1)
@

Let's consider 3 curves \Sexpr{spark(app1$ale, c(-6, 6), width = 5)}, \Sexpr{spark(app2$ale, c(-6, 6), width = 5)} and \Sexpr{spark(app3$ale, c(-6, 6), width = 5)}.
The three ALE curves only differ by a scaling factor in  dicrection of the y-axis.
All three need the same amount of segments to be approximated with R squared above 0.95:
 \Sexpr{spark(app1, c(-6, 6), width = 5)}, \Sexpr{spark(app2, c(-6, 6), width = 5)} and \Sexpr{spark(app3, c(-6, 6), width = 5)}
We are only interested in the average of the complexity.
An alternative would be the sum, but we already capture the number of features.
So we propose to weight each of the complexity measures by the variance of the corresponding ALE plot.
In our example if the three curves were ALE plots from three different features,  \Sexpr{spark(app1$ale, c(-6, 6), width = 5)} would get a weight of \Sexpr{sprintf("%.2f", app1$var)}, \Sexpr{spark(app2$ale, c(-6, 6), width = 5)} a weight of \Sexpr{sprintf("%.2f", app2$var)} and \Sexpr{spark(app3$ale, c(-6, 6), width = 5)} a weight of \Sexpr{sprintf("%.2f", app3$var)}.
If all features in the data are uncorrelated, this has the nice interpretation that each feature main effect complexity (MEC) is weighted by the percentage it contributed to the variance of the complete main effects model.
In case of correlated features, it's more complicated.
If we were to fairly assign to each feature the share of the first order model variance, we would need an attribution method such as the Shapley value.
The big problem is that the contribution can be negative when features are correlated and one of the correlated features has a much stronger effect.
This can happen when the covariance is negative.
The interpretation is not very intuitive then.
An advantage of weighting by the individual variances is that it has a visual connection:
The smaller the variance, the lower the amplitude of the curve in y-direction.
And we want that weighted by the data distribution, so taking the minimum and maximu for scaling is not an option.

% =============================================================================
% Algorithm for complexity
% =============================================================================
The following algorithm describes our procedure for determining the average main effect complexity (AMEC).

TODO: Iterate over the description of the algorithm to improve it
\begin{algorithm}
\caption{Estimate average main effect complexity (AMEC).}\label{algo:ale-complexity}
\begin{enumerate}
\item Input: Model $\fh$, PRL-bound $\epsilon$, complexity limit $max_{seg}$
\item Output: Weighted average complexity $C$
\item For each $j \in \pset$ do:
\begin{enumerate}
\item Estimate ALE $\falej$
\item If $\falej(x_j) = 0$ for all $x$, stop and return $\galej(x) = 0$
\item Set $K=0$ and $\galej = 0$
\item While $K < max_{seg}$ AND $PRL(\falej, \galej, L) < 1- \epsilon$:
\begin{itemize}
\item $\galej = FindBestSplits(K, \falej, \epsilon)$
\end{itemize}
\item $C_j = K + \sum_{k=1}^K \mathbb{I}_{\beta_k > 0}$
\item ALE variance $V_j = \frac{1}{n}\sum_{i=1}^n \hat{f}^{ALE}_j(x^{(i)})$
\end{enumerate}
\item $C = \frac{1}{\sum_{j=1}^pV_j}\sum_{j=1}^p V_jc_j$.
\end{enumerate}
\end{algorithm}



% =============================================================================
% Find best splits
% =============================================================================
\begin{algorithm}
\caption{FindBestSplits for given K.}\label{algo:ale-complexity-splits}
Input: K, $\falej$, $\epsilon$, Number of samples $m$

$\hat{z}_1,\ldots, \hat{z}_K = argmin_{z_1,\ldots,z_K} \sum_{k=1}^{K+1} \sum_{\xi\in Z_k}\left(\falej(\xij) - (\beta_{j0}^k + \beta_{j1}^k\xij)\right)^2$ where $Z_1 = [min(\xj), z_1], \ldots, Z_k = [z_{k-1}, z_k],\ldots,Z_{K+1} = [z_K,max(\xj)]$ and $\beta_{j0}^k$ and $\beta_{j1}^k$ are the least square estimates. If $K=0$, then $Z_1 = [min(\xj), max(\xj)]$.

Optimize with Generalized Simulated annealing.

$\galej(\xj) = \sum_{k=1}^{K+1} \I_{\xi\in Z_k}\left(\hat{\beta}_0^k + \hat{\beta}_j^k\xij\right)$.
Set slopes to zero. For each segment $k \in 1,\ldots,K$:
\begin{itemize}
\item Set $\beta_{j0}^k = \frac{1}{n_k}\sum_{\xi\in Z_k}\falej(\xij)$ and $\beta_{j1}^k$ = 0
\item If still $R^2 > 1 - \epsilon$, overwrite both $\beta$.
\end{itemize}

* If the feature is numeric, fix all $\beta_{j1}^k$ at zero and we applied random search of break points instead of GenSA.
\end{algorithm}




% =============================================================================
% Different epsilons
% =============================================================================

<<epsilons,  fig.cap = "Effect of different epsilons on complexity of approximation. On the left the ALE plot can be approximated with a single linear segment for the given epsilon of 0.05. On the right two segments are needed to approximate the ALE curve when epsilon is set to 0.001.">>=

n = 1000
grid.size = 50
dat1 = mlbench::mlbench.friedman2(n)
dat1 = data.frame(cbind(dat1$x, y = dat1$y))
tsk = makeRegrTask(data = dat1, target = "y")
lrn = makeLearner("regr.ranger")
mod = train(lrn, tsk)
pred.ksvm = iml::Predictor$new(mod, dat1)
fc = FunComplexity$new(pred.ksvm, epsilon = 0.01,grid.size = grid.size)

eps1 = 0.05
fc1 = FunComplexity$new(pred.ksvm, epsilon = eps1,  grid.size = grid.size)
#p1 = plot(fc$approx_models$V3)

eps2 = 0.005
fc2 = FunComplexity$new(pred.ksvm, epsilon = eps2, grid.size = grid.size, max_seg_num = 10)

eps3 = 0.001
fc3 = FunComplexity$new(pred.ksvm, epsilon = eps3, grid.size = grid.size, max_seg_num = 10)
#p2 = plot(fc$approx_models$V3)
#gridExtra::grid.arrange(p1, p2, nrow = 1)
@
Different values for $\epsilon$ yield different measures of complexity:
An ALE curve that looks like this \Sexpr{spark(fc1$approx_models$V3$ale)} can be approximated with a linear model \Sexpr{spark(fc1$approx_models$V3, approx = TRUE)} (\Sexpr{eps1}) and gets increasingly more complex when we allow less error, e.g. \Sexpr{spark(fc2$approx_models$V3, approx = TRUE)} (\Sexpr{eps2}) and  \Sexpr{spark(fc3$approx_models$V3, approx = TRUE)} (\Sexpr{eps3}).




% =============================================================================
% AMEC ignores internal model complexity and uses external complexity
% =============================================================================
<<tree-increase-data>>=
set.seed(42)
library(rpart)
n = 1000
dat = data.frame(x = runif(n), x2 = rnorm(n))
nsd = 0.4
dat$y = 2 * dat$x + rnorm(n, sd = nsd)

grid.size = 100
epsilon = 0.05
@
The complexity measures ignores how complex the prediction process is within the model, e.g. it does not matter whether the linear relationship comes from a single linear model or a blend of hundreds of linear models.
We demonstrate how the number of nodes and complexity measure C increase with increasing model performacne, but at some point the complexity C decreases again.
For this example, \Sexpr{n} data points are drawn from  a uniform distribution between 0 and 1, and the target is simulated as $2 * x + \epsilon$, where $\epsilon \sim N(0,\Sexpr{nsd})$.
We train a three different trees with different maximal depths, as shown in Table \ref{tab:tree}.

<<tree-increase, dependson="tree-increase-data",results="asis">>=
rp_width = function(rp) sum(rp$frame$var == "<leaf>")

rp1 = rpart(y ~ ., data = dat, control = rpart.control(maxdepth = 1))
pred1 = Predictor$new(rp1, data = dat)
fc1 = FunComplexity$new(pred1, grid.size = grid.size, epsilon = epsilon)
#p1 = fc1$approx_models$x$plot() +
#  ggtitle(sprintf("Leaf nodes: %i, C: %i", rp_width(rp1), fc1$c_wmean))

rp2 = rpart(y ~ ., data = dat, control = rpart.control(maxdepth = 2))
pred2 = Predictor$new(rp2, data = dat)
fc2 = FunComplexity$new(pred2, grid.size = grid.size, epsilon = epsilon)
#p2 = fc2$approx_models$x$plot() +
#  ggtitle(sprintf("Leaf nodes: %i, C: %i", rp_width(rp2), fc2$c_wmean))

rp3 = rpart(y ~ ., data = dat, control = rpart.control(maxdepth = 3))
pred3 = Predictor$new(rp3, data = dat)
fc3 = FunComplexity$new(pred3, grid.size = grid.size, epsilon = epsilon)
#p3 = fc3$approx_models$x$plot() +
#  ggtitle(sprintf("Leaf nodes: %i, C: %i", rp_width(rp3), fc3$c_wmean))

#grid.arrange(p1, p2, p3, nrow = 1)
n_terminal = function(tree) {
  nrow(tree$frame[tree$frame$var == "<leaf>",])
}

tab = data.frame(
  "Tree Depth" = c(1,2,3),
  "Terminal nodes" = c(n_terminal(rp1), n_terminal(rp2),n_terminal(rp3)),
  "ALE" = c(spark(fc1$approx_models$x),spark(fc2$approx_models$x),spark(fc3$approx_models$x)),
  "Approximation" = c(
    spark(fc1$approx_models$x, approx = TRUE),
    spark(fc2$approx_models$x, approx = TRUE),
    spark(fc3$approx_models$x, approx = TRUE)),
  "AMEC" = c(fc1$c_wmean, fc2$c_wmean, fc3$c_wmean))

caption = "Increasing the tree depth always increases number of terminal nodes, but the complexity measure increases at some point because the prediction function can be approximated by a single linear function."
print.xtable(xtable(tab), sanitize.text.function = as.is, booktabs = TRUE, caption = caption, label = "tab:tree")
@



% =============================================================================
% =============================================================================
% =============================================================================
% Material
% =============================================================================
% =============================================================================
% =============================================================================


% =============================================================================
% C example
% =============================================================================
% We illustrate the measure with a short example.
% We simulated 500 data points with 4 features as a regression problem \citep{friedman1991multivariate}.
% The features are uniformly distributed in the following intervals: $0\leq x_1 \leq 100$, $ 40\pi \leq x_2 \leq 560 \pi$, $ 0 \leq x_3 \leq 1$, $ 1 \leq x_4 \leq 11$.
% The regression target was simulated as:
%
% $$y = (x_1^2 + (x_2 \cdot x_3 - (1/(x_2 \cdot x_4)))^2)^{0.5} + e$$
% where $ e \sim N(0,125)$.
%
% We trained a random forest, plotted the ALEs and calculated the complexities with the parameters $\epsilon = 0.95$, $max_{segnum} = 5$ and $max_{segcat} = 9$


% Since the model relied the most on $x2$ and $x3$ as main effects, the weighted main effect complexity is \Sexpr{fc$c_wmean}.
% Both features main effects can be approximated with a linear segment.
% The other two feature have a very low variance and get a low weight.

<<spark-table, dependson="epsilons", results="asis", eval=FALSE>>=
dat = data.frame(test = c(1,2), sp = c(spark(fc$approx_models$V1), spark(fc$approx_models$V2)), stringsAsFactors = FALSE)
xtable::print.xtable(xtable::xtable(dat), sanitize.text.function = xtable::as.is)
@


% =============================================================================
% Did not work out
% =============================================================================
% We tried a few alternatives to approximate the ALE plots for measuring their complexity, which turned out to be impractical.
% We tried approximation with cubic (overlapping) splines and measuring the complexity as degrees of freedom.
% The degrees of freedom exploded when the ALE plot was not quite linear, but almost, yielding an unintuitive measure.
% Tree splits did not work, like for example model based partitioning TODO: CITE mob.
% One reason is that the greedy approach did not work well and we found it better to search per number of splits.



