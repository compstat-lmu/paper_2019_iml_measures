\Sexpr{set_parent('paper.Rnw')}
\subsection{Measure ALE First Order Complexity}
\label{sec:curve}

We propose a measure that, for each of the main effects, approximates the ALE curve with linear segments and adds up the parameters needed if we were to model all those segments in a linear regression model.

Recipe:

\begin{enumerate}
\item Input: Model f, PRL-bound $\epsilon$, upper bound complexity $c_{max}$
\item For all features $x_1,\ldots,x_p$:
\begin{itemize}
\item Estimated first order ALE
\item Approximate with segmented linear regression
\item Complexity of approximation = Number of weights needed in linear regression model ($m_j$)
\item For numerical: Number of linear segments - 1 (Because of intercept)
\item For categorical: Number of sets - 1 (Because of intercept)
\end{itemize}
\item Sum up all complexity measures $c_j$: $c = 1 + \sum_{j=1}^p c_j$. The 1 is for global intercept.
\end{enumerate}


Algorithm for computing best approximation:
\begin{enumerate}
\item Input: ALE function $f_{j,ALE}$, feature $x_j$, max. number of segments $max_{seg}$, Allowed error $\epsilon$
\item Output: Approximiation model $g_{j}$
\item If $f_{j,ALE}(x) = 0$ for any x, stop and return $g_j(x)=0$
\item For $k \in 1,\ldots,max_{seg}$:
\begin{itemize}
\item If feature $x_j$ is categorical, order by increasing $f_{j, ALE}(x)$ per level.
\item Minimize SSE for given break points in x (for numerical) or in $f_{j, ALE}(x)$ for categorical
\item For numerical we fit linear models in each segment, for categorical just the mean of the levels in a segment.
\item If $R^2 < 1-\epsilon$, stop and return.
\end{itemize}
\end{enumerate}


Optimization problem for numerical features:

Given a fixed number $K$ of intervals $I_k$ defined by K-1 break points $z_k$: $[\min(x_j), z_1], ]z_1, z_2], \ldots ] z_k,\max(x_j)]$

$$\argmin{z_1,\ldots,z_{K-1}} \sum_{k=1}{K}\sum_{x^{(i)} \in I_k} (f_{j,ALE}(x^{(i)}) - (\beta_{0,k} + \beta_{1,k} x^{(i)}))^2$$

where $\beta_{0,k}$ and $\beta_{1,k}$ are the ordinary least square estimates in interval $I_k$.


Optimization problem for numerical features:
Given a fixed number $K$ of intervals $I_k$ defined by K-1 break points $z_k$:

$$x^{(i)} \in I_k: z_{k-1} \leq f_{j,ALE}(x^{(i)}) < z_{k}$$
where  $z_0 = min(f_{j,ALE}(x^{(i)})) i=1,\ldots,n$ and $z_k = max(f_{j,ALE}(x^{(i)})) i=1,\ldots,n$

$$\argmin{z_1,\ldots,z_{K-1}} \sum_{k=1}{K}\sum_{i: x^{(i)} \in I_k} (f_{j,ALE}(x^{(i)}) - c)^2$$

where $c = \frac{1}{n}\sum_{i: x^{(i)} \in I_k} f_{j,ALE}(x^{(i)})$ is the mean  ALE in that interval.


We look at each of the ALE plots and  answer: How many weights would we need if we were to approximate all of the ALE plots with a linear model

The following figure shows an ALE plot for a feature from the Friedman2 formula from mlbench TODO:CITE Friedman paper.
We generated 500 data points from this formula an trained a random forest to predict.

$$y = (x_1^2 + (x_2 \cdot x_3 - (1/(x_2 \cdot x_4)))^2)^{0.5} + e$$

Input are uniformly distributed in ranges:
$$0\leq x_1 \leq 100$$
$$ 40\pi \leq x_2 \leq 560 \pi$$
$$ 0 \leq x_3 \leq 1$$
$$ 1 \leq x_4 \leq 11$$

and

$$ e \sim N(0,125)$$
<<fun-complex>>=
n = 300
grid.size = 40
dat1 = mlbench::mlbench.friedman2(n)
dat1 = data.frame(cbind(dat1$x, y = dat1$y))

dat1$V1 = cut(dat1$V1, breaks = 100 * c(0, 0.2, 0.4, 0.6, 0.8, 1), include.lowest = TRUE)
dat1$V2 = -dat1$V2
tsk = makeRegrTask(data = dat1, target = "y")
lrn = makeLearner("regr.ranger")
mod = train(lrn, tsk)
pred.ksvm = iml::Predictor$new(mod, dat1)
fc = FunComplexity$new(pred.ksvm, epsilon = 0.05, max_feat_cost = 10, grid.size = grid.size)
#FeatureEffects$new(pred.ksvm, grid.size = grid.size)$plot(nrow = 1)
plot(fc, nrow = 1)

# Introduce second epsilon
fc$plot(fc$relevant_features)

fc$complexity_wtotal
fc$complexity_wtotal2
@


Best step and best piece-wise linear models for $epsilon = 0.05$

Categorical feature
<<depends = "fun-complex">>=
plot(fc$approx_models$V1)
@


Numerical feature
<<depends = "fun-complex">>=
plot(fc$approx_models$V4)
@



The effect of different values for $epsilon$, here on the piece-wise linear model:
<<depends = "fun-complex">>=
fc = FunComplexity$new(pred.ksvm, epsilon = 0.05, max_feat_cost  = 10, grid.size = grid.size)
p1 = plot(fc$approx_models$V4)

fc = FunComplexity$new(pred.ksvm, epsilon = 0.001, max_feat_cost = 10, grid.size = grid.size)
p2 = plot(fc$approx_models$V4)

fc = FunComplexity$new(pred.ksvm, epsilon = 0.05, max_feat_cost = 10, grid.size = grid.size)
p3 = plot(fc$approx_models$V1)

fc = FunComplexity$new(pred.ksvm, epsilon = 0.001, max_feat_cost = 10, grid.size = grid.size)
p4 = plot(fc$approx_models$V1)

gridExtra::grid.arrange(p1, p2, p3, p4, nrow = 2, ncol = 2)

@


<<depends = "fun-complex">>=
fc.ksvm = FunComplexity$new(pred.ksvm, epsilon = 0.05, max_feat_cost = 10, grid.size = grid.size)
p1 = plot(fc.ksvm, nrow = 1)

lrn = makeLearner("regr.rpart")
#lrn = makeLearner("regr.kknn")

mod = train(lrn, tsk)
pred.rpart = iml::Predictor$new(mod, dat1)
fc.rpart = FunComplexity$new(pred.rpart, epsilon = 0.05, max_feat_cost = 10, grid.size = grid.size)

p2 = plot(fc.rpart, nrow = 1)

gridExtra::grid.arrange(p1, p2, row = 2)
@


Things that did not work out:
\begin{itemize}
\item Version for cubic splines approx
\item Version for tree split approx
\end{itemize}

