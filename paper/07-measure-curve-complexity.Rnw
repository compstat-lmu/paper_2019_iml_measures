\Sexpr{set_parent('paper.Rnw')}
\subsection{Average Main Effect Complexity (AMEC)}
\label{sec:curve}

% =============================================================================
% Measure Intro
% =============================================================================
<<c-demo-prepare>>=
n = 300
x = runif(n = n)
y = log(x)
dat = data.frame(y = y, x = x, e = rnorm(n, sd = 0.1))
mod = mgcv::gam(y ~ s(x), data = dat)
pred = Predictor$new(mod, dat)
fe = FeatureEffect$new(pred, "x")
app = AleNumApprox$new(ale = fe, max_seg = 2, epsilon = 0.05)
@

We propose MEC, a measure for the curve complexity of the ALE main effects $\falej$.
For an individual ALE curve we measure complexity as the number of parameters we need to approximate the curve with linear segments.
We compute the main effect complexity for the entire prediction function as the average main effect complexity, weighted by the variance of each curve.
Figure~\ref{fig:c-demo} shows an exemplary ALE plot (= main effect) and an approximation with linear segments.
Throughout the paper, we will represent main effect ALE curves with sparklines, e.g. Figure~\ref{fig:c-demo} looks like this in-text \Sexpr{spark(app)}.
Vertical bars -- if present -- represent the segments the curve was divided into for approximation.


<<epsilons,  fig.cap = "Effect of different epsilons on complexity of approximation. On the left the ALE plot can be approximated with a single linear segment for the given epsilon of 0.05. On the right two segments are needed to approximate the ALE curve when epsilon is set to 0.001.">>=

n = 1000
grid.size = 50
dat1 = mlbench::mlbench.friedman2(n)
dat1 = data.frame(cbind(dat1$x, y = dat1$y))
tsk = makeRegrTask(data = dat1, target = "y")
lrn = makeLearner("regr.ranger")
mod = train(lrn, tsk)
pred.ksvm = iml::Predictor$new(mod, dat1)
fc = FunComplexity$new(pred.ksvm, epsilon = 0.01,grid.size = grid.size)

eps1 = 0.05
fc1 = FunComplexity$new(pred.ksvm, epsilon = eps1,  grid.size = grid.size)
#p1 = plot(fc$approx_models$V3)

eps2 = 0.005
fc2 = FunComplexity$new(pred.ksvm, epsilon = eps2, grid.size = grid.size, max_seg_num = 10)

eps3 = 0.001
fc3 = FunComplexity$new(pred.ksvm, epsilon = eps3, grid.size = grid.size, max_seg_num = 10)
#p2 = plot(fc$approx_models$V3)
#gridExtra::grid.arrange(p1, p2, nrow = 1)
@

<<c-demo2, fig.cap = "Two identical ALE curves with the difference that one has a jump at 0.5. The measures complexity is higher for the left curve, since we need more segments to describe the variance of the curve. Both have an R squared above 0.95">>=
n = 3000
grid.size = 100
x = runif(n = n)
y = 0.5 * sin(15 * x)
dat = data.frame(y = y, x = x, e = rnorm(n, sd = 0.1))
mod = mgcv::gam(y ~ s(x), data = dat)
pred = Predictor$new(mod, dat)
fe = FeatureEffect$new(pred, "x", grid.size = grid.size)
app1_wiggle = AleNumApprox$new(ale = fe, max_seg = 6, epsilon = 0.05)
#p1 = plot(app1)  + scale_y_continuous(limits = c(-7,7))

# add
dat2 = dat
dat2$y = 0.5 * sin(20 * dat$x) + 10 * as.numeric(dat$x >0.5)

mod = mgcv::gam(y ~ s(x) + I(x >0.5), data = dat2)
pred = Predictor$new(mod, dat2)
fe = FeatureEffect$new(pred, "x", grid.size = grid.size)
app2_jump = AleNumApprox$new(ale = fe, max_seg = 5, epsilon = 0.05)
#p2 = plot(app2) + scale_y_continuous(limits = c(-7,7))
#grid.arrange(p1, p2, nrow = 1)
@


<<c-demo3, fig.cap = "Three ALE curves that only differ by a scaling factor in  dicrection of the y-axis. All three need the same amount of segments to be approximated with R squared above 0.95">>=
n = 3000
grid.size = 100
x = runif(n = n)


y = 0.2 * sin(15 * x)
dat = data.frame(y = y, x = x, e = rnorm(n, sd = 0.1))
mod = mgcv::gam(y ~ s(x), data = dat)
pred = Predictor$new(mod, dat)
fe = FeatureEffect$new(pred, "x", grid.size = grid.size)
app1 = AleNumApprox$new(ale = fe, max_seg = 6, epsilon = 0.05)
p1 = plot(app1)  + scale_y_continuous(limits = c(-7,7))

y = 2 * sin(15 * x)
dat = data.frame(y = y, x = x, e = rnorm(n, sd = 0.1))
mod = mgcv::gam(y ~ s(x), data = dat)
pred = Predictor$new(mod, dat)
fe = FeatureEffect$new(pred, "x", grid.size = grid.size)
app2 = AleNumApprox$new(ale = fe, max_seg = 6, epsilon = 0.05)
p2 = plot(app2)  + scale_y_continuous(limits = c(-7,7))

y = 6 * sin(15 * x)
dat = data.frame(y = y, x = x, e = rnorm(n, sd = 0.1))
mod = mgcv::gam(y ~ s(x), data = dat)
pred = Predictor$new(mod, dat)
fe = FeatureEffect$new(pred, "x", grid.size = grid.size)
app3 = AleNumApprox$new(ale = fe, max_seg = 6, epsilon = 0.05)
p3 = plot(app3)  + scale_y_continuous(limits = c(-7,7))
#grid.arrange(p1, p2, p3, nrow  = 1)
@


<<c-demo, fig.cap = "ALE curve (black line) approximated by two linear segments. Segments are vertical lines, approximation the doted lines.", out.height ="4cm", out.width="10cm", fig.height=4, fig.width=10, fig.align="center", dependson="c-demo-prepare">>=
plot(app) + ggtitle("") + scale_y_continuous("ALE")
@

% =============================================================================
% Measure Intuition
% =============================================================================
The procedure to compute MEC is described in Algorithm~\ref{algo:AMEC}.
The motivation behind approximating the ALE curves is to replace the curve with a function of known complexity, in this case measured as the number of parameters needed to describe the curve with linear segments, a similar concept as the degrees of freedom often used in linear regression models.
Another goal was to allow the approximation to have some error, so that a curve that wiggles around a straight line to be approximated by a straight line, e.g. we would like the curve \Sexpr{spark(fc1$approx_models$V3$ale)} be approximated with a single linear segment.
We measured the approximation with $R^2$ of the approximation vs. the full ALE curve and allow the $R^2$ be 1 - $\epsilon$ as defined by the user.
% =============================================================================
% Measure Sum Up and Fix
% =============================================================================
First we compute the main effect complexity for each feature individually and then average the results.
If we average the main effect complexities without weighting, we run into the problem that some effects might be very complex, but don't contribute much to the prediction and we visually interpret the feature effect as having no impact and being wiggly around effect of zero.
For example, the two ALE curves  \Sexpr{spark(app1_wiggle$ale, c(-7, 7), width = 5)} and \Sexpr{spark(app2_jump$ale, c(-7, 7), width = 5)} are identical except for the jump at 0.5 in the second curve.
We need more linear segments to get the same approximation $R^2$ as for the second curve.
One way to mitigate the problem of getting high AMEC estimates through wiggly main effects that are irrelevant for the prediction is to weight each main effect by its variance.
Let's consider 3 main effect curves \Sexpr{spark(app1$ale, c(-6, 6), width = 5)}, \Sexpr{spark(app2$ale, c(-6, 6), width = 5)} and \Sexpr{spark(app3$ale, c(-6, 6), width = 5)}, which only differ by a scaling factor in the y-axis.
All three curves need the five segments to be approximated with  $R^2 >= 0.95$:
 \Sexpr{spark(app1, c(-6, 6), width = 5)}, \Sexpr{spark(app2, c(-6, 6), width = 5)} and \Sexpr{spark(app3, c(-6, 6), width = 5)}
When weighted by the variance,  \Sexpr{spark(app1$ale, c(-6, 6), width = 5)} gets a weight of \Sexpr{sprintf("%.2f", app1$var)}, \Sexpr{spark(app2$ale, c(-6, 6), width = 5)} a weight of \Sexpr{sprintf("%.2f", app2$var)} and \Sexpr{spark(app3$ale, c(-6, 6), width = 5)} a weight of \Sexpr{sprintf("%.2f", app3$var)}.

% =============================================================================
% Algorithm for complexity
% =============================================================================
\begin{algorithm}
\caption{Average main effect complexity (AMEC).}\label{algo:AMEC}
	\KwInput{$\fh$, allowed error $\epsilon$, segments limit $max_{seg}$, features $x_j,j\in \pset$}
	\KwOutput{Average Main Effect Complexity AMEC}
	\For{$j \in \pset$}{
Estimate ALE $\falej$\;
	%\If{$\falej(x_j) = 0$ for all $x$}{
	%	$MEC_j = 0, V_j=0$\;
	%	Continue with next feature\;
	%	}
	Approximate $\falej$ with $g_j(x_j) = \beta_0 + \beta_1 x_j$\;
	Compute $R^2$ of $g_j(x_j)$\;
	Set $K=0$\;
	\While{$K \leq max_{seg}$ AND $R^2 < (1- \epsilon)$}{
		Set $K = K+1$\;
		\tcp{Optimize intervals $Z_k$ via generalized simulated annealing, $\beta$'s per segment with ordinary least squares}
		\tcp{For categorical feature, fix slope to zero} 
		$g_j = \sum_{k=1}^{K+1} \I_{\xi\in Z_k}\left(\hat{\beta}_0^k + \hat{\beta}_j^k\xij\right)$

		Compute $R^2$ of $g_j$
	}
	\tcp{Post-processing of slopes}
	\For{$k \in 1,\ldots,K$}{
		Set $\beta_{j0}^k = \frac{1}{n_k}\sum_{\xi\in Z_k}\falej(\xij)$ and $\beta_{j1}^k = 0$ in $g_j$\;
		\lIf{$R^2(g_j) > (1-\epsilon)$}
		{
			Use new $\beta_{j0}^k$, $\beta_{j1}^k$ in $g_j$
		}
		\lElse{
			Keep old $\beta_{j0}^k$, $\beta_{j1}^k$ in $g_j$	
			}
			}
$MEC_j = K + \sum_{k=1}^K \mathbb{I}_{\beta_k > 0}$\;
$V_j = \frac{1}{n}\sum_{i=1}^n \left(\hat{f}^{ALE}_j(x^{(i)})\right)^2$\;
}
	\Return{$AMEC = \frac{1}{\sum_{j=1}^pV_j}\sum_{j=1}^p V_jMEC_j$\;}
\end{algorithm}






% =============================================================================
% Different epsilons
% =============================================================================
The choice of $\epsilon$ affects the complexity measures:
This ALE main effect \Sexpr{spark(fc1$approx_models$V3$ale)} can be approximated with a linear model \Sexpr{spark(fc1$approx_models$V3, approx = TRUE)} when $\epsilon = \Sexpr{eps1}$ but needs \Sexpr{(fc3$approx_models$V3$n_coefs + 1) / 2} segments when $\epsilon$ is decreased to \Sexpr{eps3}: \Sexpr{spark(fc3$approx_models$V3, approx = TRUE)}.




% =============================================================================
% AMEC ignores internal model complexity and uses external complexity
% =============================================================================
<<tree-increase-data>>=
set.seed(42)
library(rpart)
n = 1000
dat = data.frame(x = runif(n), x2 = rnorm(n))
nsd = 0.4
dat$y = 2 * dat$x + rnorm(n, sd = nsd)

grid.size = 100
epsilon = 0.05
@
The complexity measures ignores how complex the internal structure of the model is, e.g. it does not matter whether the linear relationship comes from a single linear model or a blend of hundreds of linear models.
We demonstrate how the number of nodes and complexity measure C increase with increasing model performacne, but at some point the complexity C decreases again.
For this example, \Sexpr{n} data points were drawn from a uniform distribution between 0 and 1, and the target is simulated as $2 * x + \epsilon$, where $\epsilon \sim N(0,\Sexpr{nsd})$.
We trained three trees with different maximal depths, as shown in Table~\ref{tab:tree}.

<<tree-increase, dependson="tree-increase-data",results="asis">>=
rp_width = function(rp) sum(rp$frame$var == "<leaf>")

rp1 = rpart(y ~ ., data = dat, control = rpart.control(maxdepth = 1))
pred1 = Predictor$new(rp1, data = dat)
fc1 = FunComplexity$new(pred1, grid.size = grid.size, epsilon = epsilon)
#p1 = fc1$approx_models$x$plot() +
#  ggtitle(sprintf("Leaf nodes: %i, C: %i", rp_width(rp1), fc1$c_wmean))

rp2 = rpart(y ~ ., data = dat, control = rpart.control(maxdepth = 2))
pred2 = Predictor$new(rp2, data = dat)
fc2 = FunComplexity$new(pred2, grid.size = grid.size, epsilon = epsilon)
#p2 = fc2$approx_models$x$plot() +
#  ggtitle(sprintf("Leaf nodes: %i, C: %i", rp_width(rp2), fc2$c_wmean))

rp3 = rpart(y ~ ., data = dat, control = rpart.control(maxdepth = 3))
pred3 = Predictor$new(rp3, data = dat)
fc3 = FunComplexity$new(pred3, grid.size = grid.size, epsilon = epsilon)
#p3 = fc3$approx_models$x$plot() +
#  ggtitle(sprintf("Leaf nodes: %i, C: %i", rp_width(rp3), fc3$c_wmean))

#grid.arrange(p1, p2, p3, nrow = 1)
n_terminal = function(tree) {
  nrow(tree$frame[tree$frame$var == "<leaf>",])
}

tab = data.frame(
  "Tree Depth" = c(1,2,3),
  "Terminal nodes" = c(n_terminal(rp1), n_terminal(rp2),n_terminal(rp3)),
  "ALE" = c(spark(fc1$approx_models$x),spark(fc2$approx_models$x),spark(fc3$approx_models$x)),
  "Approximation" = c(
    spark(fc1$approx_models$x, approx = TRUE),
    spark(fc2$approx_models$x, approx = TRUE),
    spark(fc3$approx_models$x, approx = TRUE)),
  "AMEC" = c(fc1$c_wmean, fc2$c_wmean, fc3$c_wmean))

caption = "Increasing the tree depth always increases number of terminal nodes, but the complexity measure increases at some point because the prediction function can be approximated by a single linear function."
print.xtable(xtable(tab, caption = caption), sanitize.text.function = as.is, booktabs = TRUE, label = "tab:tree")
@



% =============================================================================
% =============================================================================
% =============================================================================
% Material
% =============================================================================
% =============================================================================
% =============================================================================

%If all features in the data are uncorrelated, this has the nice interpretation that each feature main effect complexity (MEC) is weighted by the percentage it contributed to the variance of the complete main effects model.
%In case of correlated features, it's more complicated.
%If we were to fairly assign to each feature the share of the first order model variance, we would need an attribution method such as the Shapley value.
%The big problem is that the contribution can be negative when features are correlated and one of the correlated features has a much stronger effect.
%This can happen when the covariance is negative.
%The interpretation is not very intuitive then.
%An advantage of weighting by the individual variances is that it has a visual connection:
%The smaller the variance, the lower the amplitude of the curve in y-direction.
%And we want that weighted by the data distribution, so taking the minimum and maximu for scaling is not an option.


% =============================================================================
% Find best splits
% =============================================================================
%\begin{algorithm}
%\caption{ApproxLinearSegments}\label{algo:aleapprox}
%	\KwInput{K, $\falej$, $\epsilon$}
%	\KwOutput{$\beta \in \mathbb{R}^k, R^2$}
%	Define equidistant initial breakpoints: $\hat{z}_1,\ldots, \hat{z}_K$\;
%	Define segments $Z_1 = [min(\xj), z_1], \ldots, Z_k = [z_{k-1}, z_k],\ldots,Z_{K+1} = [z_K,max(\xj)]$ \\
%If $K=0$, then $Z_1 = [min(\xj), max(\xj)]$\;
%
%	Optimize break points: $\hat{z}_1,\ldots, \hat{z}_K = argmin_{z_1,\ldots,z_K} \sum_{k=1}^{K+1} \sum_{\xi\in Z_k}\left(\falej(\xij) - (\beta_{j0}^k + \beta_{j1}^k\xij)\right)^2$ where and $\beta_{j0}^k$ and $\beta_{j1}^k$ are the least square estimates. 
%Optimize with Generalized Simulated annealing.
%
%$\galej(\xj) = \sum_{k=1}^{K+1} \I_{\xi\in Z_k}\left(\hat{\beta}_0^k + \hat{\beta}_j^k\xij\right)$.
%Set slopes to zero. For each segment $k \in 1,\ldots,K$:
%\begin{itemize}
%\item Set $\beta_{j0}^k = \frac{1}{n_k}\sum_{\xi\in Z_k}\falej(\xij)$ and $\beta_{j1}^k$ = 0
%\item If still $R^2 > 1 - \epsilon$, overwrite both $\beta$.
%\end{itemize}
%
%* If the feature is numeric, fix all $\beta_{j1}^k$ at zero and we applied random search of break points instead of GenSA.
%\end{algorithm}


% =============================================================================
% C example
% =============================================================================
% We illustrate the measure with a short example.
% We simulated 500 data points with 4 features as a regression problem \citep{friedman1991multivariate}.
% The features are uniformly distributed in the following intervals: $0\leq x_1 \leq 100$, $ 40\pi \leq x_2 \leq 560 \pi$, $ 0 \leq x_3 \leq 1$, $ 1 \leq x_4 \leq 11$.
% The regression target was simulated as:
%
% $$y = (x_1^2 + (x_2 \cdot x_3 - (1/(x_2 \cdot x_4)))^2)^{0.5} + e$$
% where $ e \sim N(0,125)$.
%
% We trained a random forest, plotted the ALEs and calculated the complexities with the parameters $\epsilon = 0.95$, $max_{segnum} = 5$ and $max_{segcat} = 9$


% Since the model relied the most on $x2$ and $x3$ as main effects, the weighted main effect complexity is \Sexpr{fc$c_wmean}.
% Both features main effects can be approximated with a linear segment.
% The other two feature have a very low variance and get a low weight.

<<spark-table, dependson="epsilons", results="asis", eval=FALSE>>=
dat = data.frame(test = c(1,2), sp = c(spark(fc$approx_models$V1), spark(fc$approx_models$V2)), stringsAsFactors = FALSE)
print.xtable(xtable(dat), sanitize.text.function = as.is)
@


% =============================================================================
% Did not work out
% =============================================================================
% We tried a few alternatives to approximate the ALE plots for measuring their complexity, which turned out to be impractical.
% We tried approximation with cubic (overlapping) splines and measuring the complexity as degrees of freedom.
% The degrees of freedom exploded when the ALE plot was not quite linear, but almost, yielding an unintuitive measure.
% Tree splits did not work, like for example model based partitioning TODO: CITE mob.
% One reason is that the greedy approach did not work well and we found it better to search per number of splits.



