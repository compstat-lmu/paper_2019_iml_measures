\Sexpr{set_parent('paper.Rnw')}
\subsection{Measure ALE First Order Complexity}
\label{sec:curve}

We propose a measure that, for each of the main effects, approximates the ALE curve with linear segments and adds up the parameters needed if we were to model all those segments in a linear regression model.

Recipe:

\begin{enumerate}
\item Input: Model f, PRL-bound $\epsilon$, upper bound complexity $c_{max}$
\item For all features $x_1,\ldots,x_p$:
\begin{itemize}
\item Estimated first order ALE
\item Compute the approximation using the least amount of linear segments possible while still having a PRL above $1-\epsilon$
\item Measure complexity of segments as number of weights needed when approximating the segments with a piecewise linear regression model ($m_j$)
\item Compute complexity of best step-wise function (segments with slope 0) approximation model $m_{j,step}$ using ctree TODO: CITE
\item Compute complexity of best iece-wise linear  approximation model $m_{j,seg}$ using mob TODO: CITE
\item $c_j = min(c_{max}, m_{j, seg}, m_{j, step})$
\end{itemize}
\item Sum up all complexity measures $c_j$: $c = \sum_{j=1}^p c_j$
\end{enumerate}


Algorithm for computing best piece-wise model or step-wise model for a single feature.
\begin{enumerate}
\item Fully grow tree with feature $x_j$ predicting $f_{j,ALE}$
\item If  PRL of fit is below $1-\epsilon$ return max-c
\item Prune back tree until $PRL > 1-\epsilon$.
\item Count segments
\end{enumerate}

Repeat this algorithm for each of the features and add up all complexities.


The step and segments approximation are both computed with conditional inference and model-based trees, taking into account the data distribution to compute how well the segments apprimate the ALE function based on PRL.

We look at each of the ALE plots and  answer: How many weights would we need if we were to approximate all of the ALE plots with a linear model

The following figure shows an ALE plot for a feature from the Friedman formula.
We generated 200 data points from this formula an trained a random forest to predict.
<<>>=
n = 500
dat1 = mlbench::mlbench.friedman1(n)
dat1 = data.frame(cbind(dat1$x, y = dat1$y))
tsk = makeRegrTask(data = dat1, target = "y")
lrn = makeLearner("regr.rpart")
mod = train(lrn, tsk)
pred = iml::Predictor$new(mod, dat1)

fname = "V1"
ale = FeatureEffect$new(pred, fname)
plot(ale)
@


Best step and best piece-wise linear models for $epsilon = 0.05$
<<>>=
epsilon = 0.05
amod = fit_approx_mod(pred, ale, type = "step", epsilon = epsilon)
cp = count_pieces(amod)
p1 = plot_complexity_(feature_values = dat1[,fname], ale = ale, mod = amod, epsilon = epsilon, complexity = cp)

amod = fit_approx_mod(pred, ale, type = "segment", epsilon = epsilon)
cp = count_pieces(amod)
p2 = plot_complexity_(feature_values = dat1[,fname], ale = ale, mod = amod, epsilon = epsilon, complexity = cp)

gridExtra::grid.arrange(p1, p2, nrow = 1)
@


The effect of different values for $epsilon$, here on the piece-wise linear model:
<<>>=
epsilon = 0.9
amod = fit_approx_mod(pred, ale, type = "segment", epsilon = epsilon)
cp = count_pieces(amod)
p1 = plot_complexity_(feature_values = dat1[,fname], ale = ale, mod = amod, epsilon = epsilon, complexity = cp)


epsilon = 0.1
amod = fit_approx_mod(pred, ale, type = "segment", epsilon = epsilon)
cp = count_pieces(amod)
p2 = plot_complexity_(feature_values = dat1[,fname], ale = ale, mod = amod, epsilon = epsilon, complexity = cp)

epsilon = 0.05
amod = fit_approx_mod(pred, ale, type = "segment", epsilon = epsilon)
cp = count_pieces(amod)
p3 = plot_complexity_(feature_values = dat1[,fname], ale = ale, mod = amod, epsilon = epsilon, complexity = cp)

gridExtra::grid.arrange(p1, p2, p3, nrow = 1)

@



Things that did not work out:
\begin{itemize}
\item Version for cubic splines approx
\item Version for tree split approx
\end{itemize}

