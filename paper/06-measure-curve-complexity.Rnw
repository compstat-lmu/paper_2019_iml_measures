\Sexpr{set_parent('paper.Rnw')}
\subsection{Main Effect Complexity (MEC)}
\label{sec:curve}

% =============================================================================
% Measure Intro
% =============================================================================
<<c-demo-prepare>>=
n = 300
x = runif(n = n)
y = log(x)
dat = data.frame(y = y, x = x, e = rnorm(n, sd = 0.1))
mod = mgcv::gam(y ~ s(x), data = dat)
pred = Predictor$new(mod, dat)
fe = FeatureEffect$new(pred, "x")
app = AleNumApprox$new(ale = fe, max_seg = 2, epsilon = 0.05, m_nf = 100, post_process = FALSE)
@
To determine the average shape complexity of ALE main effects $\falej$, we propose the main effect complexity (MEC) measure.
For a single ALE main effect, we define $\text{MEC}_j$ as the number of parameters needed to approximate the curve with piece-wise linear models.
For the entire model, MEC is the average $\text{MEC}_j$ over all main effects, weighted with their variance.
Figure~\ref{fig:c-demo} shows an ALE plot (= main effect) and its approximation with two linear segments.
%
<<epsilons,  fig.cap = "Effect of different epsilons on complexity of approximation. On the left the ALE plot can be approximated with a single linear segment for the given epsilon of 0.05. On the right two segments are needed to approximate the ALE curve when epsilon is set to 0.001.">>=
n = 1000
grid.size = 50
dat1 = mlbench::mlbench.friedman2(n)
dat1 = data.frame(cbind(dat1$x, y = dat1$y))
tsk = makeRegrTask(data = dat1, target = "y")
lrn = makeLearner("regr.ranger", num.trees = 30)
mod = train(lrn, tsk)
pred.ksvm = iml::Predictor$new(mod, dat1)
fc = FunComplexity$new(pred.ksvm, epsilon = 0.01,grid.size = grid.size)

eps1 = 0.05
fc1 = FunComplexity$new(pred.ksvm, epsilon = eps1,  grid.size = grid.size)
#p1 = plot(fc$approx_models$V3)

eps2 = 0.005
fc2 = FunComplexity$new(pred.ksvm, epsilon = eps2, grid.size = grid.size, max_seg_num = 10)

eps3 = 0.001
fc3 = FunComplexity$new(pred.ksvm, epsilon = eps3, grid.size = grid.size, max_seg_num = 10)
#p2 = plot(fc$approx_models$V3)
#gridExtra::grid.arrange(p1, p2, nrow = 1)
@
%
<<c-demo2, fig.cap = "Two identical ALE curves with the difference that one has a jump at 0.5. The measures complexity is higher for the left curve, since we need more segments to describe the variance of the curve. Both have an R squared above 0.95">>=
n = 3000
grid.size = 100
x = runif(n = n)
y = 0.5 * sin(15 * x)
dat = data.frame(y = y, x = x, e = rnorm(n, sd = 0.1))
mod = mgcv::gam(y ~ s(x), data = dat)
pred = Predictor$new(mod, dat)
fe = FeatureEffect$new(pred, "x", grid.size = grid.size)
app1_wiggle = AleNumApprox$new(ale = fe, max_seg = 6, epsilon = 0.05)
#p1 = plot(app1)  + scale_y_continuous(limits = c(-7,7))

# add
dat2 = dat
dat2$y = 0.5 * sin(20 * dat$x) + 10 * as.numeric(dat$x >0.5)

mod = mgcv::gam(y ~ s(x) + I(x >0.5), data = dat2)
pred = Predictor$new(mod, dat2)
fe = FeatureEffect$new(pred, "x", grid.size = grid.size)
app2_jump = AleNumApprox$new(ale = fe, max_seg = 5, epsilon = 0.05)
#p2 = plot(app2) + scale_y_continuous(limits = c(-7,7))
#grid.arrange(p1, p2, nrow = 1)
@
%
%
<<c-demo3, fig.cap = "Three ALE curves that only differ by a scaling factor in  dicrection of the y-axis. All three need the same amount of segments to be approximated with R squared above 0.95">>=
n = 3000
grid.size = 100
x = runif(n = n)
y = 0.2 * sin(15 * x)
dat = data.frame(y = y, x = x, e = rnorm(n, sd = 0.1))
mod = mgcv::gam(y ~ s(x), data = dat)
pred = Predictor$new(mod, dat)
fe = FeatureEffect$new(pred, "x", grid.size = grid.size)
app1 = AleNumApprox$new(ale = fe, max_seg = 6, epsilon = 0.05)
p1 = plot(app1)  + scale_y_continuous(limits = c(-7,7))

y = 2 * sin(15 * x)
dat = data.frame(y = y, x = x, e = rnorm(n, sd = 0.1))
mod = mgcv::gam(y ~ s(x), data = dat)
pred = Predictor$new(mod, dat)
fe = FeatureEffect$new(pred, "x", grid.size = grid.size)
app2 = AleNumApprox$new(ale = fe, max_seg = 6, epsilon = 0.05)
p2 = plot(app2)  + scale_y_continuous(limits = c(-7,7))

y = 6 * sin(15 * x)
dat = data.frame(y = y, x = x, e = rnorm(n, sd = 0.1))
mod = mgcv::gam(y ~ s(x), data = dat)
pred = Predictor$new(mod, dat)
fe = FeatureEffect$new(pred, "x", grid.size = grid.size)
app3 = AleNumApprox$new(ale = fe, max_seg = 6, epsilon = 0.05)
p3 = plot(app3)  + scale_y_continuous(limits = c(-7,7))
#grid.arrange(p1, p2, p3, nrow  = 1)
@
%
%
<<c-demo, fig.cap = "ALE curve (solid line) approximated by two linear segments (dotted line).", out.height ="4cm", out.width="10cm", fig.height=4, fig.width=10, fig.align="center", dependson="c-demo-prepare">>=
plot(app) + ggtitle("") + scale_y_continuous("ALE")
@
% =============================================================================
% Measure Intuition
% =============================================================================
We use segmented linear regression to approximate the ALE curve.
Within the segments, linear models are estimated with ordinary least squares.
The breakpoints that define the segments are found by greedy and exhaustive search along the interval boundaries of the ALE curve.
Greedy here means that e.g. when fitting a segmented linear model with two breakpoints, we first optimize first breakpoint and then the second breakpoint with the first breakpoint fixed.
We measure the degrees of freedom as the number of non-zero coefficients for intercepts and slopes of the linear models.
The approximation allows some error, e.g., an almost linear main effect may have $\text{MEC}_j=1$, even if dozens of parameters would be needed to describe it perfectly. 
The approximation quality is measured with R-squared ($R^2$), i.e., the proportion of variance of $\falej$ that is explained by the approximation with linear segments.
An approximation has to reach an $R^2 \geq 1-\epsilon$, where $\epsilon$ is the user defined maximum approximation error.
We also introduced parameter $max_{seg}$, the maximum number of segments.
In the case that an approximation cannot reach an $R^2 \geq 1-\epsilon$ with a given $max_{seg}$, $\text{MEC}_j$ is computed with the maximum number of segments.
The selected maximum approximation error $\epsilon$ should be small, but not too small.
We found $\epsilon$ between $0.01$ and $0.1$ visually meaningful (i.e. a subjectively good approximation) and used $\epsilon=0.05$ throughout the paper.
We apply a post-processing step that greedily sets slopes of the linear segments to zero, as long as $R^2\in\{1-\epsilon,1\}$.
The post-processing potentially decreases the $\text{MEC}_j$, especially for models with constant segments like decision trees.
% =============================================================================
% Measure Sum Up and Fix
% =============================================================================
$\text{MEC}_j$ is averaged over all features to obtain the global main effect complexity.
Each $\text{MEC}_j$ is weighted with the variance of the corresponding ALE main effect to give more weight to features that contribute more to the prediction.
%For example, the two ALE curves  \Sexpr{spark(app1_wiggle$ale, c(-7, 7), width = 5)} and \Sexpr{spark(app2_jump$ale, c(-7, 7), width = 5)} are identical except for the jump at 0.5 in the second curve.
%We need more linear segments to get the same approximation $R^2$ as for the second curve.
%One way to mitigate the problem of getting high MEC estimates through wiggly main effects that are irrelevant for the prediction is to weight each main effect by its variance.
%For example, the  main effect curves \Sexpr{spark(app1$ale, c(-6, 6), width = 5)}, \Sexpr{spark(app2$ale, c(-6, 6), width = 5)} and \Sexpr{spark(app3$ale, c(-6, 6), width = 5)} differ from each other by a scaling factor in the y-axis, but each requires five segments to be approximated with $R^2 >= 0.95$:
% \Sexpr{spark(app1, c(-6, 6), width = 5)}, \Sexpr{spark(app2, c(-6, 6), width = 5)}, and \Sexpr{spark(app3, c(-6, 6), width = 5)}.
%When weighted with the variance,  \Sexpr{spark(app1$ale, c(-6, 6), width = 5)} gets a weight of \Sexpr{sprintf("%.2f", app1$var)}, \Sexpr{spark(app2$ale, c(-6, 6), width = 5)} a weight of \Sexpr{sprintf("%.2f", app2$var)} and \Sexpr{spark(app3$ale, c(-6, 6), width = 5)} a weight of \Sexpr{sprintf("%.2f", app3$var)}.
Algorithm~\ref{algo:AMEC} describes the MEC computation in detail.

% =============================================================================
% Algorithm for complexity
% =============================================================================
\begin{algorithm}
\caption{Main Effect Complexity (MEC).}\label{algo:AMEC}
	\KwInput{Model $f$, approximation error $\epsilon$, max. segments $max_{seg}$, data $\D$}
	Define $R^2(g_j, \falej) := \sum_{i=1}^n (g_j(\xij) - \falej(\xij))^2 / \sum_{i=1}^n (\falej(\xij))^2$\;
	\For{$j \in \pset$}{
        Estimate $\falej$\;
	\tcp{Approximate ALE with linear model}
	Fit $g_j(x_j) = \beta_0 + \beta_1 x_j$ predicting $\falej(\xij)$ from $\xij$, $i\in{1,\ldots,n}$\;
	Set $K=1$\;
	\tcp{Increase nr. of segments until approximation is good enough}
	\While{$K < max_{seg}$ AND $R^2(g_j,\falej) < (1- \epsilon)$}{
	        \tcp{Find intervals $Z_k$ through exhaustive search along ALE curve breakpoints}
		\tcp{For categorical feature, set slopes $\beta_{1,k}$ to zero}
		$g_j(x_j) = \sum_{k=1}^{K+1} \I_{\xj\in Z_k}\cdot\left(\beta_{0,k} + \beta_{1,k}\xj\right)$\;
		Set $K = K+1$
	}
	Greedily set slopes to zero while $R^2>1-\epsilon$\;
	%\tcp{Post-processing of slopes}
	%\For{$k \in 1,\ldots,K$}{
	%	\tcp{$n_k$ is number of instances in interval $Z_k$}
	%	Set $\beta_{0,k} = \frac{1}{n_k}\sum_{i:\xi\in Z_k}\cdot\falej(\xij)$ and $\beta_{1,k} = 0$ in $g_j$\;
	%	\lIf{$R^2(g_j, \falej) > (1-\epsilon)$}
	%	{
	%		Use new $\beta_{0,k}$, $\beta_{1,k}$ in $g_j$
	%	}
	%	\lElse{
	%		Keep old $\beta_{0,k}$, $\beta_{1,k}$ in $g_j$
	%		}
	%		}
\tcp{Sum of non-zero coefficients minus first intercept}
	$MEC_j = K + \sum_{k=1}^K \mathbb{I}_{\beta_{1,k} > 0} - 1$\;
$V_j = \frac{1}{n}\sum_{i=1}^n (\falej(x^{(i)}))^2$\;
}
	\Return{$MEC = \frac{1}{\sum_{j=1}^pV_j}\sum_{j=1}^p V_j \cdot MEC_j$\;}
\end{algorithm}



% =============================================================================
% Experiment
% =============================================================================
<<MEC-experiment, eval=FALSE>>=
data("BostonHousing", package = "mlbench")
BostonHousing$chas = as.numeric(BostonHousing$chas)
# Build different MARS model
tsk = makeRegrTask(data = BostonHousing, target = "medv")
lrn = makeLearner("regr.earth")
mod = train(lrn, tsk)


experiment_slopes = function(epsilon, m){
  epsilon = 0.05
  m = 6
  pred = Predictor$new(mod, data = BostonHousing)
  fc = FunComplexity$new(pred, epsilon = epsilon, grid.size = 200, max_seg_num = m, post_process = FALSE)

  extract_slope_approx = function(fc, dat, feature) {
    if(is.null(fc$approx_models[[feature]]$segments)) return(0)
    dat = data.frame(feature = dat[,feature])
    dat$interval = cut(dat$feature, breaks = fc$approx_models[[feature]]$breaks, include.lowest = TRUE)
    merge(data.table(dat), fc$approx_models[[feature]]$segments, sort = FALSE)$slope
  }

  coefs_approx = lapply(pred$data$feature.names, function(fname) {
    extract_slope_approx(fc, BostonHousing, fname)
  })

  coefs_approx_df = data.frame(coefs_approx)
  colnames(coefs_approx_df) = pred$data$feature.names

  # Create table of coefficients for data for earth model
  mm = earth:::model.matrix.earth(mod$learner.model, BostonHousing)
  # data.frame indicating per data point which piece-wise slope was used
  mm0 = sign(mm)
  # directions of the coefficients
  dirs = rowSums(mod$learner.model$dirs)
  # Calculating the coefficients
  cfs = mod$learner.model$coefficients[,1]
  cfs = diag(dirs[names(cfs)] * cfs)
  cfsm = mm0 %*% cfs
  colnames(cfsm) = colnames(mm)
  # Matching coefficients from multiple pieces to each feature
  cfsmm = melt(cfsm)
  cfsmm = cfsmm[cfsmm$Var2 != "(Intercept)",]
  cfsmm$Var2 = gsub("h(", "", cfsmm$Var2, fixed = TRUE)
  cfsmm$Var2 = gsub(")", "", cfsmm$Var2, fixed = TRUE)
  cfsmm$Var2 = gsub("[-\\.0-9]", "", cfsmm$Var2)
  # Getting data into usable format
  cfsmm = data.table(cfsmm)
  cfsmm = cfsmm[,.(value = sum(value)),by = c("Var1", "Var2")]
  cfsmm = dcast(cfsmm, formula = Var1 ~ Var2)
  earth_coefs  = data.frame(cfsmm)[,-1]
  # Computation of cosine similarity
  angle = function(s1,s2) {
    (1 + s1 * s2)/(sqrt(1 + s1^2) * sqrt(1 + s2^2))
  }
  cosines = angle(unlist(earth_coefs), unlist(coefs_approx_df[,colnames(earth_coefs)]))
  mean(cosines >= 0.999)
}

e1 = experiment_slopes(0.01, 5)
e1
e2 = experiment_slopes(0.05, 5)
e2
e3 = experiment_slopes(0.01, 10)
## Part two, extract the segments



# extract breaks from earth; named list, one vector per feature
# extract breaks from approx; named list, one vector per feature

@

%We tested the extraction of linear segments with a model for which we can validate if the correct segments were extracted: Multivariate Adaptive Regression Splines (MARS).
%In our experiment, we used the Boston housing data, trained a MARS model, extracted MEC and compared the slopes estimated by MARS and the ones estimated by our ALE approximation which is computed for the MEC.
%For each datapoint and each feature, we computed both the slopes for MARS and ALE approximation, and compared both using the cosine similarity.
%We see the lines at a data point as equivalent if cosine distance $\in\{0.999,1\}$.


% =============================================================================
% =============================================================================
% =============================================================================
% Material
% =============================================================================
% =============================================================================
% =============================================================================

% =============================================================================
% AMEC ignores internal model complexity and uses external complexity
% =============================================================================
<<tree-increase-data, eval=FALSE>>=
set.seed(42)
library(rpart)
n = 1000
dat = data.frame(x = runif(n), x2 = rnorm(n))
nsd = 0.4
dat$y = 2 * dat$x + rnorm(n, sd = nsd)

grid.size = 100
epsilon = 0.05
@



%If all features in the data are uncorrelated, this has the nice interpretation that each feature main effect complexity (MEC) is weighted by the percentage it contributed to the variance of the complete main effects model.
%In case of correlated features, it's more complicated.
%If we were to fairly assign to each feature the share of the first order model variance, we would need an attribution method such as the Shapley value.
%The big problem is that the contribution can be negative when features are correlated and one of the correlated features has a much stronger effect.
%This can happen when the covariance is negative.
%The interpretation is not very intuitive then.
%An advantage of weighting by the individual variances is that it has a visual connection:
%The smaller the variance, the lower the amplitude of the curve in y-direction.
%And we want that weighted by the data distribution, so taking the minimum and maximu for scaling is not an option.


% =============================================================================
% Find best splits
% =============================================================================
%\begin{algorithm}
%\caption{ApproxLinearSegments}\label{algo:aleapprox}
%	\KwInput{K, $\falej$, $\epsilon$}
%	\KwOutput{$\beta \in \mathbb{R}^k, R^2$}
%	Define equidistant initial breakpoints: $\hat{z}_1,\ldots, \hat{z}_K$\;
%	Define segments $Z_1 = [min(\xj), z_1], \ldots, Z_k = [z_{k-1}, z_k],\ldots,Z_{K+1} = [z_K,max(\xj)]$ \\
%If $K=0$, then $Z_1 = [min(\xj), max(\xj)]$\;
%
%	Optimize break points: $\hat{z}_1,\ldots, \hat{z}_K = argmin_{z_1,\ldots,z_K} \sum_{k=1}^{K+1} \sum_{\xi\in Z_k}\left(\falej(\xij) - (\beta_{j0}^k + \beta_{j1}^k\xij)\right)^2$ where and $\beta_{j0}^k$ and $\beta_{j1}^k$ are the least square estimates.
%Optimize with Generalized Simulated annealing.
%
%$\galej(\xj) = \sum_{k=1}^{K+1} \I_{\xi\in Z_k}\left(\hat{\beta}_0^k + \hat{\beta}_j^k\xij\right)$.
%Set slopes to zero. For each segment $k \in 1,\ldots,K$:
%\begin{itemize}
%\item Set $\beta_{j0}^k = \frac{1}{n_k}\sum_{\xi\in Z_k}\falej(\xij)$ and $\beta_{j1}^k$ = 0
%\item If still $R^2 > 1 - \epsilon$, overwrite both $\beta$.
%\end{itemize}
%
%* If the feature is numeric, fix all $\beta_{j1}^k$ at zero and we applied random search of break points instead of GenSA.
%\end{algorithm}


% =============================================================================
% C example
% =============================================================================
% We illustrate the measure with a short example.
% We simulated 500 data points with 4 features as a regression problem \citep{friedman1991multivariate}.
% The features are uniformly distributed in the following intervals: $0\leq x_1 \leq 100$, $ 40\pi \leq x_2 \leq 560 \pi$, $ 0 \leq x_3 \leq 1$, $ 1 \leq x_4 \leq 11$.
% The regression target was simulated as:
%
% $$y = (x_1^2 + (x_2 \cdot x_3 - (1/(x_2 \cdot x_4)))^2)^{0.5} + e$$
% where $ e \sim N(0,125)$.
%
% We trained a random forest, plotted the ALEs and calculated the complexities with the parameters $\epsilon = 0.95$, $max_{segnum} = 5$ and $max_{segcat} = 9$


% Since the model relied the most on $x2$ and $x3$ as main effects, the weighted main effect complexity is \Sexpr{fc$c_wmean}.
% Both features main effects can be approximated with a linear segment.
% The other two feature have a very low variance and get a low weight.

<<spark-table, dependson="epsilons", results="asis", eval=FALSE>>=
dat = data.frame(test = c(1,2), sp = c(spark(fc$approx_models$V1), spark(fc$approx_models$V2)), stringsAsFactors = FALSE)
print.xtable(xtable(dat), sanitize.text.function = as.is)
@
% =============================================================================
% Different epsilons
% =============================================================================
%The choice of $\epsilon$ affects the complexity measures:
%This ALE main effect \Sexpr{spark(fc1$approx_models$V3$ale)} can be approximated with a linear model \Sexpr{spark(fc1$approx_models$V3, approx = TRUE)} when $\epsilon = \Sexpr{eps1}$ but needs \Sexpr{(fc3$approx_models$V3$n_coefs + 1) / 2} segments when $\epsilon$ is decreased to \Sexpr{eps3}: \Sexpr{spark(fc3$approx_models$V3, approx = TRUE)}.



% =============================================================================
% Did not work out
% =============================================================================
% We tried a few alternatives to approximate the ALE plots for measuring their complexity, which turned out to be impractical.
% We tried approximation with cubic (overlapping) splines and measuring the complexity as degrees of freedom.
% The degrees of freedom exploded when the ALE plot was not quite linear, but almost, yielding an unintuitive measure.
% Tree splits did not work, like for example model based partitioning TODO: CITE mob.
% One reason is that the greedy approach did not work well and we found it better to search per number of splits.

%The complexity measures ignores how complex the internal structure of the model is, e.g. it does not matter whether the linear relationship comes from a single linear model or a blend of hundreds of linear models.
%We demonstrate how the number of nodes and complexity measure C increase with increasing model performacne, but at some point the complexity C decreases again.
%For this example, \Sexpr{n} data points were drawn from a uniform distribution between 0 and 1, and the target is simulated as $2 * x + \epsilon$, where $\epsilon \sim N(0,\Sexpr{nsd})$.
%We trained three trees with different maximal depths, as shown in Table~\ref{tab:tree}.

<<tree-increase, dependson="tree-increase-data",results="asis", eval=FALSE>>=
rp_width = function(rp) sum(rp$frame$var == "<leaf>")

rp1 = rpart(y ~ ., data = dat, control = rpart.control(maxdepth = 1))
pred1 = Predictor$new(rp1, data = dat)
fc1 = FunComplexity$new(pred1, grid.size = grid.size, epsilon = epsilon)
#p1 = fc1$approx_models$x$plot() +
#  ggtitle(sprintf("Leaf nodes: %i, C: %i", rp_width(rp1), fc1$c_wmean))

rp2 = rpart(y ~ ., data = dat, control = rpart.control(maxdepth = 2))
pred2 = Predictor$new(rp2, data = dat)
fc2 = FunComplexity$new(pred2, grid.size = grid.size, epsilon = epsilon)
#p2 = fc2$approx_models$x$plot() +
#  ggtitle(sprintf("Leaf nodes: %i, C: %i", rp_width(rp2), fc2$c_wmean))

rp3 = rpart(y ~ ., data = dat, control = rpart.control(maxdepth = 3))
pred3 = Predictor$new(rp3, data = dat)
fc3 = FunComplexity$new(pred3, grid.size = grid.size, epsilon = epsilon)
#p3 = fc3$approx_models$x$plot() +
#  ggtitle(sprintf("Leaf nodes: %i, C: %i", rp_width(rp3), fc3$c_wmean))

#grid.arrange(p1, p2, p3, nrow = 1)
n_terminal = function(tree) {
  nrow(tree$frame[tree$frame$var == "<leaf>",])
}

tab = data.frame(
  "Tree Depth" = c(1,2,3),
  "Terminal nodes" = c(n_terminal(rp1), n_terminal(rp2),n_terminal(rp3)),
  "ALE" = c(spark(fc1$approx_models$x),spark(fc2$approx_models$x),spark(fc3$approx_models$x)),
  "Approximation" = c(
    spark(fc1$approx_models$x, approx = TRUE),
    spark(fc2$approx_models$x, approx = TRUE),
    spark(fc3$approx_models$x, approx = TRUE)),
  "AMEC" = c(fc1$c_wmean, fc2$c_wmean, fc3$c_wmean))

caption = "Increasing the tree depth always increases number of terminal nodes, but the complexity measure increases at some point because the prediction function can be approximated by a single linear function."
print.xtable(xtable(tab, caption = caption), sanitize.text.function = as.is, booktabs = TRUE, label = "tab:tree")
@



