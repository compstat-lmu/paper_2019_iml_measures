\Sexpr{set_parent('paper.Rnw')}
\subsection{Number of Features (NF)}
\label{sec:nfeatures}

% =============================================================================
% About sparsity
% =============================================================================
We propose an approach based on feature permutation to determine how many features are used by a model.
We regard features as "used" when changing a feature changes the prediction.
If available, the model-specific number of features is preferable.
The model-agnostic version is useful when the prediction function is only accessible via API or when the feature transformation pipeline is very complex.

% =============================================================================
% Why model-agnostic
% =============================================================================\
%f available, a model-specific method for extracting the number of features used by the model is preferable, e.g. counting the number of non-zero weights in a sparse linear regression model.
% model-agnostic heuristic is useful when the prediction function is accessible but not the internal structure of the model (e.g. prediction via API call), or when combining preprocessing steps and models complicates programmatic extraction (e.g. training a decision tree on sparse principal components).

% =============================================================================
% Feature-count heuristic: intuition
% =============================================================================
The proposed procedure is formally described in Algorithm~\ref{algo:nfeat}.
To estimate whether the $j$-th feature was used, we sample instances from data $\D$, replace their $j$-th feature values with random values from the distribution of $X_j$ (e.g., by sampling $\xj$ from other instances from $\D$), and observe whether the predictions change.
If the prediction of any sample changes, the feature was used for the prediction.
% =============================================================================
% Feature-count heuristic: algorithm
% =============================================================================
\begin{algorithm}
\caption{Number of Features Used (NF)}\label{algo:nfeat}
\KwInput{Number of samples $M$, data $\D$}
NF = 0\;
	\For{$j \in 1,\ldots,p$}{
		Draw $M$ instances $\{x^{(m)}\}_{m=1}^M$ from dataset $\D$\;
			Create $\{x^{(m)*}\}_{m=1}^M$ as a copy of $\{x^{(m)}\}_{m=1}^M$ \;
			\For{$m\in 1,\ldots,M$}{
				Sample $\xj^{(new)}$ from $\{\xij\}_{i=1}^n$ with the constraint that $\xj^{(new)} \neq \xj^{(m)}$\;
				Set $\xj^{(m)*} = \xj^{(new)}$\;
			}
			\lIf{$\fh(x^{(m)*}) \neq \fh(x^{(m)}) \text{ for any }  m \in \{1,\ldots,M\}$}{$NF = NF + 1$.
		}
		}
\Return NF
\end{algorithm}
%
% =============================================================================
% False negatives
% =============================================================================
%The rate of false positives is zero, i.e. the probability that the heuristic counts a feature as used, but the model did not use the feature is zero.
%The probability of a false negative, i.e. the heuristic overlooks a feature, depends on the number of samples $M$, the model function $f$ and the data distribution.
%Let $P_{dep}^j$ be the probability that the prediction of a random instance depends on the value of $x_j$.
%For an instance that depends on $x_j$ for its prediction, let $P_{change}^j$ be the probability that a sample from $X_j$ changes the prediction for an instance i.
%Then the probability of overlooking feature j is: $P_{fn}^j=(1 - P_{dep}^j + P_{dep}^j (1 - P_{change}^j))^M$
%With the simplifying assumption that $P_{fn}^j = P_{fn} \forall j \in 1,\ldots,p$, the probability that we miss at least one feature is $1 - (1 - P_{fn})^p$.
%For a linear model without interactions and only numerical features, the false negative rate is 0:
%$P_{dep}^j=1$ and $P_{change}^j = 1$, so that $P_{fn}^j = (1 - 1 + 0)^M = 0$.
%Let us assume a non-linear model where only one percent of instances rely on feature $\xj$ ($P_{dep}^j=0.01$) and these instances have a probability of 0.02 that the feature permutation changes the prediction ($P_{change}^j=0.02$).
%If we set $M=100$, then $P_{fn}^j = (0.99 + 0.01\cdot 0.01)^{100} \approx 0.37$.
%If we increase M to 500, the probability that NF counts too few features drops to $\approx 0.007$.

<<NF-false-negatives>>=
set.seed(42)
n_repetitions = 100

nf_tree = function(mlrmod){
  # Don't count leaf
  length(setdiff(unique(mlrmod$learner.model$frame$var), "<leaf>"))
}

nf_glmnet = function(mlrmod){
  sum(abs(mlrmod$learner.model$beta[,1]) > 0)
}

data(Boston, package = "MASS")
tsk =  makeRegrTask(data = Boston, target = "medv")
lrn1c = makeLearner("regr.rpart", maxdepth = 1)
lrn2c = makeLearner("regr.rpart", maxdepth = 2)
lrn3c = makeLearner("regr.rpart", maxdepth = 10)
lrn1 = makeLearner("regr.glmnet", lambda = 10)
lrn2 = makeLearner("regr.glmnet", lambda = 5)
lrn3 = makeLearner("regr.glmnet", lambda = 2)
lrn4 = makeLearner("regr.glmnet", lambda = 1)
lrn5 = makeLearner("regr.glmnet", lambda = 0.1)
lrn6 = makeLearner("regr.glmnet", lambda = 0.001)
lrns = list(lrn1c, lrn2c, lrn3c, lrn1, lrn2, lrn3, lrn4, lrn5, lrn6)
mods = lapply(lrns, function(m) train(m, tsk))


NF = function(pred, m){
 fnames = pred$data$feature.names
 sum(unlist(lapply(fnames, function(fname) feature_used(pred, fname, sample_size = m))))
}

NF_experiment = function(lrn){
  mod = train(lrn, tsk)
  if(inherits(mod$learner.model, "glmnet")) {
    model = "elastic net"
    nf_true = nf_glmnet(mod)
  } else {
    model = "CART"
    nf_true = nf_tree(mod)
  }
  pred = Predictor$new(mod, data = Boston)
  res = lapply(1:n_repetitions, function(i) {
    fc10 = NF(pred, m = 10)
    fc50 = NF(pred, m = 50)
    fc500 = NF(pred, m = 500)
    data.frame(m = c(10, 50, 500), NF = c(fc10, fc50, fc500), NF.true = nf_true)
  })
  res = rbindlist(res)
  res$model = model
  res
}
dfs = rbindlist(lapply(lrns, NF_experiment))
aggr = dfs[,.(med = round(mean(abs(NF - NF.true)),2)),by = c("m", "model")]
maes_cart = sprintf("%.3f", aggr[model == "CART", med])
@

We tested the NF heuristic with the Boston Housing data.
We trained decision trees (CART) with maximum depths $\in\{1,2,10\}$ leading to \Sexpr{nf_tree(mods[[1]])}, \Sexpr{nf_tree(mods[[2]])} and  \Sexpr{nf_tree(mods[[3]])} features used and an L1-regularized linear model with penalty $\lambda\in\{10, 5, 2, 1, 0.1, 0.001\}$ leading to \Sexpr{nf_glmnet(mods[[4]])}, \Sexpr{nf_glmnet(mods[[5]])}, \Sexpr{nf_glmnet(mods[[6]])}, \Sexpr{nf_glmnet(mods[[7]])}, \Sexpr{nf_glmnet(mods[[8]])} and \Sexpr{nf_glmnet(mods[[9]])} features used.
For each model, we estimated NF with sample sizes $M\in\{10, 50, 500\}$ and repeated each estimation \Sexpr{n_repetitions} times.
For the elastic net models, NF was always equal to the number of features with non-zero weight estimates.\Sexpr{stopifnot(all(aggr[model == "elastic net", med] == 0))}
For the CART models, the mean absolute differences between NF and the number of features used in the trees were \Sexpr{maes_cart[1]} ($M=10$), \Sexpr{maes_cart[2]} ($M=50$) and \Sexpr{maes_cart[3]} ($M=500$).
