\Sexpr{set_parent('paper.Rnw')}
\subsection{Number of Features (NF)}
\label{sec:nfeatures}

% =============================================================================
% About sparsity
% =============================================================================
We propose an approach based on feature permutation to determine how many features are used by the model for predictions.
We regard features as "used" by the model when changing a feature changes the prediction, which may differ from the numbers of features available during training.

% =============================================================================
% Why model-agnostic
% =============================================================================\
If available, model-specific methods for extracting the number of features used by the model are preferable to a model-agnostic estimation heuristic, such as counting the number of non-zero weights in a sparse linear regression model.
A model-agnostic heuristic is useful when the prediction function is accessible but not the internal structure of the model (e.g. prediction via API call), or when combining preprocessing steps and models complicates programmatic extraction (e.g. training a decision tree on sparse principal components). 

% =============================================================================
% Feature-count heuristic: intuition
% =============================================================================
The proposed procedure is formally described in Algorithm~\ref{algo:nfeat}.
To estimate whether feature $x_j$ was used, we sample instances from data $\D$, permute values for $x_j$, and take the difference of the  predictions before and after permutation.
If the difference for any sample is different from zero, the feature was used for the prediction. 
% =============================================================================
% Feature-count heuristic: algorithm
% =============================================================================
\begin{algorithm}
\caption{Number of features used (NF)}\label{algo:nfeat}
\KwInput{Number of samples $M$, data $\D$}
NF = 0\;
	\For{$j \in 1,\ldots,p$}{
		Draw $M$ instances $\{x^{(m)}\}_{m=1}^M$ from dataset $\D$\;
			Create $\{x^{(m)*}\}_{m=1}^M$ as $\{x^{(m)}\}_{m=1}^M$ with permuted values for the j-th feature\;
			\lIf{$\exists m \in \{1,\ldots,M\}:\fh(x^{(m)*}) \neq \fh(x^{(m)})$}{$NF += 1$. 
		}
		}
\Return NF	
\end{algorithm}

% =============================================================================
% False negatives
% =============================================================================
The rate of false positives is zero, i.e. the probability that the heuristic counts a feature as used, but the model did not use the feature is zero.
The probability of a false negative, i.e. the heuristic overlooks a feature, depends on the number of samples $M$, the model function $f$ and the data distribution.
Let $P_{dep}^j$ be the probability that the prediction of a random instance depends on the value of $x_j$.
For an instance that depends on $x_j$ for its prediction, let $P_{change}^j$ be the probability that a sample from $X_j$ changes the prediction for an instance i.
Then the probability of overlooking feature $x_j$ is: $P_{fn}^j=(1 - P_{dep}^j + P_{dep}^j (1 - P_{change}^j))^M$
With the simplifying assumption that $P_{fn}^j = P_{fn} \forall j \in 1,\ldots,p$, the probability that we miss at least one feature is $1 - (1 - P_{fn})^p$.

For a linear model without interactions and only numerical features, the false negative rate is 0:
$P_{dep}=1$ and $P_{change}^j = 0$, so that $P_{fn}^j = (1 - 1 + 0)^M = 0$.
For a non-linear model where only one percent of instances rely on feature $\xj$ ($P_{dep}=0.01$) and these instance have a probability of 0.02 that the feature permutation changes the prediction ($P_{change}=0.02$).
If we set $M=100$, then $\mathbb{P}(\hat{z}_j = 0| z_j = 1) = (0.99 + 0.01\cdot 0.01)^{100} \approx 0.37$.
If we increase M to 500, the probability drops to $\approx 0.007$.

