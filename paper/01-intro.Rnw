\Sexpr{set_parent('paper.Rnw')}
\section{Introduction}
\label{sec:introduction}

% Motivation
Machine learning (ML) algorithms such as neural networks and support vector machines (SVM) are often considered to produce black box models because they do not provide any direct explanation for their predictions.
However, these methods often outperform simple linear models or decision trees in predictive performance as they are able to model complex relationships in the data.
%\citep{fernandez2014we}.
Nevertheless, trees and linear models are still preferred in areas such as life sciences and social sciences due to their simplicity and interpretability \citep{Lipton2016}. %Caruana2015a
Some researchers have therefore developed model-agnostic interpretability tools for black box models, which often measure or visualize feature effects or feature importance \citep{goldstein2015peeking, kononenko2010efficient, Fisher2018}.


Machine Learning
Problem: Interpretability
Solution: Interpretable Models and Post-hoc methods
Both not perfect


Approaches:
Finding most influential data points \citep{Datta2016}
Feature importance \citep{Fisher2018} TODO: Cite giuseppe
SHAP \citep{lundberg2017unified}
Interpretable models: \citep{Caruana2015a} TODO: Cite some more here
Shapley Values \citep{vstrumbelj2014explaining} \citep{vstrumbelj2011general}

ICE curves \citep{goldstein2015peeking}
PDP \citep{friedman2001greedy}
LIME \citep{Ribeiro2016b}

Prototypes and Criticisms  \citep{kim2016examples}


We have the following paper structure:
\begin{itemize}
\item In Motivation we motivate why we need model-agnostic interpretability measures
\item In Decomposition we show ways to decompose the model into lower order feature effects
\item The decomposition can be estimated with Accumulated Local effects, which we demonstrate in this chapter
\item In Measure chapter we show a specific way to view the decomposition that allows us to measure its complexity in a meaningful way
\item We go deeper by showing how to measure the degree of interactions in a model
\item And how to measure the complexity of the main effects
\item First a simple application to demonstrate the measures with wine dataset comparing two models
\item An application that shows how these measures can be used in multi-objective optimization to solve the accuracy / interpretability tradeoff.
\end{itemize}



Some authors suggest that we need to have some interpretability across different models. \citep{bibal2016interpretability}
We address this by using model-agnostic methods that can be applied to the model.
They also call for multi-objective optimization "In contrast, heuristics
can be integrated in learning through multi-objective optimisation techniques."

Motivation: We want to be able to compare interpretability across models.
Problem: No measure available.
Novelty: New model-agnostic measures for interpretability. End-to-end, multicrit optimization for accuracy and interpretability (interpretability not measured with intermediary number from model.)
We define interpretability as "How well do post-hoc methods work" and how sparse are they.
For us the model is interpretable, when I have to look at as little as post-hoc analysis as possible and when those post-hoc analyses are as truthful to the model as possible.

Our contribution / novelty: Suggesting (novel) interpretability measures and testing them


Argumentation: We can decompose the function.
Individual components ccan be approximated by ALE.
We measure the complexity and all the interactions we did not approximate.
We show that when interaction is small, we have an additive model.
And when other measure is minimized, we get linear effects or sparsity.
All the post-hoc methods tend to work better when the two criteria are optimized.


We also are based on theory (functional ANOVA decomposition)


Tradeoff between accuracy and interpretability has to be decided by the user. / Arguments for Multiobjective instead of weighted single objective.f \citep{freitas2004critical}


Limitations of post-hoc methods

**Effect of high number of features**
Example with few and many features and showing Shapley explanations and many partial dependence plots.

**Effect of interactions**
Show ICE / PDP example.
Show problem with local and global linear surrogate models.

**Effect of complex non-linear feature effects**
Show crazy pdp.
Show problems with LIME.


Sources of lack of interpretability: many features, complex relationship with prediction and interactions -> interpretability measure should capture those


One of the measures describes the complexity of first-order model with the degrees of freedom.
The other measure describes how big the higher-order portion of the model is, and with that kind of equating anything higher order as non-interpretable.


We need model-agnostic measures: \citep{ruping2006learning}
One drawback of formal complexity measures is that they are either only
applicable for a specific class of models, e.g. the depth of a decision tree, the
norm of a parameter vector or the number of prototypes, or provide only a very
coarse measure of complexity, like the number of attributes used. Hence, they
provide little help in choosing the right hypothesis space in the first place. People
sometimes argue that one hypothesis language is “obviously” more intuitive than
others, but this is of course highly subjective and hence useless.


Critique: Interpretability not well defined \citep{Lipton2016}
