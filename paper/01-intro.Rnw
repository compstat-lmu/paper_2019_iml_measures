\Sexpr{set_parent('paper.Rnw')}
\section{Introduction}
\label{sec:introduction}

%==============================================================================
% Interpretability requirement and tradeoff
%==============================================================================
Predictive models are optimized for predictive performance.
Additionally it is often required that the models can be interpreted to make predictions actionable, gain trust, meet regulatory requirements, generate insights, etc..
Interpretability and predictive performance usually come in a trade-off.
Black box machine learning models like gradient boosting and neural networks often achieve a high predictive performance compared to structurally simpler models like linear models and decision trees, but are usually opaque.
%==============================================================================
% Two flawed ways of increasing interpretability
%==============================================================================
To achieve model interpretability, practitioners usually choose one of two approaches:
Either they select a model class apriori that is deemed interpretable (e.g. linear regression models) as is common in fields like life sciences and social sciences \citep{Liption2016}, or they consider all kinds of possible models, select the one with the best predictive performance and apply post-hoc interpretation methods.
Interpretation methods quantify the effects features have on the prediction, compute feature importances or explain individual predictions, see \citep{molnar2019} for an overview.
%\citep{fernandez2014we}.
% =============================================================================
% Problems with post-hoc
% =============================================================================
Model-agnostic post-hoc interpretation methods can be used with any model, regardless of how complex the model is.
Yet the usefulness and reliability of these methods is still tied to properties of the model.
A lack of sparseness, complex relationships between features and prediction and interactions cause a detoriation of post-hoc interpretations:
Interpretations become more unmanageable, verbose and unreliable, when more features were used, when feature effects diverge from simple relationships (e.g. from linear to non-linear), when the prediction heavily relies on feature interactions.

% =============================================================================
% Model selection as use case
% =============================================================================
Either way, the interpretability / performance tradeoff between different models is unquantified.
For example how do we compare the interpretability of a decision tree and a decision list or of a linear model and a support vector machine?
% =============================================================================
% Need interpretability measures
% =============================================================================
A solution to explicitly consider interpretability in the model selection process is model-agnostic measures of the models prediction function complexity \citep{ruping2006learning,bibal2016interpretability}, which allow comparisons in terms of interpretability even between different model classes.
Interpretability measures allows the user to explicitly decide on the tradeoff between accuracy and interpretability instead of "fixing" interpretability (e.g. by choosing a certain model class) and optimizing only performance \citep{freitas2004critical}.

% =============================================================================
% Our solution and novelty
% =============================================================================
Our contributions are the following:
After a review of related work \ref{sec:other} and of the theory of functional decomposition \ref{sec:decomposition}, we suggest three novel model-agnostic measures of interpretability.
The first measure is the number of features used by the model, for which we propose a novel estimation heuristic \ref{sec:nfeatures}.
Based on a decomposition and condensation of the model prediction function, we suggest measures for interaction strength \ref{sec:interaction} and for average complexity of the feature main effects \ref{sec:curve}.
We demonstrate that minimizing these three measures improves the readability, reliability and fidelity of post-hoc interpretation methods \ref{sec:post-hoc}.
Finally we illustrate the usefulness of the complexity measures in multi-objective optimization setting to allow a tradeoff between interpretability and accuracy \ref{sec:multiobj}.

