\Sexpr{set_parent('paper.Rnw')}
\section{Related Work and Background}
\label{sec:related}

In this section, we introduce notation, review related work and describe the functional decomposition on which we based the proposed complexity measures.

\subsubsection{Notation:} We consider machine learning prediction functions $f:x \mapsto y$, where $x \in \mathbb{R}^p$ is a  p-dimensional feature vector and $y \in \mathbb{R}$ the prediction.
For the decomposition of this function, we use $f_S:x_S \mapsto y$, $S \subseteq\{1, \ldots, p\} \quad x_S \in \mathbb{R}^{|S|}$ to denote a function that maps a vector with a subset of the features to a marginal prediction.
For a single feature, this is written as $f_j$.
We denote the training data for the machine learning model with the tuples $\mathbb{D} = \{x^{(i)},y^{(i)}\}_{i=1}^n$ and refer to the value of the j-th feature from the i-th instance as $x_j^{(i)}$.
We write $X_j$, when we refer to the j-th feature as a random variable.

\subsection{Interpretability Measures}
\label{sec:other}

This section gives a non-exhaustive overview of approaches to measure and optimize for interpretability.
% =============================================================================
% Model-specific or class-specific measures of interpretability
% =============================================================================
Many measures of model complexity or interpretability are model-specific, i.e. only models from the same class can be compared (e.g. only decisiontrees).
Model size is often used as a measure for quantifying interpretability (e.g. number of decision rules, tree depth, ...) \citep{huysmans2011empirical,ruping2006learning,askira1998knowledge,yang2017scalable}. 
Akaikes Information Criterion (AIC) \citep{akaike1998information} and the  Bayesian Information Criterion (BIC) \citep{schwarz1978estimating} are more broadly applicable measures for the trade-off between goodness of fit and degrees of freedom.
In AIC and BIC, the tradeoff between interpretability and performance is fixed and only one dimension of interpretability, the degrees of freedom, is considered.
%\citep{hauenstein2018computing} use the generalized degrees of freedom to extend the AIC to a model-agnostic measure, but it is computationally expensive as it requires refitting the model multiple times and does not give more granular insight into the complexity.
%Also refitting the model changes the model and its complexity.
In \citep{zhou2018measuring} the authors propose a general form for model interpetability which includes the structural complexity of the model, but again compare model-specific measures like number of leaves, average depth and maximum depth of a tree. 
Often used is also the number of features, but this is usually done by inspecting model parameters (e.g. number of non-zero weights in sparse linear regression).

% =============================================================================
% Model-agnostic measures
% =============================================================================
In \citep{philipp2018measuring} the authors propose model-agnostic measures of model stability, based on the semantic similarity of the predictions when the model is repeatedly trained on different samples of the training data.
Similar to our approach, they based the measures on the predictions, not the structure of the model, but they measure a different property.

An approach similar to our proposal is described in \citep{plumb2019regularizing}.
They propose explanation fidelity and explanation stability metrics for quantifying the quality of local explanation models, e.g. as produced with LIME \citep{ribeiro2016should}.
They proposed to incorporate the metrics as regularizers in the loss function to train a neural network.
Their local explainability focused metrics are complementary to ours, sicnce we consider more global model properties.

% =============================================================================
% Other approaches
% =============================================================================
Other measures of interpretability are based on surveys and human studies.
For example in \citep{dhurandhar2017tip} the authors propose to measure interpretability as the improvement in the task the machine learning model helps to solve.
In \citep{friedler2019assessing} an interpretability measure based on a runtime operation count is proposed and evaluated in user studies.

\subsection{Functional Decomposition}
\label{sec:decomposition}

% =============================================================================
% General decomposition
% =============================================================================
Any high-dimensional prediction function can be decomposed into a sum of components with increasing dimensionalityi: an intercept, first-order feature effects, second-order effects up to the p-th order effect:

\begin{eqnarray}\label{eqn:decomp} f(x)  = &\overbrace{f_0}^\text{Intercept} + \overbrace{\sum_{j=1}^p f_j(x_j)}^\text{1st order effects} + \overbrace{\sum_{j\neq k}^p f_{jk}(x_j, x_k)}^\text{2nd order effects} + \ldots + \overbrace{f_{1,\ldots,p}(x_1, \ldots, x_p)}^\text{p-th order effect}
%\\ =  & \sum_{S \subseteq \{1,\ldots,p\}} f_S(x_S)
\end{eqnarray}
% =============================================================================
% Some useful decompositions
% =============================================================================
Without constraints on the components, this decomposition is not unique.
For example, you could choose arbitrary functions like setting everyhing to 0, and define the last component as the difference between the arbitrary decomposition and the true model function.
Stone \citep{stone1994use} suggested to define orthogonality constraints and approximate the function with weighted integrals.
Hooker \citep{hooker2007generalized} defined centering, orthogonality and variance decomposition as desirable properties, which results in hierarchically orthogonal components under the correlation inner product.

% =============================================================================
% ALE decompositions
% =============================================================================
Accumulated Local Effects (ALE) were proposed in \cite{apley2016visualizing} as a tool for visualizing feature effects (see e.g. Figure~\ref{fig:c-demo}) and also yield a unique decomposition of the prediction function $f_S = f_{S,ALE}$.
The ALE decomposition is unique under an orthogonality-like property further described in \citep{apley2016visualizing}.

%\begin{eqnarray}
%f(x)  = & f_0 + \sum_{j=1}^p \falej(x_j)+ \sum_{j\neq k}^p f_{ALE,jk}(x_j, x_k) + \ldots + f_{ALE,1,\ldots,p}(x_1, \ldots, x_p)
%\end{eqnarray}
% =============================================================================
% ALE definition
% =============================================================================

The ALE main effect $f_{j,ALE}$ of a feature $x_j, j \in \{1,\ldots,p\}$ for a prediction function $\fh$ is defined as 
\begin{eqnarray}\label{eqn:ale}
\falej(x_j) = \int_{z_{0,j}}^{x_j} \mathbb{E}\left[\frac{\delta \fh(X_1,\ldots,X_p)}{\delta X_j}|X_j = z_j\right]dz_j-c_j
\end{eqnarray}
Here, $z_{0,j}$ is a lower bound of $X_j$ (usually minimal observed value of $x_j$) and the expectation $\mathbb{E}$ is computed marginal on the distribution of all features conditioned on the value for $x_j$.
The constant $c_j$ is chosen so that the mean of $f_{j,ALE}(x_j)$ with respect to the marginal distribution of $X_j$ is zero.
The ALE main effects are defined as the gradients with respect to the features, but are estimated with finite differences, meaning they can be computed for non-differentiable prediction functions.
For the actual estimation we refer to \citep{apley2016visualizing}.
% =============================================================================
% ALE is best
% =============================================================================
We use the ALE decomposition, because ALE are relatively cheap to compute, the effects can be computed sequentially instead of simultanously as in \citep{hooker2007generalized} and, most importantly ALEs don't require knowledge of the data distribution.
ALE plots have software implementations, see \citep{iml,alepackage}.



% =============================================================================
% =============================================================================
% =============================================================================
% Material
% =============================================================================
% =============================================================================
% =============================================================================
% Categorical features
%Categorical features require an ordering of the categories so that accumulated local effects can be estimated.
%Any ordering of the categories will yield a valid ALE, but the interpretation differs, because category effects are interpreted in terms of changes to the neighbouring categories.
%Compared to \citep{hooker2007generalized}, ALE yields a fundamentally different decomposition of the model function $f(x)$.
%\citep{apley2016visualizing} show that this decomposition is unique because of what they call "pseudo-orthogonality".
%Pseudo-orthogonality means that for any component of the ALE decomposition, a repeated application of ALE computation yields zero or the same ALE component (if the same ALE effect is computed).
%For example, lets look at the 1st-order (=main) effect of $x_1$ $f_{ALE,1}$ and the second order effect between $x_1$ and $x_2$ $f_{ALE,1,2}$.
%If we tried to extract the main order effect for $x_1$ from $f_{ALE, 1,2}$ the result would be a constant function that is always 0.
% =============================================================================
% ALE property: Pseudo-orthogonality
% =============================================================================
%Let $H_S(f_S)$ be a function that maps a function to its ALE function with features S.
%ALE defines the components in such a way that they are "pseudo-orthogonal", which is not true orthogonality, but a similar concept.
%"Pseudo-orthogonality": $H_j(f_j) = f_j$ and $H_u(f_j) = 0$ for each $j \subseteq D$ and $u \subseteq D$ with $u \neq j$.
%In words, pseudo-orthogonality is:
%The S-ALE of an S-ALE function is again S-ALE and the S-ALE function of u-ALE is 0 when S and u are not equal.

%We prefer the orthogonality-like  over orthogonality, since when feature are correlated, the true additive structure is not reflected when effects have to be orthogonal.
%%For example if the model is a linear model with $f(x) = x_1 + 2\cdot x_2$, with $x_1$ and $x_2$ being correlated, we prefer to recover $f_{1}(x_1) = 1$ and $f_{2}(x_2)$ which is the result under ALE decomposition, but not under the constraint of orthogonality.

% =============================================================================
% Concepts for interpretability
% =============================================================================
%There are a few concepts of interpretability.
%Other suggested measures of interpretability is to constrain model form:
%Monotonicity constraints: CITE.
%Causality.CITE.
%Additivity. CITE.
%Sparsity. CITE.
%Linearity. CITE.





% =============================================================================
% Some taxonomy
% =============================================================================
% \citep{bibal2016interpretability} distinguish between interprebaility on model level and on representation level.
% This adds to the taxonomy of \citep{doshi2017towards}
% Quantiative measurements of model: Either some heuristic of model (e.g. number of features in Lasso) or user-based surveys \citep{freitas2014comprehensible}.
% Quantiative heuristic measure model.
% User-based surveys measure representation

% =============================================================================
% Categorization of our method
% =============================================================================
% According to \citep{bibal2016interpretability} we can distinguish measures of interpretability for following areas.
% \begin{itemize}
% \item model (specific, general)
% \item representation (specific, general)
% \end{itemize}
% Our method depends on representation and on the model of course.
% So something in between.



% Pseudo-orthogonality > Orthogonality
% True orthogonality is not desirable, as illustrated with this next example.
% For example if $f(x) = x_1 + x_2$ and $X_1$ and $X_2$ are correlated, then fANOVA decomposition will not give the correct main effects $f_1(x_1)$ and $f_2(x_2)$, but ALE components will decompose it in such a way.
% For fANOVA approach this would be.
% We assume that expected value of $x_1$ and $x_2$ is 0:
% \begin{eqnarray*}
% f_{1,fANOVA}(x_1) =& \mathbb{E}(f(x)|X_1=x_1) - E_X(f(x)) \\
%                   =& \mathbb{E}(X_1| X_1=x_1) + \mathbb{E}(X_2|X_1 = x_1) - (\mathbb{E}(X_1) + \mathbb{E}(X_2)) \\
%                   =& x_1 +  \mathbb{E}(X_2|X_1 = x_1)  \\
%                   \neq & x_1
% \end{eqnarray*}
% Intuition: Feature x1 and x2 are correlated. But effects are forced to be non-correlated.
