\Sexpr{set_parent('paper.Rnw')}
\section{Related Work and Background}
\label{sec:related}

In this section we introduce the notation, review related work and describe the functional decomposition on which we base the proposed complexity measures.

\subsubsection{Notation:} We consider machine learning prediction functions $f:x \mapsto y$, where $x \in \mathbb{R}^p$ is a p-dimensional feature vector and $y \in \mathbb{R}$ is the prediction.
For the decomposition of this function, we write $f_S:x_S \mapsto y$, $S \subseteq\{1, \ldots, p\} \quad x_S \in \mathbb{R}^{|S|}$ to denote a function that maps a vector with a subset of features to a marginal prediction.
For a single feature, we write $f_j$.
We refer to the training data for the machine learning model with the tuples $\D = \{(x^{(i)},y^{(i)})\}_{i=1}^n$ and refer to the value of the j-th feature from the i-th instance as $x_j^{(i)}$.
We write $X_j$ to refer to the j-th feature as a random variable.

\subsection{Interpretability Measures}
\label{sec:other}

This section provides a non-exhaustive overview of approaches to measuring and optimizing interpretability.
% =============================================================================
% Model-specific or class-specific measures of interpretability
% =============================================================================
Many measures of interpretability are model-specific, i.e. only models of the same class can be compared (e.g. decision trees).
Model size is often used as a measure for interpretability (e.g. number of decision rules, tree depth, number of decision rules, ...) \citep{huysmans2011empirical,ruping2006learning,askira1998knowledge,yang2017scalable}. 
Akaikes Information Criterion (AIC) \citep{akaike1998information} and Bayesian Information Criterion (BIC) \citep{schwarz1978estimating} are more widely applicable measures for the trade-off between goodness of fit and degrees of freedom.
AIC and BIC fixate a certain compromise between interpretability and performance, and consider only one dimension of interpretability, the degrees of freedom.
In \citep{zhou2018measuring} the authors propose an interpretability evaluation model that considers the (model-specific) structural complexity of machine learning models.
In linear models, the number of features is often used as a measure of interpretability \citep{schielzeth2010simple}.
% =============================================================================
% Model-agnostic measures
% =============================================================================
In \citep{philipp2018measuring} the authors propose model-agnostic measures of model stability, based on the semantic similarity of predictions when the model is re-trained on different samples of the training data.
Similar to our approach, the measures are based on the predictions, not on the structure of the model.

In \citep{plumb2019regularizing} the authors propose explanation fidelity and explanation stability metrics of local explanation models such as LIME \citep{ribeiro2016should}.
They propose to incorporate the metrics as regularizer into the loss function of a neural network to simultaneously optimize for predictive performance and higher quality of local explanations.
Their local explainability metrics complement ours, since we consider global model properties.

% =============================================================================
% Other approaches
% =============================================================================
Further approaches measure interpretability as the usability of a (interpretable) model to support a human in a task, usually measured in a survey as response time, correctness of the response and task difficulty \citep{zhou2018measuring,huysmans2011empirical,dhurandhar2017tip}.
In \citep{friedler2019assessing} an interpretability measure based on runtime operation count is proposed and evaluated in user studies.

\subsection{Functional Decomposition}
\label{sec:decomposition}

% =============================================================================
% General decomposition
% =============================================================================
Any high-dimensional prediction function can be decomposed into a sum of components with increasing dimensionality: an intercept, first-order feature effects, second-order effects and so on up to the p-th order effect:

\begin{eqnarray}\label{eqn:decomp} f(x)  = &\overbrace{f_0}^\text{Intercept} + \overbrace{\sum_{j=1}^p f_j(x_j)}^\text{1st order effects} + \overbrace{\sum_{j\neq k}^p f_{jk}(x_j, x_k)}^\text{2nd order effects} + \ldots + \overbrace{f_{1,\ldots,p}(x_1, \ldots, x_p)}^\text{p-th order effect}
%\\ =  & \sum_{S \subseteq \{1,\ldots,p\}} f_S(x_S)
\end{eqnarray}
% =============================================================================
% Some useful decompositions
% =============================================================================
This decomposition is only unique with constraints for the components.
%For example, you can set all components to 0, and define the last component as the difference between the arbitrary decomposition and the true model function.
Stone \citep{stone1994use} suggested orthogonality constraints and approximating the prediction function with weighted integrals.
Hooker \citep{hooker2007generalized} defined centering, orthogonality and variance decomposition as desirable properties, resulting in unique and hierarchically orthogonal components under the correlation inner product.

% =============================================================================
% ALE decompositions
% =============================================================================
Accumulated Local Effects (ALE) were proposed in \cite{apley2016visualizing} as a tool for visualizing feature effects (e.g. Figure~\ref{fig:c-demo}) and as an alternative unique decomposition of the prediction function with components $f_S = f_{S,ALE}$.
The ALE decomposition is unique under an orthogonality-like property further described in \citep{apley2016visualizing}.

%\begin{eqnarray}
%f(x)  = & f_0 + \sum_{j=1}^p \falej(x_j)+ \sum_{j\neq k}^p f_{ALE,jk}(x_j, x_k) + \ldots + f_{ALE,1,\ldots,p}(x_1, \ldots, x_p)
%\end{eqnarray}
% =============================================================================
% ALE definition
% =============================================================================

The ALE main effect $f_{j,ALE}$ of a feature $x_j, j \in \{1,\ldots,p\}$ for a prediction function $\fh$ is defined as 
\begin{eqnarray}\label{eqn:ale}
\falej(x_j) = \int_{z_{0,j}}^{x_j} \mathbb{E}\left[\frac{\delta \fh(X_1,\ldots,X_p)}{\delta X_j}|X_j = z_j\right]dz_j-c_j
\end{eqnarray}
Here, $z_{0,j}$ is a lower bound of $X_j$ (usually minimum observed value of $x_j$) and the expectation $\mathbb{E}$ is computed conditional on the value for $x_j$ and over the marginal distribution of all other features.
The constant $c_j$ is chosen so that the mean of $f_{j,ALE}(x_j)$ with respect to the marginal distribution of $X_j$ is zero.
The ALE main effects are defined as the gradients with respect to the features, but are estimated with finite differences, i.e. they can be computed for non-differentiable prediction functions.
For the actual estimation we refer to \citep{apley2016visualizing}.
% =============================================================================
% ALE is best
% =============================================================================
We use the ALE decomposition, because ALE are computationally cheap to compute (worst case $O(n)$ per feature main effect), the effects can be computed sequentially instead of simultaneously as in \citep{hooker2007generalized} and, most importantly, ALEs do not require knowledge of the data distribution.
Also, ALE are the only decomposition we found with software implementations, see \citep{iml,alepackage}.

% =============================================================================
% =============================================================================
% =============================================================================
% Material
% =============================================================================
% =============================================================================
% =============================================================================
% Categorical features
%Categorical features require an ordering of the categories so that accumulated local effects can be estimated.
%Any ordering of the categories will yield a valid ALE, but the interpretation differs, because category effects are interpreted in terms of changes to the neighbouring categories.
%Compared to \citep{hooker2007generalized}, ALE yields a fundamentally different decomposition of the model function $f(x)$.
%\citep{apley2016visualizing} show that this decomposition is unique because of what they call "pseudo-orthogonality".
%Pseudo-orthogonality means that for any component of the ALE decomposition, a repeated application of ALE computation yields zero or the same ALE component (if the same ALE effect is computed).
%For example, lets look at the 1st-order (=main) effect of $x_1$ $f_{ALE,1}$ and the second order effect between $x_1$ and $x_2$ $f_{ALE,1,2}$.
%If we tried to extract the main order effect for $x_1$ from $f_{ALE, 1,2}$ the result would be a constant function that is always 0.
% =============================================================================
% ALE property: Pseudo-orthogonality
% =============================================================================
%Let $H_S(f_S)$ be a function that maps a function to its ALE function with features S.
%ALE defines the components in such a way that they are "pseudo-orthogonal", which is not true orthogonality, but a similar concept.
%"Pseudo-orthogonality": $H_j(f_j) = f_j$ and $H_u(f_j) = 0$ for each $j \subseteq D$ and $u \subseteq D$ with $u \neq j$.
%In words, pseudo-orthogonality is:
%The S-ALE of an S-ALE function is again S-ALE and the S-ALE function of u-ALE is 0 when S and u are not equal.

%We prefer the orthogonality-like  over orthogonality, since when feature are correlated, the true additive structure is not reflected when effects have to be orthogonal.
%%For example if the model is a linear model with $f(x) = x_1 + 2\cdot x_2$, with $x_1$ and $x_2$ being correlated, we prefer to recover $f_{1}(x_1) = 1$ and $f_{2}(x_2)$ which is the result under ALE decomposition, but not under the constraint of orthogonality.

% =============================================================================
% Concepts for interpretability
% =============================================================================
%There are a few concepts of interpretability.
%Other suggested measures of interpretability is to constrain model form:
%Monotonicity constraints: CITE.
%Causality.CITE.
%Additivity. CITE.
%Sparsity. CITE.
%Linearity. CITE.





% =============================================================================
% Some taxonomy
% =============================================================================
% \citep{bibal2016interpretability} distinguish between interprebaility on model level and on representation level.
% This adds to the taxonomy of \citep{doshi2017towards}
% Quantiative measurements of model: Either some heuristic of model (e.g. number of features in Lasso) or user-based surveys \citep{freitas2014comprehensible}.
% Quantiative heuristic measure model.
% User-based surveys measure representation

% =============================================================================
% Categorization of our method
% =============================================================================
% According to \citep{bibal2016interpretability} we can distinguish measures of interpretability for following areas.
% \begin{itemize}
% \item model (specific, general)
% \item representation (specific, general)
% \end{itemize}
% Our method depends on representation and on the model of course.
% So something in between.



% Pseudo-orthogonality > Orthogonality
% True orthogonality is not desirable, as illustrated with this next example.
% For example if $f(x) = x_1 + x_2$ and $X_1$ and $X_2$ are correlated, then fANOVA decomposition will not give the correct main effects $f_1(x_1)$ and $f_2(x_2)$, but ALE components will decompose it in such a way.
% For fANOVA approach this would be.
% We assume that expected value of $x_1$ and $x_2$ is 0:
% \begin{eqnarray*}
% f_{1,fANOVA}(x_1) =& \mathbb{E}(f(x)|X_1=x_1) - E_X(f(x)) \\
%                   =& \mathbb{E}(X_1| X_1=x_1) + \mathbb{E}(X_2|X_1 = x_1) - (\mathbb{E}(X_1) + \mathbb{E}(X_2)) \\
%                   =& x_1 +  \mathbb{E}(X_2|X_1 = x_1)  \\
%                   \neq & x_1
% \end{eqnarray*}
% Intuition: Feature x1 and x2 are correlated. But effects are forced to be non-correlated.
