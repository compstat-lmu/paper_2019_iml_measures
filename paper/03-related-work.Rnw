\Sexpr{set_parent('paper.Rnw')}
\section{Related Work and Background}
\label{sec:related}

In this section we review related approaches for quantifying interpretability and summarize the background on functional decomposition based on which we developed the interpretability measures.

\subsection{Interpretability Measures}
\label{sec:other}
% =============================================================================
% Lack of definition of interpretability
% =============================================================================
Interpretability depends on the domain \citep{rudin2018please, freitas2014comprehensible, huysmans2011empirical,ruping2006learning} and is not a well defined property \citep{Lipton2016}.
\citep{askira1998knowledge} argue that we can't summarize interpretability with a single metric. 
Many researchers rely on intuition of what interpretability is, which leads to using  structurally simpler models (decision trees, decision rules, ...) instead of black-boxes like SVMs and neural networks \citep{bibal2016interpretability}.
To address these shortcomings of the definition of interpretability, we propose multiple measures instead of a single one.
Also, as we will demonstrate, the functional decomposition can also be a starting point for alternative measures, which can accomodate to the domain in which the models are used.
By offering multiple measures, the tradeoff between them can be adapted in any applicaion.
\citep{freitas2014comprehensible} says that acceptability and interpretability of a model might not be the same.
For example a doctor might reject a short decision tree because it is too simplistic.
Not always is a strictly smaller model (in terms of nodes) more accepted by the user.
\citep{lavravc1999selected} showed a case where medical experts preferred larger trees, since shor trees were not as informative for their decision making.
Doshi et. al  \citep{doshi2017towards} propose different levels at which interpretation methods can be evaluated: application-grounded evaluation (real humans, real tasks), human-grounded (real humans, simple tasks, e.g. through mechanical turk) and functionally-grounded evaluation (no humans, proxy tasks).
Our proposed measures evaluated the interpretability at the functional level.


% =============================================================================
% Model-specific or class-specific measures of interpretability
% =============================================================================
Current measures of model complexity or interpretability are usually model-specific, i.e. only models from the same class can be compared (e.g. compare decision trees with different depths, but not a linear model with decision trees).
Model size is often used as heuristic \citep{huysmans2011empirical,ruping2006learning,askira1998knowledge,yang2017scalable} for quantifying how interpretable a model is (e.g. number of decision rules, tree depth, ...).

But more concerned with representation of models, not with a number of interpretability that can be attached to a model.
\citep{Ozhou2018measuring} propose a general form for model interpetability which includes the structural complexity of the model but again compare model-specific measures like number of leaves, average depth and maximum depth of a tree. 

Akaikes Information Criterion (AIC)  \citep{akaike1998information} and the  Bayesian Information Criterion (BIC)  \citep{schwarz1978estimating} are more broadly applicable tradeoff measure between accuracy and interpretability.
They both rely on a way to estimate the degrees of freedom and a likelihood function, which restricts the applicability to certain model classes (e.g. linear models).
Also the tradeoff between interpretability and performance is fixed and only one dimension of interpretability is considered.
\citep{hauenstein2018computing} use the generalized degrees of freedom to extend the AIC to a model-agnostic measure, but it is computationally expensive as it requires refitting the model multiple times and does not give more granular insight into the complexity.

% =============================================================================
% Other approaches
% =============================================================================
Other approaches involve measuring interpretability through surveys and experiments involving people.
Dhurandhar et. al 2018 \citep{dhurandhar} propose to measure interpretability as improvement in the task the machine learning model originally tries to solve
\citep{friedler2019assessing} measure whether models are intepretable in user studies and proopose as objective measure the runtime operation count of the model.
Ribeiro et. 2017 \citep{Ribeiro2016b} al measured the interpretability of the LIME approach through user studies.
% =============================================================================
% Bridge to decomposition
% =============================================================================
Our approach differs because we don't involve humans, but also because we don't measure the interpretability through how the model is represented, but rather how complex the components of the functional decomposition is.
For example, a decision tree can be represented by a tree structure or a set of decision rules, yet the functional complexity remains the same.
On the other hand, the representation of a tree can become more complex, while the functional complexity decreases (because the tree approximates a linear function).


\subsection{Functional Decomposition}
\label{sec:decomposition}

% =============================================================================
% General decomposition
% =============================================================================
We can decompose any high-dimensional prediction function of a model into a sum of components with increasing dimensionality.
Let $\fh(x)$ be the model prediction function that maps the feature $x$ to the prediction $\fh: \mathbb{R}^p \rightarrow \mathbb{R}$ where x is a p-dimensional feature vector: $x = (x_1, \ldots, x_p)$.

We write the decomposition as:
\begin{eqnarray}\label{eqn:decomp} f(x)  = &\overbrace{f_0}^\text{Intercept} + \overbrace{\sum_{j=1}^p f_j(x_j)}^\text{1st order effects} + \overbrace{\sum_{j\neq k}^p f_{jk}(x_j, x_k)}^\text{2nd order effects} + \ldots + \overbrace{f_{1,\ldots,p}(x_1, \ldots, x_p)}^\text{p-th order effect}
%\\ =  & \sum_{S \subseteq \{1,\ldots,p\}} f_S(x_S)
\end{eqnarray}

The decomposition contains a constant mean plus the feature first order effects, second order effects up to the p-th order effect.
We also call the components in this sum that depend on the main effects and higher-order effects interactions.

% =============================================================================
% Some useful decompositions
% =============================================================================
We need constraints for the components to get a unique decomposition.
Multiple suggestions have been made in the literature.
Stone \citep{stone1994use} suggest to approximate the function with weighted integrals and function orthogonality constraints.
Hooker \citep{hooker2007generalized} defined centering, orthogonality and variance decomposition as desirable properties, which results in hierarchically orthogonal components under the correlation inner product.

% =============================================================================
% ALE decompositions
% =============================================================================
Another approach is Accumulated Local Effects proposed by \cite{apley2016visualizingapley2016visualizing} (ALE) as a tool for visualizing feature effects, that also represents a decomposition.
\begin{eqnarray} f(x)  = & f_0 + \sum_{j=1}^p \falej(x_j)+ sum_{j\neq k}^p f_{ALE,jk}(x_j, x_k) + \ldots + f_{ALE,1,\ldots,p}(x_1, \ldots, x_p)
\end{eqnarray}


Compared to \citep{hooker2007generalized}, ALE yields a fundamentally different decomposition of the model function $f(x)$.
\citep{apley2016visualizingapley2016visualizing} show that this decomposition is unique because of what they call "pseudo-orthogonality".
Pseudo-orthogonality means that for any component of the ALE decomposition, a repeated application of ALE computation yields zero or the same ALE component (if the same ALE effect is computed).
For example, lets look at the 1st-order (=main) effect of $x_1$ $f_{ALE,1}$ and the second order effect between $x_1$ and $x_2$ $f_{ALE,1,2}$.
If we tried to extract the main order effect for $x_1$ from $f_{ALE, 1,2}$ the result would be a constant function that is always 0.
% =============================================================================
% ALE property: Pseudo-orthogonality
% =============================================================================
%Let $H_S(f_S)$ be a function that maps a function to its ALE function with features S.
%ALE defines the components in such a way that they are "pseudo-orthogonal", which is not true orthogonality, but a similar concept.
%"Pseudo-orthogonality": $H_j(f_j) = f_j$ and $H_u(f_j) = 0$ for each $j \subseteq D$ and $u \subseteq D$ with $u \neq j$.
%In words, pseudo-orthogonality is:
%The S-ALE of an S-ALE function is again S-ALE and the S-ALE function of u-ALE is 0 when S and u are not equal.

% =============================================================================
% ALE is best
% =============================================================================
We decided to use the ALE decomposition, because ALE are relatively cheap to compute, can be computed sequentially instead of all together as in \citep{hooker2007generalized}, they don't require knowledge of the data distribution.
We prefer pseudo-orthogonality over orthogonality, since when feature are correlated, the true additive structure is not reflected when effects have to be orthogonal.
For example if the model is a linear model with $f(x) = x_1 + 2\cdot x_2$, with $x_1$ and $x_2$ being correlated, we prefer to recover $f_{1}(x_1) = 1$ and $f_{2}(x_2)$ which is the result under ALE decomposition, but not under the constraint of orthogonality.
Also ALE plots have software solutions, see \citep{iml,alepackage}.



% =============================================================================
% =============================================================================
% =============================================================================
% Material
% =============================================================================
% =============================================================================
% =============================================================================

% =============================================================================
% Concepts for interpretability
% =============================================================================
%There are a few concepts of interpretability.
%Other suggested measures of interpretability is to constrain model form:
%Monotonicity constraints: CITE.
%Causality.CITE.
%Additivity. CITE.
%Sparsity. CITE.
%Linearity. CITE.





% =============================================================================
% Some taxonomy
% =============================================================================
% \citep{bibal2016interpretability} distinguish between interprebaility on model level and on representation level.
% This adds to the taxonomy of \citep{doshi2017towards}
% Quantiative measurements of model: Either some heuristic of model (e.g. number of features in Lasso) or user-based surveys \citep{freitas2014comprehensible}.
% Quantiative heuristic measure model.
% User-based surveys measure representation

% =============================================================================
% Categorization of our method
% =============================================================================
% According to \citep{bibal2016interpretability} we can distinguish measures of interpretability for following areas.
% \begin{itemize}
% \item model (specific, general)
% \item representation (specific, general)
% \end{itemize}
% Our method depends on representation and on the model of course.
% So something in between.



% Pseudo-orthogonality > Orthogonality
% True orthogonality is not desirable, as illustrated with this next example.
% For example if $f(x) = x_1 + x_2$ and $X_1$ and $X_2$ are correlated, then fANOVA decomposition will not give the correct main effects $f_1(x_1)$ and $f_2(x_2)$, but ALE components will decompose it in such a way.
% For fANOVA approach this would be.
% We assume that expected value of $x_1$ and $x_2$ is 0:
% \begin{eqnarray*}
% f_{1,fANOVA}(x_1) =& \mathbb{E}(f(x)|X_1=x_1) - E_X(f(x)) \\
%                   =& \mathbb{E}(X_1| X_1=x_1) + \mathbb{E}(X_2|X_1 = x_1) - (\mathbb{E}(X_1) + \mathbb{E}(X_2)) \\
%                   =& x_1 +  \mathbb{E}(X_2|X_1 = x_1)  \\
%                   \neq & x_1
% \end{eqnarray*}
% Intuition: Feature x1 and x2 are correlated. But effects are forced to be non-correlated.
