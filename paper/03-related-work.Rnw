\Sexpr{set_parent('paper.Rnw')}
\section{Related Work and Background}
\label{sec:related}

In this section we review related approaches for quantifying interpretability and summarize the background on functional decomposition based on which we developed the interpretability measures.

\subsection{Interpretability Measures}
\label{sec:other}
% =============================================================================
% Lack of definition of interpretability
% =============================================================================
Interpretability depends on the domain \citep{rudin2018please, freitas2014comprehensible, huysmans2011empirical,ruping2006learning} and is not a well defined property \citep{Lipton2016}.
\citep{askira1998knowledge} argue that we can't summarize interpretability with a single metric. 
Many researchers rely on intuition of what interpretability is, which leads to using  structurally simpler models (decision trees, decision rules, ...) instead of black-boxes like SVMs and neural networks \citep{bibal2016interpretability}.
To address these shortcomings of the definition of interpretability, we propose multiple measures instead of a single one.
Also, as we will demonstrate, the functional decomposition can also be a starting point for alternative measures, which can accomodate to the domain in which the models are used.
By offering multiple measures, the tradeoff between them can be adapted in any applicaion.
\citep{freitas2014comprehensible} says that acceptability and interpretability of a model might not be the same.
For example a doctor might reject a short decision tree because it is too simplistic.
Not always is a strictly smaller model (in terms of nodes) more accepted by the user.
\citep{lavravc1999selected} showed a case where medical experts preferred larger trees, since shor trees were not as informative for their decision making.
Doshi et. al  \citep{doshi2017towards} propose different levels at which interpretation methods can be evaluated: application-grounded evaluation (real humans, real tasks), human-grounded (real humans, simple tasks, e.g. through mechanical turk) and functionally-grounded evaluation (no humans, proxy tasks).
Our proposed measures evaluated the interpretability at the functional level.


% =============================================================================
% Model-specific or class-specific measures of interpretability
% =============================================================================
Current measures of model complexity or interpretability are usually model-specific, i.e. only models from the same class can be compared (e.g. compare decision trees with different depths, but not a linear model with decision trees).
Model size is often used as heuristic \citep{huysmans2011empirical,ruping2006learning,askira1998knowledge,yang2017scalable} for quantifying how interpretable a model is (e.g. number of decision rules, tree depth, ...).

But more concerned with representation of models, not with a number of interpretability that can be attached to a model.
\citep{Ozhou2018measuring} propose a general form for model interpetability which includes the structural complexity of the model but again compare model-specific measures like number of leaves, average depth and maximum depth of a tree. 

Akaikes Information Criterion (AIC)  \citep{akaike1998information} and the  Bayesian Information Criterion (BIC)  \citep{schwarz1978estimating} are more broadly applicable tradeoff measure between accuracy and interpretability.
They both rely on a way to estimate the degrees of freedom and a likelihood function, which restricts the applicability to certain model classes (e.g. linear models).
Also the tradeoff between interpretability and performance is fixed and only one dimension of interpretability is considered.
\citep{hauenstein2018computing} use the generalized degrees of freedom to extend the AIC to a model-agnostic measure, but it is computationally expensive as it requires refitting the model multiple times and does not give more granular insight into the complexity.

% =============================================================================
% Other approaches
% =============================================================================
Other approaches involve measuring interpretability through surveys and experiments involving people.
Dhurandhar et. al 2018 \citep{dhurandhar} propose to measure interpretability as improvement in the task the machine learning model originally tries to solve
\citep{friedler2019assessing} measure whether models are intepretable in user studies and proopose as objective measure the runtime operation count of the model.
Ribeiro et. 2017 \citep{Ribeiro2016b} al measured the interpretability of the LIME approach through user studies.
% =============================================================================
% Bridge to decomposition
% =============================================================================
Our approach differs because we don't involve humans, but also because we don't measure the interpretability through how the model is represented, but rather how complex the components of the functional decomposition is.
For example, a decision tree can be represented by a tree structure or a set of decision rules, yet the functional complexity remains the same.
On the other hand, the representation of a tree can become more complex, while the functional complexity decreases (because the tree approximates a linear function).


\subsection{Functional Decomposition}
\label{sec:decomposition}


% =============================================================================
% General decomposition
% =============================================================================
We can decompose the high-dimensional model prediction function $f(x)$ into a sum of functions describing feature and interaction effects.
Let f be the model prediction function that maps from features to the prediction, $f(x): X \rightarrow \mathbb{R}$ where x is a p-dimenionsal feature vector: $x = (x_1, \ldots, x_p)$.

\begin{eqnarray} f(x)  = &\overbrace{f_0}^\text{Intercept} + \overbrace{\sum_{j=1}^p f_j(x_j)}^\text{1st order effects} + \overbrace{\sum_{j\neq k}^p f_{jk}(x_j, x_k)}^\text{2nd order effects} + \ldots + \overbrace{f_{1,\ldots,p}(x_1, \ldots, x_p)}^\text{p-th order effect}
%\\ =  & \sum_{S \subseteq \{1,\ldots,p\}} f_S(x_S)
\end{eqnarray}

The decomposition contains a constant mean plus the feature first order effects, second order effects up to the p-th order effect.
The notation of the j-th order effect can be simplified by using a multi-index notation: $S \subset \{1,\ldots,p\}$.


% =============================================================================
% Some useful decompositions
% =============================================================================
We need further constraints to get a unique decomposition.
Multiple suggestions have been made in the literature.
Stone \citep{stone1994use} suggest to approximate the function with weighted integrals and function orthogonality constraints.
\citep{hooker2004discovering,hooker2007generalized} defined centering, orthogonality and variance decomposition as desirable properties, which results in hierarchically orthogonal components (in decomposition) under the correlation inner product.
If the feature are uncorrelated, Partial Dependence  plots \citep{friedman2001greedy} can be used to describe main effects and other effects (minus some constant).
But, as shown by \citep{hooker2007generalized} (TODO: Check citation), it does not work well when features are correlated and the effect estimates are biased.
The partial dependence plot suffers from extrapolation into areas without data, which the ALE plots solve.
TODO: Find paper with PDP decomposition


% =============================================================================
% ALE decompositions
% =============================================================================
% This means that higher order effects are uncorrelated with any lower-order effects that are subsets of the higher order effect: $f_{j,ANOVA}$ and $f_{S,ANOVA}$ are uncorrelated when $j\subset S$.
% Any other combination of j and S are not in general uncorrelated.
Another suggestion is a decomposition into Accumulated Local Effects \citep{apley2016visualizingapley2016visualizing} (ALE).
Compared to partial dependence plots, ALE plots also work when features are correlated.
Yields fundamentally different decomposition of $f(x)$, which is better for visualization of effects than Hooker decomp \citep{hooker2007generalized}.
This is the solution used in this paper and is explained a bit in detail here:
\citep{apley2016visualizingapley2016visualizing} show that this decomposition is unique, meaning that for any decomposition that fulfills the "pseudo-orthogonality" property, the only solution is the ALE component.
A fitted supervised machine learning model $f$ can be decomposed as:
$f(x) = \sum_{S\subseteq P} f_{S,ALE}(x_S)$
where $f_{S,ALE}$ represents the $|S|$-order effect of feature(s) $x_S$ on $f$.


% =============================================================================
% ALE property: Pseudo-orthogonality
% =============================================================================
Let $H_S(f_S)$ be a function that maps a function to its ALE function with features S.
ALE defines the components in such a way that they are "pseudo-orthogonal", which is not true orthogonality, but a similar concept.
"Pseudo-orthogonality": $H_j(f_j) = f_j$ and $H_u(f_j) = 0$ for each $j \subseteq D$ and $u \subseteq D$ with $u \neq j$.
In words, pseudo-orthogonality is:
The S-ALE of an S-ALE function is again S-ALE and the S-ALE function of u-ALE is 0 when S and u are not equal.



% =============================================================================
% ALE is best
% =============================================================================
We decided to use ALE plots instead of the other approaches for the following reasons:
ALE plots are easier to compute, because the can be computed sequentially (each main effect, then each 2nd-order and so on), do not require to estimate the data distribution as Hookers approach.
fANOVA has to be computed all at once, involving a complex system of equations.
Pseudo-orthogonality preferable over orthogonality, since when feature are correlated, the true additive structure is not reflected when effects have to be orthogonal.
Other advantages are the superior computational efficiency and the more sytematic way to compute the decomposition.
Also ALE plots have software solutions, see \citep{iml,alepackage}.



% =============================================================================
% =============================================================================
% =============================================================================
% Material
% =============================================================================
% =============================================================================
% =============================================================================

% =============================================================================
% Concepts for interpretability
% =============================================================================
%There are a few concepts of interpretability.
%Other suggested measures of interpretability is to constrain model form:
%Monotonicity constraints: CITE.
%Causality.CITE.
%Additivity. CITE.
%Sparsity. CITE.
%Linearity. CITE.





% =============================================================================
% Some taxonomy
% =============================================================================
% \citep{bibal2016interpretability} distinguish between interprebaility on model level and on representation level.
% This adds to the taxonomy of \citep{doshi2017towards}
% Quantiative measurements of model: Either some heuristic of model (e.g. number of features in Lasso) or user-based surveys \citep{freitas2014comprehensible}.
% Quantiative heuristic measure model.
% User-based surveys measure representation

% =============================================================================
% Categorization of our method
% =============================================================================
% According to \citep{bibal2016interpretability} we can distinguish measures of interpretability for following areas.
% \begin{itemize}
% \item model (specific, general)
% \item representation (specific, general)
% \end{itemize}
% Our method depends on representation and on the model of course.
% So something in between.



% Pseudo-orthogonality > Orthogonality
% True orthogonality is not desirable, as illustrated with this next example.
% For example if $f(x) = x_1 + x_2$ and $X_1$ and $X_2$ are correlated, then fANOVA decomposition will not give the correct main effects $f_1(x_1)$ and $f_2(x_2)$, but ALE components will decompose it in such a way.
% For fANOVA approach this would be.
% We assume that expected value of $x_1$ and $x_2$ is 0:
% \begin{eqnarray*}
% f_{1,fANOVA}(x_1) =& \mathbb{E}(f(x)|X_1=x_1) - E_X(f(x)) \\
%                   =& \mathbb{E}(X_1| X_1=x_1) + \mathbb{E}(X_2|X_1 = x_1) - (\mathbb{E}(X_1) + \mathbb{E}(X_2)) \\
%                   =& x_1 +  \mathbb{E}(X_2|X_1 = x_1)  \\
%                   \neq & x_1
% \end{eqnarray*}
% Intuition: Feature x1 and x2 are correlated. But effects are forced to be non-correlated.
