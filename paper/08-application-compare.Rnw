\Sexpr{set_parent('paper.Rnw')}
\section{Application: Compare interpretability of two models}
\label{sec:app1}

This example shows the following;
\begin{itemize}
\item Measures in action
\item How to extract a model from a complex model
\end{itemize}


Data from OpenML? \citep{Casalicchio2017}

All examples with mlr \citep{JMLR:v17:15-066} in R \citep{r2016}.
Available on Github the source code for examples.

Modeled with Random Forest \citep{Breiman2001}

We compare random forest with linear model in terms of performance

<<wine-data>>=
wine_orig = read.csv(here("./data/winequalityN.csv"))
wine_orig = na.omit(wine_orig)


wine_orig = wine_orig[wine_orig$type == "white",]
wine_orig$type = NULL
# set.seed(42)
# train_test_i = sample(1:nrow(wine_orig), size = round(0.7 * nrow(wine_orig)))
# validation_i = setdiff(1:nrow(wine_orig), train_test_i)
# wine = wine_orig[train_test_i, ]
wine = wine_orig
print(head(wine))
@

<<wine-tune, dependson="wine-data", results = "asis">>=
# Benchmark both xgboost and linear model
ctrl = makeTuneControlGrid()
ps_xgboost = makeParamSet(
  makeIntegerParam("nrounds", lower = 1, upper = 500)
)
lrn_xgboost = makeTuneWrapper("regr.xgboost", resampling = cv3,
  measures = list(mae), par.set = ps_xgboost, control = ctrl)
ps_ctree = makeParamSet(
  makeIntegerParam("maxdepth", lower = 1, upper = 10)
)
lrn_tree = makeTuneWrapper("regr.ctree", resampling = cv3,
  measures = list(mae), par.set = ps_ctree, control = ctrl)
lrn_fl = makeLearner("regr.featureless")

rdesc = cv5
lrns = list(lrn_xgboost, lrn_tree, lrn_fl)
tsk = makeRegrTask(data = wine, target = "quality")
bmr = benchmark(lrns, tsk, rdesc, measures = list(mae), show.info = FALSE)
bmr_tab = getBMRAggrPerformances(bmr, as.df = TRUE)

baseline_mae = mean(abs(wine$quality - median(wine$quality)))

xtable::xtable(bmr_tab)
@


Now we compare it in terms of interpretability

<<wine-xgboost, dependson = "wine-tune">>=
mod = train(lrn_xgboost, tsk)
pred_rf = Predictor$new(mod, getTaskData(tsk))
fc_xg = FunComplexity$new(pred_rf, epsilon = 0.05, max_c = 10)
plot(fc_xg, nrow = 2)
fc_xg$r2
fc_xg$c_wmean
@

The xgboost model is rather complex, as visible in the plots in \ref{fig:wine-xgboost}.
Average weighted complexity is \Sexpr{fc_xg$c_wmean} and the $R^2$ of the first order ALE approximation is only \Sexpr{fc_xg$r2}, which means that a lot of interactions are modeled.
Of the available \Sexpr{sum(tsk$task.desc$n.feat)} features, \Sexpr{fc_xg$n_features} were used.


<<wine-tree, dependson = "wine-tune">>=
mod_tree = train(lrn_tree, tsk)
pred_tree = Predictor$new(mod_tree, getTaskData(tsk))
fc_tree = FunComplexity$new(pred_tree, epsilon = 0.05, max_c = 10)
plot(fc_tree, nrow = 2)
@

The xgboost model is rather complex, as visible in the plots in \ref{fig:wine-xgboost}.
Average weighted complexity is \Sexpr{fc_tree$c_wmean} and the $R^2$ of the first order ALE approximation is only \Sexpr{fc_tree$r2}, which means that a lot of interactions are modeled.

Of the available \Sexpr{sum(tsk$task.desc$n.feat)} features, \Sexpr{fc_tree$n_features} were used.
