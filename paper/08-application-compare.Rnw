\Sexpr{set_parent('paper.Rnw')}
\section{Application: Compare interpretability of two models}
\label{sec:app1}


Idea: We only compare tree based methods or also rule-based methods.
Goal: Show that these tools are useful even if linear models are not our goal. Also that it can be used to compare trees.
RIPPER, CART, SBRL, OneR, ctree


Datasets:

Biodegradability
https://www.openml.org/d/1494

Wind and Solar energy based on wheather
https://github.com/hugorcf/Renewable-energy-weather/blob/master/renewable.ipynb


This example shows the following;
\begin{itemize}
\item Measures in action
\item How to extract a model from a complex model
\end{itemize}



All examples with mlr \citep{JMLR:v17:15-066} in R \citep{r2016}.
Available on Github the source code for examples.

Modeled with Random Forest \citep{Breiman2001}

We compare random forest with linear model in terms of performance

<<adult-data>>=
SAMPLE = TRUE

adult <- read.table('https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data',
                    sep = ',', fill = F, strip.white = T)
if(SAMPLE) {
  adult = adult[sample(1:nrow(adult), size = 1000),]
  print("ADULT DATA WAS SAMPLED")
}

colnames(adult) <- c('age', 'workclass', 'fnlwgt', 'educatoin',
                     'educatoin_num', 'marital_status', 'occupation', 'relationship', 'race', 'sex',
                     'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income')
tsk = makeClassifTask(data = adult, target = "income")
@

<<adult-tune, dependson="adult-data", results = "asis">>=
# Benchmark both xgboost and linear model
ctrl = makeTuneControlRandom(maxit = 10)
ps_ranger = makeParamSet(
  makeIntegerParam("num.trees", lower = 100, upper = 500),
  makeIntegerParam("mtry", lower = 1, upper = sum(tsk$task.desc$n.feat))
)
lrn_ranger = makeTuneWrapper(makeLearner("classif.ranger", predict.type = "prob"), resampling = cv3,
  measures = list(acc), par.set = ps_ranger, control = ctrl)
ps_ctree = makeParamSet(
  makeIntegerParam("maxdepth", lower = 1, upper = 10)
)
lrn_tree = makeTuneWrapper(makeLearner("classif.ctree", predict.type = "prob"), resampling = cv3,
  measures = list(mmce), par.set = ps_ctree, control = ctrl)
lrn_fl = makeLearner("classif.featureless")

rdesc = cv5
lrns = list(lrn_ranger, lrn_tree, lrn_fl)
bmr = benchmark(lrns, tsk, rdesc, measures = list(acc), show.info = FALSE)
bmr_tab = getBMRAggrPerformances(bmr, as.df = TRUE)

xtable::xtable(bmr_tab)
@


Now we compare it in terms of interpretability

<<adult-ranger, dependson = "adult-tune">>=
mod = train(lrn_ranger, tsk)
pred_rf = Predictor$new(mod, getTaskData(tsk),class = 1)
fc_xg = FunComplexity$new(pred_rf, epsilon = 0.05, max_c = 10)
plot(fc_xg, nrow = 2)
@

The ranger model is rather complex, as visible in the plots in \ref{fig:wine-xgboost}.
Average weighted complexity is \Sexpr{fc_xg$c_wmean} and the $R^2$ of the first order ALE approximation is only \Sexpr{fc_xg$r2}, which means that a lot of interactions are modeled.
Of the available \Sexpr{sum(tsk$task.desc$n.feat)} features, \Sexpr{fc_xg$n_features} were used.


<<adult-tree, dependson = "adult-tune">>=
mod_tree = train(lrn_tree, tsk)
pred_tree = Predictor$new(mod_tree, getTaskData(tsk), class  = 1 )
fc_tree = FunComplexity$new(pred_tree, epsilon = 0.05, max_c = 10)
plot(fc_tree, nrow = 2)
fc_tree$c_wmean
fc_tree$n_features
fc_tree$r2
@

The xgboost model is rather complex, as visible in the plots in \ref{fig:wine-xgboost}.
Average weighted complexity is \Sexpr{fc_tree$c_wmean} and the $R^2$ of the first order ALE approximation is only \Sexpr{fc_tree$r2}, which means that a lot of interactions are modeled.

Of the available \Sexpr{sum(tsk$task.desc$n.feat)} features, \Sexpr{fc_tree$n_features} were used.


