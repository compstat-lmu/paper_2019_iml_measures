\Sexpr{set_parent('paper.Rnw')}
\section{Application: Compare with model-specific measures}
\label{sec:app1}



In this section we demonstrate with various simulations that our proposed measures can be more useful to describe model complexity than model-specific measures that are based on the structure of the model.


\subsection{Linear relationships in a tree}
<<tree-increase-data, fig.cap="Increasing the tree>>=
set.seed(42)
library(rpart)
n = 1000
dat = data.frame(x = runif(n), x2 = rnorm(n))
nsd = 0.4
dat$y = 2 * dat$x + rnorm(n, sd = nsd)

grid.size = 100
epsilon = 0.05
@
Decision trees need to grow very deep to approximate linear relationship.
When a tree approximates a linear relationship, arguably the model structure might be complex, but the actual prediction function might be approximately linear.
We demonstrate how the two both the number of nodes and complexity measure C increase with increasing model accuracy, but at some point the complexity C decreases again.
We argue that the measure of complexity is better, since it describes the complexity on the level of the output.
In the following example, \Sexpr{n} data points are drawn from  a uniform distribution between 0 and 1, and the target is simulated as $2 * x + \epsilon$, where $\epsilon \sim N(0,\Sexpr{nsd})$.
<<tree-increase, fig.cap="Increasing the tree depth always increases number of terminal nodes, but the complexity measure increases at some point because the prediction function can be approximated by a single linear function.">>=

rp_width = function(rp) sum(rp$frame$var == "<leaf>")

rp1 = rpart(y ~ ., data = dat, control = rpart.control(maxdepth = 1))
pred1 = Predictor$new(rp1, data = dat)
fc1 = FunComplexity$new(pred1, grid.size = grid.size, epsilon = epsilon)
p1 = fc1$approx_models$x$plot() +
  ggtitle(sprintf("Leaf nodes: %i, C: %i", rp_width(rp1), fc1$c_wmean))

rp2 = rpart(y ~ ., data = dat, control = rpart.control(maxdepth = 2))
pred2 = Predictor$new(rp2, data = dat)
fc2 = FunComplexity$new(pred2, grid.size = grid.size, epsilon = epsilon)
p2 = fc2$approx_models$x$plot() +
  ggtitle(sprintf("Leaf nodes: %i, C: %i", rp_width(rp2), fc2$c_wmean))

rp3 = rpart(y ~ ., data = dat, control = rpart.control(maxdepth = 3))
pred3 = Predictor$new(rp3, data = dat)
fc3 = FunComplexity$new(pred3, grid.size = grid.size, epsilon = epsilon)
p3 = fc3$approx_models$x$plot() +
  ggtitle(sprintf("Leaf nodes: %i, C: %i", rp_width(rp3), fc3$c_wmean))

grid.arrange(p1, p2, p3, nrow = 1)
@


\subsection{Interaction: tree vs. logistic regression}

In this example we demonstrate how deceptive model formulization can be and that we need a measure to really compare if one or the other model relies more heavily on interactions.
Interaction means that the output of the model can't be fully explained by the main effects of the model.
The logistic regression model might be a linear model on the level of the log odds, but not on the probability level.

<<interaction-demo-data>>=
library(rpart)
library(partykit)
library(ranger)
set.seed(42)
n = 500
dat = data.frame(x1 = rnorm(n), x2 = rnorm(n), x3  = rnorm(n))
dat$y = ifelse(exp(dat$x1 +  dat$x2 + dat$x3^2)> 0.1, 1, 0)
# to remove linear separability
change_prob = 0.01
change_index = which(rbinom(size = 1, n = n, prob = change_prob) == 1)
dat$y[change_index] = 1 - dat$y[change_index]
dat$y = factor(dat$y)

grid.size = 100
epsilon = 0.05
@

In the following example we simulate 3 features each is normal distributed with mean zero and standard deviation of 1.
We sample \Sexpr{n} data points from this distribution and simulate the target y as follows:

$$\eta = exp(x_1 + x_2 + x_3)$$
$$y = \begin{cases}\eta>0.5&1\\\eta \leq 0.5 0\\\end{cases}$$
We flip each of the computed y's with a probability of \Sexpr{change_prob * 100} \% to avoid linear separability.

We train a logistic regression model.

<<interaction-demo>>=
rp = glm(y ~ ., data = dat, family = "binomial")
pred = Predictor$new(rp, data = dat, predict.fun = function(model, newdata) predict(model, newdata, type = "response"))
fc = FunComplexity$new(pred)
@

The main effect model explains \Sexpr{100 * fc2$r2}\% of output for the logistic regression model.
Measures have advantage to operate on the level of outcome.
Usual interpretation of parameters in logistic regression, which are linear within the non-linear transformation function (logit), hides that logistic regression models interactions.
The interactions come from the logistic function and saturation.
When an instance has already 0.995 probability, changing a feature by one unit might increase probability to 0.996, but if probability would be lower, like 0.5, an increase might change it to 0.8.


% TODO: Continue here


\subsubsection{PDP unreliable high Interaction}

For one toy task and different learner/hyperparameters (that increase IA) show that PDP hide infos.
PDP hides infos means that centered ICE curves vary.
Show with plot.

Using PDP here because, unlike ALE, they have a pendant for individual observations:ICE curves, which visualize heterogeneity of feature effects.

Friedman 2

Comparing kknn with k = XXX, gamboost and random forest.

<<PDP-unreliable, fig.cap="Comparing PDP+ICE for models with different IA measure. The higher IA, the less reliable the PDP information. ICE shows high variance of feature effects. Here x3 is shown, x1 x2 and x4 look similar.">>=
set.seed(123)
n = 100
dat = mlbench::mlbench.friedman2(n, sd = 0.3)
datx = data.frame(cbind(dat$x, "y" = dat$y))
task = makeRegrTask(data  = datx, target = "y")

y_limit = c(-200, 1200)
feature = "V2"

lrn = makeLearner("regr.gamboost")
mod = train(lrn, task)
pred = Predictor$new(mod, datx, class = 1)
fc = FunComplexity$new(pred, grid.size = 100)
r2_gamboost = 1 - fc$r2
fe = FeatureEffects$new(pred, method = "pdp+ice", center.at = 0)
p_gamboost = plot(fe$effects[[feature]], ylim = y_limit) +
  ggtitle(sprintf("gamboost, IA=%.2f", r2_gamboost))


lrn = makeLearner("regr.ranger")
mod = train(lrn, task)
pred = Predictor$new(mod, datx)
fc = FunComplexity$new(pred, grid.size = 100)
r2_ksvm = 1- fc$r2
fe = FeatureEffects$new(pred, method = "pdp+ice", center.at = 0)
p_ksvm = plot(fe$effects[[feature]], ylim = y_limit) +
    ggtitle(sprintf("ranger (random forest), IA=%.2f", r2_ksvm))


lrn = makeLearner("regr.kknn", k = 3)
mod = train(lrn, task)
pred = Predictor$new(mod, datx, class = 1)
fc = FunComplexity$new(pred, grid.size = 100)
r2_kknn = 1- fc$r2
fe = FeatureEffects$new(pred, method = "pdp+ice", center.at = 0)
p_kknn = plot(fe$effects[[feature]], ylim = y_limit) +
    ggtitle(sprintf("kknn, IA=%.2f", r2_kknn))

grid.arrange(p_gamboost, p_ksvm, p_kknn, nrow = 1)
@





\subsection{Complexity lm, gam, interactions}
<<>>=
library(mgcv)
library(Metrics)
set.seed(12)
n = 200

create_dat = function(n) {
  dat = data.frame(x1 = rnorm(n), x2 = rnorm(n), x3 = rnorm(n))
  dat$y = dat$x1 * dat$x2 + dat$x3^2 + rnorm(n, sd = 0.3)
  dat
}
dat = create_dat(n)
newdat = create_dat(n)

mod1 = lm(y ~ x1 + x2 + x3, data = dat)
mod2 = lm(y ~ x1 * x2 + x3, data = dat)
mod3 = gam(y ~ x1 + s(x2) + s(x3), data = dat)
mod4 = gam(y ~ s(x1, x2) + s(x3), data = dat)

pred1 = Predictor$new(mod1, dat)
fc1 = FunComplexity$new(pred1)
plot(fc1)


pred2 = Predictor$new(mod2, dat)
fc2 = FunComplexity$new(pred2)
plot(fc2)


pred3 = Predictor$new(mod3, dat)
fc3 = FunComplexity$new(pred3)
plot(fc3)


pred4 = Predictor$new(mod4, dat)
fc4 = FunComplexity$new(pred4)
plot(fc4)



analyze_lin_mod  = function(mod, fc, newdat) {
  data.frame(
    c = fc$c_wmean,
    r2 = fc$r2,
    df = mod$rank,
    mae = mae(newdat$y, predict(mod, newdat)))
}

res = lapply(list(mod1, mod2, mod3, mod4), function(mod) {
  pred = Predictor$new(mod, dat)
  fc = FunComplexity$new(pred)
  analyze_lin_mod(mod, fc, newdat)
})

xtable::xtable(rbindlist(res))

@



Datasets:

Biodegradability
https://www.openml.org/d/1494

Wind and Solar energy based on wheather
https://github.com/hugorcf/Renewable-energy-weather/blob/master/renewable.ipynb


This example shows the following;
\begin{itemize}
\item Measures in action
\item How to extract a model from a complex model
\end{itemize}

TODO: Show that different splits of the features which lead to more or less levels, does not affect model-specific measures of complexity like the length of the rules.
But it increases the IA measure.
Proposed measures are sometimes better, because they look at the effects and not on internal structures.
Sometimes a more complex structure does not lead to more complex relationships like a very deep tree that only uses one feature to approximate a linear function.
The deeper it gets, the


All examples with mlr \citep{JMLR:v17:15-066} in R \citep{r2016}.
Available on Github the source code for examples.

Modeled with Random Forest \citep{Breiman2001}

We compare random forest with linear model in terms of performance

<<adult-data>>=
SAMPLE = TRUE

adult = read.csv("../data/adult.data")
if(SAMPLE) {
  adult = adult[sample(1:nrow(adult), size = 1000),]
  print("ADULT DATA WAS SAMPLED")
}

colnames(adult) <- c('age', 'workclass', 'fnlwgt', 'educatoin',
  'educatoin_num', 'marital_status', 'occupation', 'relationship', 'race', 'sex',
  'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income')
tsk = makeClassifTask(data = adult, target = "income")
@

<<adult-tune, dependson="adult-data", results = "asis">>=
# Benchmark both xgboost and linear model
ctrl = makeTuneControlRandom(maxit = 10)
ps_ranger = makeParamSet(
  makeIntegerParam("num.trees", lower = 100, upper = 500),
  makeIntegerParam("mtry", lower = 1, upper = sum(tsk$task.desc$n.feat))
)
lrn_ranger = makeTuneWrapper(makeLearner("classif.ranger", predict.type = "prob"), resampling = cv3,
  measures = list(acc), par.set = ps_ranger, control = ctrl)
ps_ctree = makeParamSet(
  makeIntegerParam("maxdepth", lower = 1, upper = 10)
)
lrn_tree = makeTuneWrapper(makeLearner("classif.ctree", predict.type = "prob"), resampling = cv3,
  measures = list(mmce), par.set = ps_ctree, control = ctrl)
lrn_fl = makeLearner("classif.featureless")

rdesc = cv5
lrns = list(lrn_ranger, lrn_tree, lrn_fl)
bmr = benchmark(lrns, tsk, rdesc, measures = list(acc), show.info = FALSE)
bmr_tab = getBMRAggrPerformances(bmr, as.df = TRUE)

xtable::xtable(bmr_tab)
@


Now we compare it in terms of interpretability

<<adult-ranger, dependson = "adult-tune">>=
mod = train(lrn_ranger, tsk)
pred_rf = Predictor$new(mod, getTaskData(tsk),class = 1)
fc_xg = FunComplexity$new(pred_rf, epsilon = 0.05)
plot(fc_xg, nrow = 2)
@

The ranger model is rather complex, as visible in the plots in \ref{fig:wine-xgboost}.
Average weighted complexity is \Sexpr{fc_xg$c_wmean} and the $R^2$ of the first order ALE approximation is only \Sexpr{fc_xg$r2}, which means that a lot of interactions are modeled.
Of the available \Sexpr{sum(tsk$task.desc$n.feat)} features, \Sexpr{fc_xg$n_features} were used.


<<adult-tree, dependson = "adult-tune">>=
mod_tree = train(lrn_tree, tsk)
pred_tree = Predictor$new(mod_tree, getTaskData(tsk), class  = 1 )
fc_tree = FunComplexity$new(pred_tree, epsilon = 0.05)
plot(fc_tree, nrow = 2)
fc_tree$c_wmean
fc_tree$n_features
fc_tree$r2
@

The xgboost model is rather complex, as visible in the plots in \ref{fig:wine-xgboost}.
Average weighted complexity is \Sexpr{fc_tree$c_wmean} and the $R^2$ of the first order ALE approximation is only \Sexpr{fc_tree$r2}, which means that a lot of interactions are modeled.

Of the available \Sexpr{sum(tsk$task.desc$n.feat)} features, \Sexpr{fc_tree$n_features} were used.




<<sbrl>>=
library("sbrl")
library("arules")

adult2 = as.data.frame(lapply(adult, function(x) {
  if(is.factor(x) || length(unique(x)) < 5) {
    as.factor(x)
  } else {
    discretize(x, method = "interval", 3)
    #discretize(x, breaks = max(length(unique(x))-1, 5))
  }
}))

get.sbrl.rules = function(x) {
  res = lapply(1:nrow(x$rs), function(i) {
    if (i == 1)
      sprintf("If      %s (rule[%d]) then positive probability = %.8f\n",
        x$rulenames[x$rs$V1[i]], x$rs$V1[i], x$rs$V2[i])
    else if (i == nrow(x$rs))
      sprintf("else  (default rule)  then positive probability = %.8f\n",
        x$rs$V2[nrow(x$rs)])
    else sprintf("else if %s (rule[%d]) then positive probability = %.8f\n",
      x$rulenames[x$rs$V1[i]], x$rs$V1[i], x$rs$V2[i])
  })
  data.frame(rules = unlist(res))
}


adult2$label = adult2$income
adult2 = droplevels.data.frame(adult2)
rules = sbrl(adult2[setdiff(colnames(adult2), c("income"))], pos_sign = " >50K", neg_sign = " <=50K", rule_maxlen = 2)
pred = Predictor$new(rules, data = adult2, class = 1)
fc = FunComplexity$new(pred)
fc$c_wmean
fc$r2
rules = sbrl(adult2[setdiff(colnames(adult2), c("income"))], pos_sign = " >50K", neg_sign = " <=50K", rule_maxlen = 3)
pred = Predictor$new(rules, data = adult2, class = 1)
fc = FunComplexity$new(pred)
fc2$c_wmean
fc2$r2


rules = sbrl(adult2[setdiff(colnames(adult2), c("income"))], pos_sign = " >50K", neg_sign = " <=50K", lambda = )
pred = Predictor$new(rules, data = adult2, class = 1)
fc = FunComplexity$new(pred)
fc2$c_wmean
fc2$r2
@

\subsubsection{LIME unreliable when C high}

Quick Idea: Maybe LIME models match the ALE curves roughly?
Show that $R^2$ of LIME suffers with increasing in C.
GAmboost for friedman2 with increasing m.

<<>>=
library("lime")
set.seed(123)
n = 500
dat = mlbench::mlbench.friedman3(n, sd = 0.3)
datx = data.frame(cbind(dat$x, "y" = dat$y))
task = makeRegrTask(data  = datx, target = "y")

for(pn in c(-1,0,1, 2, 3)) {
  lrn = makeLearner("regr.earth", penalty  = pn)
  mod = train(lrn, task)
  pred = Predictor$new(mod, datx, class = 1)
  fc = FunComplexity$new(pred, grid.size = 100)
  c = fc$c_wmean
  print(c)
  print(fc$r2)
  fe = FeatureEffects$new(pred, method = "pdp+ice", center.at = 0)
  # TODO: Fit LIME models
  exp = lime(datx, mod)
  x = explain(datx[1:50,], exp, n_features = 2)
  print(median(x$model_r2))
}

@

\subsubsection{Correlation between GAM degrees of freedom and C??}
<<gam-approx>>=
set.seed(123)
n = 500
dat = mlbench::mlbench.friedman3(n, sd = 0.3)
datx = data.frame(cbind(dat$x, "y" = dat$y))
task = makeRegrTask(data  = datx, target = "y")
library("mgcv")


gammas = c(0.1, 0.5, 1, 2, 5, 10, 20, 100)
edf = vector()
cs = vector()
epsilons = vector()
for (eps in c(0.1, 0.05, 0.01, 0.001)) {
  for(gamma in gammas) {
    gm = gam(y ~ s(V1) + s(V2) + s(V3) + s(V4), data = datx, gamma = gamma)
    edf  = c(edf, sum(gm$edf)/4)
    pred = Predictor$new(gm, datx)
    fc = FunComplexity$new(pred, epsilon = eps)
    cs = c(cs, fc$c_wmean)
    epsilons = c(epsilons, eps)
  }
}
ggplot(data.frame(x = edf, y = cs, e = factor(epsilons))) +
  geom_line(aes(x=x,y=y, group = e, color = e))  +
  scale_x_continuous(limits = c(0,NA)) +
  scale_y_continuous(limits = c(0,NA))
@





